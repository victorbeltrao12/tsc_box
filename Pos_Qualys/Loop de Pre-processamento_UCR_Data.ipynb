{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OPQ2CvQ16qSn"},"outputs":[],"source":["!pip install aeon\n","!pip install sktime\n","!pip install tsfresh\n","!pip install tslearn\n","!pip install PyWavelets"]},{"cell_type":"markdown","metadata":{"id":"vb2wJ4B9U2pD"},"source":["### To Do list\n","\n","\n","*   Comparar os resultados do 1NN contra o SVM+RF\n","*   Comparar os resultados dos classificadores Feature Based com o SVM+RF\n","*   Comparar os resultados do MetaClf_Conc contra o MetaClf_Dict\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11750,"status":"ok","timestamp":1709298156704,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"mALblC0a6_9B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import aeon\n","from aeon.datasets import load_classification\n","from aeon.datasets.tsc_data_lists import univariate_equal_length\n","from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n","#from aeon.classification.convolution_based import RocketClassifier\n","\n","import tsfresh\n","from tsfresh import extract_features, select_features\n","from tsfresh.feature_extraction import MinimalFCParameters\n","\n","import tslearn\n","from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n","from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n","\n","import os\n","import math\n","import pywt\n","\n","from sklearn.linear_model import RidgeClassifierCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import LeaveOneOut, train_test_split\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from scipy.fftpack import fft\n","from scipy.stats import norm\n","\n","from tqdm import tqdm\n","import timeit\n","from datetime import timedelta\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1709298156705,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"InL-RPCHCjWM"},"outputs":[],"source":["# Transform data using TimeSeriesScalerMeanVariance and concatenate all transformed data\n","def transform_data(X, num_features=10):\n","    n_sax_symbols = int(X.shape[1] / num_features)\n","    n_paa_segments = int(X.shape[1] / num_features)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","\n","    return data_concat"]},{"cell_type":"code","execution_count":138,"metadata":{"executionInfo":{"elapsed":344,"status":"ok","timestamp":1709315641030,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"zB3SmlIRthyX"},"outputs":[],"source":["def select_model(option, random_state):\n","    if option == '1nn':\n","        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1)\n","    elif option == '5nn':\n","        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3)\n","    elif option == 'svm':\n","        return SVC(C=100, gamma=0.01, kernel='linear', probability=True)\n","    elif option == 'rd':\n","        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n","    else:\n","        return RandomForestClassifier(n_estimators=200, n_jobs = -1, random_state=random_state)"]},{"cell_type":"markdown","metadata":{"id":"8x8kdwW02r2f"},"source":["### Agrupamento de todas as probabilidades"]},{"cell_type":"code","execution_count":139,"metadata":{"executionInfo":{"elapsed":338,"status":"ok","timestamp":1709316162495,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"ACfkDWertiif"},"outputs":[],"source":["from sklearn.model_selection import cross_val_predict\n","def train_with_meta_classifier(X_train, y_train, base_options=['3nn'], meta_option='random_forest', random_state=42):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    meta_features = []\n","    base_predictions = []\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), ascii=True, desc=\"Training Instances\"):\n","        X_train_fold, X_test_fold = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n","\n","        base_models = []\n","        for option in base_options:\n","            model = select_model(option, random_state)\n","            model.fit(X_train_fold, y_train_fold)\n","            base_models.append(model)\n","\n","        base_preds_fold = []\n","        for model in base_models:\n","            # Predict probabilities for each class using cross-validation\n","            proba = model.predict_proba(X_test_fold)\n","            base_preds_fold.append(proba)\n","\n","        # Stack the predicted probabilities along the columns to form meta-features\n","        base_preds_fold = np.column_stack(base_preds_fold)\n","        meta_features.append(base_preds_fold)\n","\n","        # Predict classes using the ensemble of base models\n","        ensemble_proba = np.mean(base_preds_fold, axis=1)\n","        ensemble_class = np.argmax(ensemble_proba)\n","        base_predictions.append(ensemble_class)\n","\n","    # Train meta-classifier using meta-features and base predictions\n","    meta_features = np.concatenate(meta_features, axis=0)\n","    base_predictions = np.array(base_predictions)\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(meta_features, base_predictions)\n","\n","    return meta_classifier\n","\n"]},{"cell_type":"markdown","metadata":{"id":"quMKuQeJ2vAM"},"source":["### Agrupamento de todas as probabilidades com redução de dimensionalidade usando PCA"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1709311767570,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"DrljOb_HyDJa"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","\n","def train_with_meta_classifier(X_train, y_train, base_options='1nn', meta_option='random_forest', random_state=42, n_components=3):\n","    X_train_transformed = transform_data(X_train)\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    meta_features = []\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), ascii=True, desc=\"Training Instances\"):\n","        X_train_fold, X_test_fold = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n","\n","        base_models = []\n","        base_models_scores = []\n","        for option in base_options:\n","            model = select_model(option, random_state)\n","            model.fit(X_train_fold, y_train_fold)\n","            base_models.append(model)\n","            score = model.score(X_test_fold, y_test_fold)\n","            base_models_scores.append(score)\n","\n","        best_model_index = np.argmax(base_models_scores)\n","        best_model = base_models[best_model_index]\n","        proba = best_model.predict_proba(X_test_fold.reshape(1, -1))[0]  # Get predicted probabilities for the best model\n","\n","        meta_features.append(proba)\n","\n","    meta_features = np.array(meta_features)\n","\n","    # Redução de dimensionalidade com PCA\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    meta_features_pca = pca.fit_transform(meta_features)\n","\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(meta_features_pca, y_train)\n","\n","    return base_models, meta_classifier\n"]},{"cell_type":"markdown","metadata":{"id":"cGdprxmE28DV"},"source":["### Seleção das melhores caracteristicas"]},{"cell_type":"code","execution_count":99,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709313784808,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"QpJ6S_Ij3DjW"},"outputs":[],"source":["def train_with_meta_classifier(X_train, y_train, base_options=['1nn', '3nn', 'knn', 'svm', 'rf'], meta_option='random_forest', random_state=42, n_features_per_model=10):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    meta_features = []\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), ascii=True, desc=\"Training Instances\"):\n","        X_train_fold, X_test_fold = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n","\n","        base_models = []\n","        base_models_scores = []\n","        base_models_features = []  # Lista para armazenar as características selecionadas de cada modelo\n","\n","        for option in base_options:\n","            model = select_model(option, random_state)\n","            model.fit(X_train_fold, y_train_fold)\n","            base_models.append(model)\n","\n","            # Calcular a importância das características (se aplicável)\n","            if hasattr(model, 'feature_importances_'):\n","                feature_importances = model.feature_importances_\n","                # Selecionar as características mais importantes\n","                top_features_indices = np.argsort(feature_importances)[::-1][:n_features_per_model]\n","                base_models_features.append(top_features_indices)\n","            else:\n","                # Se o modelo não tiver atributo de importância das características, selecionar aleatoriamente\n","                n_features = X_train_fold.shape[1]\n","                top_features_indices = np.random.choice(n_features, n_features_per_model, replace=False)\n","                base_models_features.append(top_features_indices)\n","\n","            score = model.score(X_test_fold, y_test_fold)\n","            base_models_scores.append(score)\n","\n","        best_model_index = np.argmax(base_models_scores)\n","        best_model_features = base_models_features[best_model_index]\n","\n","        # Combinar as características selecionadas de todos os modelos\n","        meta_features_instance = np.concatenate(base_models_features, axis=1)\n","        meta_features.append(meta_features_instance)\n","\n","        meta_features.append(meta_features_instance)\n","\n","    meta_features = np.array(meta_features)\n","\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","\n","    return base_models, meta_classifier\n"]},{"cell_type":"markdown","metadata":{"id":"rk9wGOEy3FBt"},"source":["### Teste"]},{"cell_type":"code","execution_count":140,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1709316350520,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"ce8sOjJCJc15"},"outputs":[],"source":["def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","\n","    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)\n","\n","        instance_features = []\n","        for rep, model in trained_base_models.items():\n","            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))\n","            instance_features.extend(proba.flatten())\n","\n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n","\n","        meta_features_test.append(meta_feature.flatten())\n","\n","    meta_features_test = np.array(meta_features_test)\n","\n","    return predictions\n"]},{"cell_type":"code","execution_count":147,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1709317275115,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"H9s__h16NJ_9"},"outputs":[],"source":["from sklearn.model_selection import LeaveOneOut\n","from sklearn.model_selection import cross_val_predict\n","import numpy as np\n","from tqdm import tqdm\n","\n","def train_with_meta_classifier(X_train, y_train, base_options=['3nn'], meta_option='random_forest', random_state=42):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    meta_features = []\n","    base_models = []  # Lista para armazenar os classificadores base treinados\n","\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), ascii=True, desc=\"Training Instances\"):\n","        X_train_fold, X_test_fold = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n","\n","        fold_base_models = []  # Lista para armazenar os classificadores base desta dobra\n","\n","        for option in base_options:\n","            model = select_model(option, random_state)\n","            model.fit(X_train_fold, y_train_fold)\n","            fold_base_models.append(model)\n","\n","        base_models.append(fold_base_models)  # Adicione os classificadores base desta dobra à lista geral\n","\n","        base_preds_fold = []\n","        for model in fold_base_models:\n","            # Predict probabilities for each class using cross-validation\n","            proba = model.predict_proba(X_test_fold)\n","            base_preds_fold.append(proba)\n","\n","        # Stack the predicted probabilities along the columns to form meta-features\n","        base_preds_fold = np.column_stack(base_preds_fold)\n","        meta_features.append(base_preds_fold)\n","\n","    # Train meta-classifier using meta-features\n","    meta_features = np.concatenate(meta_features, axis=0)\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","\n","    return base_models, meta_classifier\n","\n","\n","def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","\n","    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)  # Aplicar a mesma transformação que foi aplicada aos dados de treinamento\n","\n","        instance_features = []\n","        for fold_models in trained_base_models:\n","            for model in fold_models:\n","                proba = model.predict_proba(x_transformed)\n","                instance_features.extend(proba.flatten())\n","\n","        # Se os dados de teste tiverem menos características do que os de treinamento,\n","        # podemos preencher as dimensões ausentes com zeros\n","        num_missing_features = X_train.shape[1] - x_transformed.shape[1]\n","        if num_missing_features > 0:\n","            instance_features.extend([0] * num_missing_features)\n","\n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n","\n","        meta_features_test.append(meta_feature.flatten())\n","\n","    meta_features_test = np.array(meta_features_test)\n","\n","    return predictions\n","\n"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1709313361373,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"j1sRMGzH8Y3z"},"outputs":[],"source":["univariate_list = list(univariate_equal_length)\n","univariate_list.sort()"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1709313361693,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"SK6XVhuFidYy","outputId":"e220781d-693d-4218-b773-d68dcefe1673"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"for name in univariate_equal_length:\\n    train, train_labels = load_classification(name, split='TRAIN')\\n    test, test_labels = load_classification(name, split='test')\\n    print(f'''{name}\\n    Train: {train.shape}\\n    Train Labels: {train_labels.shape}\\n    Teste: {test.shape}\\n    True Labels: {test_labels.shape}\\n    ''')\""]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"for name in univariate_equal_length:\n","    train, train_labels = load_classification(name, split='TRAIN')\n","    test, test_labels = load_classification(name, split='test')\n","    print(f'''{name}\n","    Train: {train.shape}\n","    Train Labels: {train_labels.shape}\n","    Teste: {test.shape}\n","    True Labels: {test_labels.shape}\n","    ''')\"\"\"\n"]},{"cell_type":"code","execution_count":207,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":817,"status":"error","timestamp":1709324263274,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"xRJH8xsgC7ED","outputId":"e7e14a3b-e837-4ee2-b2e1-c0c523d3c643"},"outputs":[{"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (30,5) (30,30) ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-207-6039d2caf9be>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# Treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mtrained_base_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_meta_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_option\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random_forest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0;31m# Teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mpredictions_test_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_meta_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_base_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-206-d16bea1b7794>\u001b[0m in \u001b[0;36mtrain_with_meta_classifier\u001b[0;34m(X_train, y_train, base_options, meta_option, random_state)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0maccuracy_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccuracy\u001b[0m  \u001b[0;31m# Criar matriz de acurácia para cada classe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mweighted_probabilities\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccuracy_vector\u001b[0m  \u001b[0;31m# Multiplicação elemento a elemento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Treinamento do meta-classificador usando as probabilidades ponderadas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (30,5) (30,30) "]}],"source":["accuracy_data = []\n","algos = ['1nn', '3nn','svm', 'rd', 'random_forest']\n","dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n","teste = ['Beef']\n","for dataset_name in teste:\n","    train, train_labels = load_classification(dataset_name, split='TRAIN')\n","    test, test_labels = load_classification(dataset_name, split='test')\n","\n","    xtrain = train.reshape(train.shape[0], -1)\n","    xtest = test.reshape(test.shape[0], -1)\n","\n","    #data_train = transform_data(xtrain)\n","    #data_test = transform_data(xtest)\n","\n","    for algo in algos:\n","      # Treino\n","      trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, train_labels, meta_option='random_forest', random_state=42)\n","      # Teste\n","      predictions_test_meta = predict_with_meta_classifier(xtest, test_labels, trained_base_models, meta_classifier)\n","      # Resultado\n","      test_accuracy_meta = np.mean(predictions_test_meta == test_labels)\n","\n","    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n","\n","accuracy_df = pd.DataFrame(accuracy_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nll2PArvqY1T"},"outputs":[],"source":["num_classes, num_features = X_train.shape"]},{"cell_type":"code","execution_count":206,"metadata":{"executionInfo":{"elapsed":333,"status":"ok","timestamp":1709324260245,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"cX1Ofmaiozt9"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def train_with_meta_classifier(X_train, y_train, base_options=['svm', 'random_forest'], meta_option='random_forest', random_state=42):\n","    # Treinamento dos modelos base\n","    num_classes, num_features = X_train.shape\n","    trained_base_models = []\n","    for option in base_options:\n","        model = select_model(option, random_state)\n","        model.fit(X_train, y_train)\n","        trained_base_models.append(model)\n","\n","    # Calcular a acurácia de cada modelo base\n","    accuracies = [accuracy_score(y_train, model.predict(X_train)) for model in trained_base_models]\n","\n","    # Calcular as probabilidades ponderadas para cada classe\n","    weighted_probabilities = np.zeros((X_train.shape[0], num_classes))  # Inicializar matriz para armazenar as probabilidades ponderadas\n","    for model, accuracy in zip(trained_base_models, accuracies):\n","        probas = model.predict_proba(X_train)\n","        accuracy_vector = np.ones((X_train.shape[0], num_classes)) * accuracy  # Criar matriz de acurácia para cada classe\n","        weighted_probabilities += probas * accuracy_vector  # Multiplicação elemento a elemento\n","\n","    # Treinamento do meta-classificador usando as probabilidades ponderadas\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(weighted_probabilities, y_train)\n","\n","    return trained_base_models, meta_classifier\n","\n","def predict_with_meta_classifier(X_test, y_train, trained_base_models, trained_meta_classifier):\n","    # Calculando as probabilidades ponderadas para os dados de teste\n","    num_classes, num_features = X_test.shape\n","    weighted_probabilities = np.zeros((X_test.shape[0], num_classes))\n","    for model in trained_base_models:\n","        probas = model.predict_proba(X_test)\n","        accuracies = accuracy_score(y_test, model.predict(X_test))  # Calcular acurácia do modelo para os dados de teste\n","        accuracy_vector = np.ones((X_test.shape[0], num_classes)) * accuracies  # Criar matriz de acurácia para cada classe\n","        weighted_probabilities += probas * accuracy_vector  # Multiplicação elemento a elemento\n","\n","    # Previsões usando o meta-classificador\n","    predictions = trained_meta_classifier.predict(weighted_probabilities)\n","\n","    return predictions\n"]},{"cell_type":"code","execution_count":193,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709323833948,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"Kx1CrinTcjr2"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def train_with_meta_classifier(X_train, y_train, base_options=['svm', 'random_forest', 'knn', 'gbc'], meta_option='random_forest', random_state=42):\n","    num_classes = X_train.shape\n","    # Treinamento dos modelos base\n","    trained_base_models = []\n","    for option in base_options:\n","        model = select_model(option, random_state)\n","        model.fit(X_train, y_train)\n","        trained_base_models.append(model)\n","\n","    # Calcular a acurácia de cada modelo base\n","    accuracies = [accuracy_score(y_train, model.predict(X_train)) for model in trained_base_models]\n","\n","    # Calcular as probabilidades ponderadas para cada classe\n","    weighted_probabilities = np.zeros((X_train.shape[0], num_classes))  # Inicializar matriz para armazenar as probabilidades ponderadas\n","    for model, accuracy in zip(trained_base_models, accuracies):\n","        probas = model.predict_proba(X_train)\n","        accuracy_vector = np.ones((probas.shape[1],)) * accuracy  # Criar vetor de acurácia para cada classe\n","        weighted_probabilities += probas * accuracy_vector.reshape(1, -1)  # Multiplicação elemento a elemento\n","\n","    # Treinamento do meta-classificador usando as probabilidades ponderadas\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(weighted_probabilities, y_train)\n","\n","    return trained_base_models, meta_classifier\n","\n","def predict_with_meta_classifier(X_test, y_train, trained_base_models, trained_meta_classifier):\n","    num_classes = X_test.shape\n","    # Calculando as probabilidades ponderadas para os dados de teste\n","    weighted_probabilities = np.zeros((X_test.shape[0], num_classes))\n","    for model in trained_base_models:\n","        probas = model.predict_proba(X_test)\n","        accuracies = accuracy_score(y_test, model.predict(X_test))  # Calcular acurácia do modelo para os dados de teste\n","        accuracy_vector = np.ones((probas.shape[1],)) * accuracies  # Criar vetor de acurácia para cada classe\n","        weighted_probabilities += probas * accuracy_vector.reshape(1, -1)  # Multiplicação elemento a elemento\n","\n","    # Previsões usando o meta-classificador\n","    predictions = trained_meta_classifier.predict(weighted_probabilities)\n","\n","    return predictions\n"]},{"cell_type":"code","execution_count":183,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709320554022,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"9D9wJo1nSQ4m"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def train_with_meta_classifier(X_train, y_train, base_options=['svm', 'random_forest'], meta_option='random_forest', random_state=42):\n","    # Treinamento dos modelos base\n","    n_classes, n_features = X_train.shape\n","\n","    trained_base_models = []\n","    for option in base_options:\n","        model = select_model(option, random_state)\n","        model.fit(X_train, y_train)\n","        trained_base_models.append(model)\n","\n","    # Calcular a acurácia de cada modelo base\n","    accuracies = [accuracy_score(y_train, model.predict(X_train)) for model in trained_base_models]\n","\n","    # Calcular as probabilidades ponderadas para cada classe\n","    weighted_probabilities = np.zeros((len(X_train), n_classes))  # Inicializar matriz para armazenar as probabilidades ponderadas\n","    for model, accuracy in zip(trained_base_models, accuracies):\n","        probas = model.predict_proba(X_train)\n","        weighted_probabilities += probas * accuracy\n","\n","    # Treinamento do meta-classificador usando as probabilidades ponderadas\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(weighted_probabilities, y_train)\n","\n","    return trained_base_models, meta_classifier\n","\n","def predict_with_meta_classifier(X_test, y_test, trained_base_models, trained_meta_classifier):\n","    # Calculando as probabilidades ponderadas para os dados de teste\n","    n_classes, n_features = X_test.shape\n","    weighted_probabilities = np.zeros((len(X_test), n_classes))\n","    for model in trained_base_models:\n","        probas = model.predict_proba(X_test)\n","        accuracy = accuracy_score(y_test, model.predict(X_test))\n","        weighted_probabilities += probas * accuracy\n","\n","    # Previsões usando o meta-classificador\n","    predictions = trained_meta_classifier.predict(weighted_probabilities)\n","\n","    return predictions\n"]},{"cell_type":"code","execution_count":157,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709317735527,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"JQnLpU8lKAFq","outputId":"dfb99416-bc14-490a-e9bb-0d6a72ead948"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"accuracy_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Dataset Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Beef\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5,\n        \"max\": 0.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"accuracy_df"},"text/html":["\n","  <div id=\"df-91258f84-28cd-4a1c-9cab-1d6c4f84bccf\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dataset Name</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Beef</td>\n","      <td>0.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91258f84-28cd-4a1c-9cab-1d6c4f84bccf')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-91258f84-28cd-4a1c-9cab-1d6c4f84bccf button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-91258f84-28cd-4a1c-9cab-1d6c4f84bccf');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_3cb1117d-e618-41a3-bfc8-17bdf76c9d2b\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('accuracy_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_3cb1117d-e618-41a3-bfc8-17bdf76c9d2b button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('accuracy_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["  Dataset Name  Accuracy\n","0         Beef       0.5"]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["accuracy_df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1709299974549,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"xXGPPjgI3-Wx"},"outputs":[],"source":["accuracy_df.to_parquet('model_acc_Hipotese_TDV3.parquet', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUBEabscEDYM"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNHo37X+eCoTDynQiLIQ4jE","collapsed_sections":["quMKuQeJ2vAM","cGdprxmE28DV"],"provenance":[{"file_id":"1qHuOQQ-weaYvbAkO8AQmskHGp7IBOwQj","timestamp":1709242331528}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
