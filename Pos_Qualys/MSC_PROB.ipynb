{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n%pip install aeon\\n%pip install tsfresh\\n%pip install tslearn\\n%pip install tensorflow\\n%pip install keras\\n%pip install pywavelets'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier\n",
        "\n",
        "from tsfresh import extract_features, select_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn import pipeline\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit\n",
        "import timeit\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Function transform data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_data_math(X, wavelet):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "    \n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    \n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "\n",
        "    # Original Data\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "\n",
        "    # FFT Data\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "\n",
        "    # DWT Data\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "\n",
        "    # Adding IDs to DataFrames\n",
        "    df_X['id'] = df_FFT['id'] = df_DWT['id'] = df_PAA['id'] = df_SAX['id'] = range(len(df_X))\n",
        "    \n",
        "    # Merging all DataFrames on 'id'\n",
        "    final_df = df_X.merge(df_FFT, on='id', suffixes=('_X', '_FFT'))\n",
        "    final_df = final_df.merge(df_DWT, on='id', suffixes=('', '_DWT'))\n",
        "    final_df = final_df.merge(df_PAA, on='id', suffixes=('', '_PAA'))\n",
        "    final_df = final_df.merge(df_SAX, on='id', suffixes=('', '_SAX'))\n",
        "    \n",
        "    \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### AmazonForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.meta_clf = RidgeClassifierCV(alphas=np.logspace(-3,3,10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "        train_preds = [clf.predict(X) for clf in self.classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier(X_train, y_train, wavelet=None):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet=wavelet)  # Transformar todo o conjunto de treino\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in X_train_transformed.items():\n",
        "        model = CombinedDecisionForest()\n",
        "        model.fit(X_trans, y_train)\n",
        "        trained_models[rep] = model\n",
        "        \n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, model in trained_models.items():\n",
        "            proba = model.predict_proba(np.array(X_train_transformed[rep].iloc[i,:]).reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "    \n",
        "    meta_features = np.array(meta_features)\n",
        "    \n",
        "    meta_classifier = RidgeClassifierCV(np.logspace(-3,3,10))\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "    \n",
        "    return trained_models, meta_classifier\n",
        "\n",
        "def predict_classifier(X_test, trained_base_model, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet=wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, X_trans in x_transformed.items():\n",
        "            proba = trained_base_model[rep].predict_proba(X_trans.values.reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())\n",
        "\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X, teste_X, y_train, y_test = load_data(\"Beef\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tsfresh import extract_features\n",
        "\n",
        "def extract_tsfresh_features(df):\n",
        "    extracted_features = extract_features(df, default_fc_parameters=MinimalFCParameters(), column_id='id')\n",
        "    \n",
        "    return extracted_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "best_wl = choose_wavelet(train_X)\n",
        "features_train = transform_data_math(train_X, best_wl)\n",
        "meta_X = extract_tsfresh_features(features_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.58it/s]\n"
          ]
        }
      ],
      "source": [
        "#Testing\n",
        "features_test = transform_data_math(teste_X, best_wl)\n",
        "meta_ts = extract_tsfresh_features(features_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9\n"
          ]
        }
      ],
      "source": [
        "#Original\n",
        "clf = CombinedDecisionForest()\n",
        "clf.fit(meta_X, y_train)\n",
        "y_hat = clf.predict(meta_ts)\n",
        "accuracy2 = accuracy_score(y_test, y_hat)\n",
        "print(f\"Accuracy: {accuracy2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Meta-Classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedMetaClassifier:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.meta_clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Treinar classificadores base\n",
        "        base_predictions = []\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "            preds = clf.predict(X)\n",
        "            base_predictions.append(preds.reshape(-1, 1))\n",
        "        \n",
        "        # Stack predictions para criar meta features\n",
        "        meta_features = np.hstack(base_predictions)\n",
        "        \n",
        "        # Treinar meta-classificador\n",
        "        self.meta_clf.fit(meta_features, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Obter previsões dos classificadores base\n",
        "        base_predictions = []\n",
        "        for clf in self.classifiers:\n",
        "            preds = clf.predict(X)\n",
        "            base_predictions.append(preds.reshape(-1, 1))\n",
        "        \n",
        "        # Stack predictions para criar meta features\n",
        "        meta_features = np.hstack(base_predictions)\n",
        "        \n",
        "        # Previsão final usando o meta-classificador\n",
        "        return self.meta_clf.predict(meta_features)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "                base_probabilities.append(probs)\n",
        "            else:\n",
        "                # Converter previsões para probabilidades (caso o classificador não suporte predict_proba)\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, len(np.unique(preds))))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "                base_probabilities.append(probs)\n",
        "        \n",
        "        return np.argmax(base_probabilities, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_classifier = CombinedMetaClassifier()\n",
        "model_classifier.fit(meta_X, y_train)\n",
        "y_hat = model_classifier.predict_proba(meta_ts)\n",
        "accuracy = accuracy_score(y_test, y_hat)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Meta-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedMetaLearning:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.meta_clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Treinar classificadores base\n",
        "        base_predictions = []\n",
        "        accuracies = []\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "            preds = cross_val_predict(clf, X, y, cv=5, method='predict')\n",
        "            base_predictions.append(preds.reshape(-1, 1))\n",
        "            accuracies.append(accuracy_score(y, preds))\n",
        "        \n",
        "        # Stack predictions para criar meta features\n",
        "        meta_features = np.hstack(base_predictions)\n",
        "        \n",
        "        # Calcular pesos com base na acurácia\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "        \n",
        "        # Treinar meta-classificador\n",
        "        self.meta_clf.fit(meta_features, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Obter previsões dos classificadores base\n",
        "        base_predictions = []\n",
        "        for clf in self.classifiers:\n",
        "            preds = clf.predict(X)\n",
        "            base_predictions.append(preds.reshape(-1, 1))\n",
        "        \n",
        "        # Stack predictions para criar meta features\n",
        "        meta_features = np.hstack(base_predictions)\n",
        "        \n",
        "        # Previsão final usando o meta-classificador\n",
        "        return self.meta_clf.predict(meta_features)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "                base_probabilities.append(probs)\n",
        "            else:\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, len(np.unique(preds))))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "                base_probabilities.append(probs)\n",
        "        \n",
        "        # Média ponderada das probabilidades dos classificadores base\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(base_probabilities, self.clf_weights)], axis=0)\n",
        "        \n",
        "        return combined_probs\n",
        "\n",
        "    def predict_with_argmax(self, X):\n",
        "        combined_probs = self.predict_proba(X)\n",
        "        return np.argmax(combined_probs, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[150], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model_learning\u001b[38;5;241m.\u001b[39mfit(meta_X, y_train)\n\u001b[0;32m      3\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model_learning\u001b[38;5;241m.\u001b[39mpredict_proba(meta_ts)\n\u001b[1;32m----> 4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_hat)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    202\u001b[0m     )\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:95\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     92\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     97\u001b[0m             type_true, type_pred\n\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    102\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
            "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
          ]
        }
      ],
      "source": [
        "model_learning = CombinedMetaLearning()\n",
        "model_learning.fit(meta_X, y_train)\n",
        "y_hat = model_learning.predict_proba(meta_ts)\n",
        "accuracy = accuracy_score(y_test, y_hat)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
