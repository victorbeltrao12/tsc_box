{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier, DrCIFClassifier\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from aeon.classification.base import BaseClassifier\n",
        "\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from scipy.fftpack import fft\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Function transform data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_data_math(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "    \n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    \n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "\n",
        "    # Original Data\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "\n",
        "    # FFT Data\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "\n",
        "    # DWT Data\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "\n",
        "    # Adding IDs to DataFrames\n",
        "    df_X['id'] = df_FFT['id'] = df_DWT['id'] = df_PAA['id'] = df_SAX['id'] = range(len(df_X))\n",
        "    \n",
        "    # Merging all DataFrames on 'id'\n",
        "    final_df = df_X.merge(df_FFT, on='id', suffixes=('_X', '_FFT'))\n",
        "    final_df = final_df.merge(df_DWT, on='id', suffixes=('', '_DWT'))\n",
        "    final_df = final_df.merge(df_PAA, on='id', suffixes=('', '_PAA'))\n",
        "    final_df = final_df.merge(df_SAX, on='id', suffixes=('', '_SAX'))\n",
        "    \n",
        "    \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### AmazonForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.meta_clf = RidgeClassifierCV(alphas=np.logspace(-3,3,10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "        train_preds = [clf.predict(X) for clf in self.classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier(X_train, y_train, wavelet=None):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet=wavelet)  # Transformar todo o conjunto de treino\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in X_train_transformed.items():\n",
        "        model = CombinedDecisionForest()\n",
        "        model.fit(X_trans, y_train)\n",
        "        trained_models[rep] = model\n",
        "        \n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, model in trained_models.items():\n",
        "            proba = model.predict_proba(np.array(X_train_transformed[rep].iloc[i,:]).reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "    \n",
        "    meta_features = np.array(meta_features)\n",
        "    \n",
        "    meta_classifier = RidgeClassifierCV(np.logspace(-3,3,10))\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "    \n",
        "    return trained_models, meta_classifier\n",
        "\n",
        "def predict_classifier(X_test, trained_base_model, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet=wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, X_trans in x_transformed.items():\n",
        "            proba = trained_base_model[rep].predict_proba(X_trans.values.reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())\n",
        "\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Normal Version\n",
        "def apply_pooling_operations(data, pooling_funcs):\n",
        "    pooled_features = []\n",
        "    for func in pooling_funcs:\n",
        "        pooled_features.append(func(data, axis=1))\n",
        "    return np.hstack(pooled_features)\n",
        "\n",
        "def transform_series(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    all_transformed_features = []\n",
        "\n",
        "    for series in X:\n",
        "        series = series.reshape(1, -1)  # Ensure series is 2D for transformations\n",
        "\n",
        "        # FFT Transformation\n",
        "        X_fft = np.abs(fft(series, axis=1))\n",
        "\n",
        "        # DWT Transformation\n",
        "        coeffs_cA, coeffs_cD = pywt.dwt(series, wavelet=wavelet, axis=1, mode='constant')\n",
        "        X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "        # PAA Transformation\n",
        "        paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "        X_paa_ = paa.inverse_transform(paa.fit_transform(series))\n",
        "        X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "\n",
        "        # SAX Transformation\n",
        "        sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "        X_sax_ = sax.inverse_transform(sax.fit_transform(series))\n",
        "        X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "\n",
        "        # Pooling Functions (MAX and PPV)\n",
        "        pooling_funcs = [\n",
        "            np.max,\n",
        "            np.min,\n",
        "            np.mean,\n",
        "            np.median,\n",
        "            np.std,\n",
        "            lambda x, axis: np.percentile(x, 90, axis=axis)\n",
        "    ]\n",
        "\n",
        "        # Apply Pooling Operations\n",
        "        pooled_X = apply_pooling_operations(series, pooling_funcs)\n",
        "        pooled_fft = apply_pooling_operations(X_fft, pooling_funcs)\n",
        "        pooled_dwt = apply_pooling_operations(X_dwt, pooling_funcs)\n",
        "        pooled_paa = apply_pooling_operations(X_paa, pooling_funcs)\n",
        "        pooled_sax = apply_pooling_operations(X_sax, pooling_funcs)\n",
        "\n",
        "        # Concatenate Features\n",
        "        concatenated_features = np.hstack((pooled_X, pooled_fft, pooled_dwt, pooled_paa, pooled_sax))\n",
        "        all_transformed_features.append(concatenated_features)\n",
        "\n",
        "    return np.array(all_transformed_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aeon.classification.shapelet_based import ShapeletTransformClassifier\n",
        "from aeon.classification.dictionary_based import ContractableBOSS\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = ContractableBOSS()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = ShapeletTransformClassifier()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Use cross-validation to determine the weights of each classifier\n",
        "        accuracies = []\n",
        "        for clf in self.classifiers:\n",
        "            scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
        "            accuracies.append(np.mean(scores))\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, target_train, target_test = load_data('StarLightCurves')\n",
        "\n",
        "#Training\n",
        "best_wl = choose_wavelet(X_train)\n",
        "features_train = transform_series(X_train, best_wl)\n",
        "#Testing\n",
        "features_test = transform_series(X_test, best_wl)\n",
        "\n",
        "feature_extractor = CombinedDecisionForest()\n",
        "feature_extractor.fit(features_train, target_train)\n",
        "    \n",
        "train_features = feature_extractor.predict_proba(features_train)\n",
        "test_features = feature_extractor.predict_proba(features_test)\n",
        "    \n",
        "meta_model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    \n",
        "meta_model.fit(train_features, target_train)\n",
        "    \n",
        "predictions = meta_model.predict(test_features)\n",
        "    \n",
        "accuracy = accuracy_score(target_test, predictions)\n",
        "\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "select = ['Beef','Car','CBF','Coffee','DiatomSizeReduction','ECG200','ECGFiveDays','FaceFour','GunPoint','Lightning2','Lightning7','MedicalImages','MoteStrain','OliveOil','SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'Trace', 'SyntheticControl','TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "accuracy_data = []\n",
        "for dataset_name in select:\n",
        "    X_train, X_test, target_train, target_test = load_data(dataset_name)\n",
        "    \n",
        "    #Training\n",
        "    best_wl = choose_wavelet(X_train)\n",
        "    features_train = transform_series(X_train, best_wl)\n",
        "    #Testing\n",
        "    features_test = transform_series(X_test, best_wl)\n",
        "    \n",
        "    feature_extractor = CombinedDecisionForest()\n",
        "    feature_extractor.fit(features_train, target_train)\n",
        "        \n",
        "    train_features = feature_extractor.predict_proba(features_train)\n",
        "    test_features = feature_extractor.predict_proba(features_test)\n",
        "        \n",
        "    meta_model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "\n",
        "    meta_model.fit(train_features, target_train)\n",
        "        \n",
        "    predictions = meta_model.predict(test_features)\n",
        "        \n",
        "    accuracy = accuracy_score(target_test, predictions)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Meta-Classificador TUNADO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "class CombinedMetaClassifier:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.param_grids = [\n",
        "            {\n",
        "                \"C\":[0.01, 0.1, 1, 10, 100, 1000],\n",
        "                \"kernel\":['linear','rbf','poly','sigmoid'],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.clf1 = SVC(probability=True)\n",
        "        self.clf2 = SVC(probability=True)\n",
        "        self.clf3 = SVC(probability=True)\n",
        "        self.clf4 = SVC(probability=True)\n",
        "        self.meta_clf = RidgeClassifierCV(np.logspace(-3,3,10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Treinar classificadores base com GridSearchCV\n",
        "        best_classifiers = []\n",
        "        for clf, param_grid in zip(self.classifiers, self.param_grids):\n",
        "            grid_search = GridSearchCV(clf, param_grid, cv=3, n_jobs=2)\n",
        "            grid_search.fit(X, y)\n",
        "            best_classifiers.append(grid_search.best_estimator_)\n",
        "            print(f'Best parameters for {clf}: {grid_search.best_params_}')\n",
        "\n",
        "        self.classifiers = best_classifiers\n",
        "\n",
        "        # Obter probabilidades dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, len(np.unique(y))))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Treinar meta-classificador\n",
        "        self.meta_clf.fit(meta_features, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                # Converter previsões para probabilidades (caso o classificador não suporte predict_proba)\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, clf.n_classes_))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Previsão final usando o meta-classificador\n",
        "        return self.meta_clf.predict(meta_features)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                # Converter previsões para probabilidades (caso o classificador não suporte predict_proba)\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, clf.n_classes_))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Previsão probabilística final usando o meta-classificador\n",
        "        return self.meta_clf.predict_proba(meta_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "select = ['Adiac', 'Beef','Car','CBF','Coffee','DiatomSizeReduction','ECG200','ECGFiveDays','FaceFour','GunPoint','Lightning2','Lightning7','MedicalImages','MoteStrain','OliveOil','SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'Trace', 'SyntheticControl','TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_data = []\n",
        "for dataset_name in select:\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    #Training\n",
        "    best_wl = choose_wavelet(features_train)\n",
        "    features_train = transform_data_math(features_train, best_wl)\n",
        "    meta_X = extract_tsfresh_features(features_train)\n",
        "\n",
        "    #Testing\n",
        "    features_test = transform_data_math(features_test, best_wl)\n",
        "    meta_ts = extract_tsfresh_features(features_test)\n",
        "    \n",
        "    model_classifier = CombinedMetaClassifier()\n",
        "    model_classifier.fit(meta_X, target_train)\n",
        "    y_hat = model_classifier.predict(meta_ts)\n",
        "    accuracy = accuracy_score(target_test, y_hat)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df_extf = pd.DataFrame(accuracy_data)\n",
        "accuracy_df_extf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_classifier = CombinedMetaClassifier()\n",
        "model_classifier.fit(meta_X, y_train)\n",
        "y_hat = model_classifier.predict(meta_ts)\n",
        "accuracy = accuracy_score(y_test, y_hat)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Meta-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_ensemble_prediction(X, model):\n",
        "    # Generate meta-features for visualization\n",
        "    meta_features = np.column_stack([clf.predict_proba(X) for clf in model.base_classifiers])\n",
        "    meta_predictions = model.meta_clf.predict_proba(meta_features)\n",
        "    \n",
        "    # Criar um DataFrame para visualização\n",
        "    df_meta = pd.DataFrame(meta_features, columns=[f'Prob Class {i+1} from Clf{j+1}' \n",
        "                                                   for j in range(len(model.base_classifiers)) \n",
        "                                                   for i in range(meta_features.shape[1] // len(model.base_classifiers))])\n",
        "    df_meta['Meta Prediction Prob Class 1'] = meta_predictions[:, 0]\n",
        "    df_meta['Meta Prediction Prob Class 2'] = meta_predictions[:, 1]\n",
        "    df_meta['Meta Prediction'] = np.argmax(meta_predictions, axis=1) + 1  # Adicionar 1 para coincidir com a classe 1-indexada\n",
        "    \n",
        "    display(df_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de uso:\n",
        "visualize_ensemble_prediction(meta_ts, model_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### USING CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, ReLU, concatenate, Flatten, Dense, Add\n",
        "from tensorflow.keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_cnn(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    y_train = le.fit_transform(y_train)\n",
        "    y_test = le.transform(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_pooling_operations_vectorized(data, pooling_funcs):\n",
        "    pooled_features = []\n",
        "    for func in pooling_funcs:\n",
        "        if isinstance(func, tuple):\n",
        "            func, q = func\n",
        "            pooled_features.append(func(data, q, axis=-1))\n",
        "        else:\n",
        "            pooled_features.append(func(data, axis=-1))\n",
        "    return np.concatenate(pooled_features, axis=-1)\n",
        "\n",
        "def transform_series_cnn(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[-1] / 4)\n",
        "    n_paa_segments = int(X.shape[-1] / 4)\n",
        "\n",
        "    pooling_funcs = [\n",
        "        np.max,\n",
        "        np.min,\n",
        "        np.mean,\n",
        "        np.median,\n",
        "        np.std,\n",
        "        (np.percentile, 90)\n",
        "    ]\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=-1))\n",
        "\n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=-1, mode='constant')\n",
        "    X_dwt = np.concatenate((coeffs_cA, coeffs_cD), axis=-1)\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "\n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "\n",
        "    # Apply Pooling Operations\n",
        "    pooled_X = apply_pooling_operations_vectorized(X, pooling_funcs)\n",
        "    pooled_fft = apply_pooling_operations_vectorized(X_fft, pooling_funcs)\n",
        "    pooled_dwt = apply_pooling_operations_vectorized(X_dwt, pooling_funcs)\n",
        "    pooled_paa = apply_pooling_operations_vectorized(X_paa_, pooling_funcs)\n",
        "    pooled_sax = apply_pooling_operations_vectorized(X_sax_, pooling_funcs)\n",
        "\n",
        "    # Add an extra dimension to arrays that have only 1 dimension\n",
        "    pooled_X = np.expand_dims(pooled_X, axis=-1)\n",
        "    pooled_fft = np.expand_dims(pooled_fft, axis=-1)\n",
        "    pooled_dwt = np.expand_dims(pooled_dwt, axis=-1)\n",
        "    pooled_paa = np.expand_dims(pooled_paa, axis=-1)\n",
        "    pooled_sax = np.expand_dims(pooled_sax, axis=-1)\n",
        "\n",
        "    # Concatenate Features\n",
        "    concatenated_features = np.concatenate((pooled_X, pooled_fft, pooled_dwt, pooled_paa, pooled_sax), axis=-1)\n",
        "\n",
        "    return concatenated_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cnn_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # First convolutional block\n",
        "    conv1 = Conv1D(filters=32, kernel_size=2, activation='relu')(input_layer)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "    norm1 = BatchNormalization()(pool1)\n",
        "\n",
        "    # Second convolutional block\n",
        "    conv2 = Conv1D(filters=32, kernel_size=2, activation='relu')(input_layer)\n",
        "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "    norm2 = BatchNormalization()(pool2)\n",
        "\n",
        "    # Third convolutional block\n",
        "    conv3 = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
        "    pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
        "    norm3 = BatchNormalization()(pool3)\n",
        "\n",
        "    # Concatenate all features\n",
        "    concatenated = concatenate([norm1, norm2, norm3], axis=-1)\n",
        "\n",
        "    # Fully connected layers\n",
        "    flat = Flatten()(concatenated)\n",
        "    dense1 = Dense(units=64, activation='relu')(flat)\n",
        "    output_layer = Dense(units=1, activation='softmax')(dense1)  # Saída binária\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data\n",
        "X_train, X_test, target_train, target_test = load_data_cnn('CBF')\n",
        "\n",
        "#Training\n",
        "best_wl = choose_wavelet(X_train)\n",
        "features_train = transform_series_cnn(X_train, best_wl)\n",
        "\n",
        "#Testing\n",
        "features_test = transform_series_cnn(X_test, best_wl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape = features_train.shape[1:]\n",
        "model_classifier = cnn_model(input_shape)\n",
        "\n",
        "model_classifier.fit(features_train, target_train, epochs=20, batch_size=2)\n",
        "y_hat = model_classifier.predict(features_test)\n",
        "accuracy = accuracy_score(target_test, y_hat)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_transformations(X):\n",
        "    n_sax_symbols = int(X.shape[-1] / 4)\n",
        "    n_paa_segments = int(X.shape[-1] / 4)\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=-1))\n",
        "\n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet='db1', axis=-1, mode='constant')\n",
        "    X_dwt = np.concatenate((coeffs_cA, coeffs_cD), axis=-1)\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa = paa.inverse_transform(paa.fit_transform(X))\n",
        "\n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax = sax.inverse_transform(sax.fit_transform(X))\n",
        "\n",
        "    return [X_fft, X_dwt, X_paa, X_sax]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aeon.classification.deep_learning import IndividualInceptionClassifier\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_individual_classifiers(X_train, y_train, X_val, y_val):\n",
        "    classifiers = []\n",
        "    val_predictions = []\n",
        "    \n",
        "    transformed_X_train = apply_transformations(X_train)\n",
        "    transformed_X_val = apply_transformations(X_val)\n",
        "\n",
        "    for X_trans, x_val_trans in zip(transformed_X_train, transformed_X_val):\n",
        "        clf = IndividualInceptionClassifier(n_epochs=1500, batch_size=64, verbose=False, save_best_model=True)\n",
        "        clf.fit(X_trans, y_train)\n",
        "        classifiers.append(clf)\n",
        "        \n",
        "        # Prever no conjunto de validação\n",
        "        val_pred = clf.predict(x_val_trans)\n",
        "        val_predictions.append(val_pred)\n",
        "\n",
        "    # Converter lista de previsões para um array numpy\n",
        "    val_predictions = np.array(val_predictions).T  # Transpor para que cada linha corresponda a um exemplo\n",
        "\n",
        "    return classifiers, val_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "\n",
        "def train_meta_classifier(X_test, y_test, classifiers):\n",
        "    # Coletar previsões no conjunto de teste para treinar o meta-classificador\n",
        "    transformed_X_test = apply_transformations(X_test)\n",
        "\n",
        "    test_predictions = []\n",
        "    for clf, X_test_trans in zip(classifiers, transformed_X_test):\n",
        "        test_pred = clf.predict(X_test_trans)\n",
        "        test_predictions.append(test_pred)\n",
        "\n",
        "    # Converter lista de previsões para um array numpy\n",
        "    test_predictions = np.array(test_predictions).T  # Transpor para que cada linha corresponda a um exemplo\n",
        "\n",
        "    # Treinar o meta-classificador\n",
        "    meta_clf = RidgeClassifierCV()\n",
        "    meta_clf.fit(test_predictions, y_test)\n",
        "\n",
        "    # Prever com o meta-classificador\n",
        "    final_predictions = meta_clf.predict(test_predictions)\n",
        "\n",
        "    return final_predictions, meta_clf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data\n",
        "X_train, X_test, target_train, target_test = load_data_cnn('CBF')\n",
        "\n",
        "#Training\n",
        "#best_wl = choose_wavelet(X_train)\n",
        "features_train = apply_transformations(X_train)\n",
        "\n",
        "#Testing\n",
        "features_test = apply_transformations(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aeon.datasets import load_arrow_head\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Carregar dados de exemplo\n",
        "X, y = load_arrow_head(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinar os classificadores individuais\n",
        "classifiers, val_predictions = train_individual_classifiers(X_tr, y_tr, X_val, y_val)\n",
        "\n",
        "# Treinar e avaliar o meta-classificador\n",
        "final_predictions, meta_clf = train_meta_classifier(X_test, y_test, classifiers)\n",
        "\n",
        "# Avaliar o desempenho\n",
        "accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
