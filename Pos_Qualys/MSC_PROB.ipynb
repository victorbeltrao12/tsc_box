{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier, DrCIFClassifier\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from aeon.classification.base import BaseClassifier\n",
        "\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from scipy.fftpack import fft\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Function transform data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_data_math(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "    \n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    \n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "\n",
        "    # Original Data\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "\n",
        "    # FFT Data\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "\n",
        "    # DWT Data\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "\n",
        "    # Adding IDs to DataFrames\n",
        "    df_X['id'] = df_FFT['id'] = df_DWT['id'] = df_PAA['id'] = df_SAX['id'] = range(len(df_X))\n",
        "    \n",
        "    # Merging all DataFrames on 'id'\n",
        "    final_df = df_X.merge(df_FFT, on='id', suffixes=('_X', '_FFT'))\n",
        "    final_df = final_df.merge(df_DWT, on='id', suffixes=('', '_DWT'))\n",
        "    final_df = final_df.merge(df_PAA, on='id', suffixes=('', '_PAA'))\n",
        "    final_df = final_df.merge(df_SAX, on='id', suffixes=('', '_SAX'))\n",
        "    \n",
        "    \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### AmazonForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.meta_clf = RidgeClassifierCV(alphas=np.logspace(-3,3,10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "        train_preds = [clf.predict(X) for clf in self.classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier(X_train, y_train, wavelet=None):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet=wavelet)  # Transformar todo o conjunto de treino\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in X_train_transformed.items():\n",
        "        model = CombinedDecisionForest()\n",
        "        model.fit(X_trans, y_train)\n",
        "        trained_models[rep] = model\n",
        "        \n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, model in trained_models.items():\n",
        "            proba = model.predict_proba(np.array(X_train_transformed[rep].iloc[i,:]).reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "    \n",
        "    meta_features = np.array(meta_features)\n",
        "    \n",
        "    meta_classifier = RidgeClassifierCV(np.logspace(-3,3,10))\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "    \n",
        "    return trained_models, meta_classifier\n",
        "\n",
        "def predict_classifier(X_test, trained_base_model, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet=wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, X_trans in x_transformed.items():\n",
        "            proba = trained_base_model[rep].predict_proba(X_trans.values.reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())\n",
        "\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Normal Version\n",
        "def apply_pooling_operations(data, pooling_funcs):\n",
        "    pooled_features = []\n",
        "    for func in pooling_funcs:\n",
        "        pooled_features.append(func(data, axis=1))\n",
        "    return np.hstack(pooled_features)\n",
        "\n",
        "def transform_series(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    all_transformed_features = []\n",
        "\n",
        "    for series in X:\n",
        "        series = series.reshape(1, -1)  # Ensure series is 2D for transformations\n",
        "\n",
        "        # FFT Transformation\n",
        "        X_fft = np.abs(fft(series, axis=1))\n",
        "\n",
        "        # DWT Transformation\n",
        "        coeffs_cA, coeffs_cD = pywt.dwt(series, wavelet=wavelet, axis=1, mode='constant')\n",
        "        X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "        # PAA Transformation\n",
        "        paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "        X_paa_ = paa.inverse_transform(paa.fit_transform(series))\n",
        "        X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "\n",
        "        # SAX Transformation\n",
        "        sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "        X_sax_ = sax.inverse_transform(sax.fit_transform(series))\n",
        "        X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "\n",
        "        # Pooling Functions (MAX and PPV)\n",
        "        pooling_funcs = [\n",
        "            np.max,\n",
        "            np.min,\n",
        "            np.mean,\n",
        "            np.median,\n",
        "            np.std,\n",
        "            lambda x, axis: np.percentile(x, 90, axis=axis)\n",
        "    ]\n",
        "\n",
        "        # Apply Pooling Operations\n",
        "        pooled_X = apply_pooling_operations(series, pooling_funcs)\n",
        "        pooled_fft = apply_pooling_operations(X_fft, pooling_funcs)\n",
        "        pooled_dwt = apply_pooling_operations(X_dwt, pooling_funcs)\n",
        "        pooled_paa = apply_pooling_operations(X_paa, pooling_funcs)\n",
        "        pooled_sax = apply_pooling_operations(X_sax, pooling_funcs)\n",
        "\n",
        "        # Concatenate Features\n",
        "        concatenated_features = np.hstack((pooled_X, pooled_fft, pooled_dwt, pooled_paa, pooled_sax))\n",
        "        all_transformed_features.append(concatenated_features)\n",
        "\n",
        "    return np.array(all_transformed_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aeon.classification.shapelet_based import ShapeletTransformClassifier\n",
        "from aeon.classification.dictionary_based import ContractableBOSS\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = ContractableBOSS()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = ShapeletTransformClassifier()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Use cross-validation to determine the weights of each classifier\n",
        "        accuracies = []\n",
        "        for clf in self.classifiers:\n",
        "            scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
        "            accuracies.append(np.mean(scores))\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, target_train, target_test = load_data('StarLightCurves')\n",
        "\n",
        "#Training\n",
        "best_wl = choose_wavelet(X_train)\n",
        "features_train = transform_series(X_train, best_wl)\n",
        "#Testing\n",
        "features_test = transform_series(X_test, best_wl)\n",
        "\n",
        "feature_extractor = CombinedDecisionForest()\n",
        "feature_extractor.fit(features_train, target_train)\n",
        "    \n",
        "train_features = feature_extractor.predict_proba(features_train)\n",
        "test_features = feature_extractor.predict_proba(features_test)\n",
        "    \n",
        "meta_model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    \n",
        "meta_model.fit(train_features, target_train)\n",
        "    \n",
        "predictions = meta_model.predict(test_features)\n",
        "    \n",
        "accuracy = accuracy_score(target_test, predictions)\n",
        "\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "select = ['Beef','Car','CBF','Coffee','DiatomSizeReduction','ECG200','ECGFiveDays','FaceFour','GunPoint','Lightning2','Lightning7','MedicalImages','MoteStrain','OliveOil','SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'Trace', 'SyntheticControl','TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "accuracy_data = []\n",
        "for dataset_name in select:\n",
        "    X_train, X_test, target_train, target_test = load_data(dataset_name)\n",
        "    \n",
        "    #Training\n",
        "    best_wl = choose_wavelet(X_train)\n",
        "    features_train = transform_series(X_train, best_wl)\n",
        "    #Testing\n",
        "    features_test = transform_series(X_test, best_wl)\n",
        "    \n",
        "    feature_extractor = CombinedDecisionForest()\n",
        "    feature_extractor.fit(features_train, target_train)\n",
        "        \n",
        "    train_features = feature_extractor.predict_proba(features_train)\n",
        "    test_features = feature_extractor.predict_proba(features_test)\n",
        "        \n",
        "    meta_model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "\n",
        "    meta_model.fit(train_features, target_train)\n",
        "        \n",
        "    predictions = meta_model.predict(test_features)\n",
        "        \n",
        "    accuracy = accuracy_score(target_test, predictions)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Meta-Classificador TUNADO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "class CombinedMetaClassifier:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.param_grids = [\n",
        "            {\n",
        "                \"C\":[0.01, 0.1, 1, 10, 100, 1000],\n",
        "                \"kernel\":['linear','rbf','poly','sigmoid'],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.clf1 = SVC(probability=True)\n",
        "        self.clf2 = SVC(probability=True)\n",
        "        self.clf3 = SVC(probability=True)\n",
        "        self.clf4 = SVC(probability=True)\n",
        "        self.meta_clf = RidgeClassifierCV(np.logspace(-3,3,10))\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Treinar classificadores base com GridSearchCV\n",
        "        best_classifiers = []\n",
        "        for clf, param_grid in zip(self.classifiers, self.param_grids):\n",
        "            grid_search = GridSearchCV(clf, param_grid, cv=2, n_jobs=2)\n",
        "            grid_search.fit(X, y)\n",
        "            best_classifiers.append(grid_search.best_estimator_)\n",
        "            print(f'Best parameters for {clf}: {grid_search.best_params_}')\n",
        "\n",
        "        self.classifiers = best_classifiers\n",
        "\n",
        "        # Obter probabilidades dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, len(np.unique(y))))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Treinar meta-classificador\n",
        "        self.meta_clf.fit(meta_features, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                # Converter previsões para probabilidades (caso o classificador não suporte predict_proba)\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, clf.n_classes_))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Previsão final usando o meta-classificador\n",
        "        return self.meta_clf.predict(meta_features)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        # Obter previsões probabilísticas dos classificadores base\n",
        "        base_probabilities = []\n",
        "        for clf in self.classifiers:\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                probs = clf.predict_proba(X)\n",
        "            else:\n",
        "                # Converter previsões para probabilidades (caso o classificador não suporte predict_proba)\n",
        "                preds = clf.predict(X)\n",
        "                probs = np.zeros((preds.size, clf.n_classes_))\n",
        "                probs[np.arange(preds.size), preds] = 1\n",
        "            base_probabilities.append(probs)\n",
        "        \n",
        "        # Stack probabilities para criar meta features\n",
        "        meta_features = np.hstack(base_probabilities)\n",
        "        \n",
        "        # Previsão probabilística final usando o meta-classificador\n",
        "        return self.meta_clf.predict_proba(meta_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tsfresh_features(X):\n",
        "    X = extract_features(X, default_fc_parameters=MinimalFCParameters(), column_id='id')\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 30/30 [00:54<00:00,  1.82s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:57<00:00,  1.90s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:38<00:00,  1.28s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:33<00:00,  1.10s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.33it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.22it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.64it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:13<00:00,  2.26it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.95it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:17<00:00,  1.70it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.64it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.10it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:38<00:00,  5.28s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:52<00:00,  5.74s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:51<00:00,  1.71s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:13<00:00,  2.25it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:16<00:00,  1.83it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:58<00:00,  1.95s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [03:45<00:00,  7.53s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:29<00:00,  2.98s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.74it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.14it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.92it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.05it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:15<00:00,  1.90it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.45it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:38<00:00,  3.29s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:37<00:00,  3.26s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.03it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.59it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:20<00:00,  1.48it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:57<00:00,  3.92s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:48<00:00,  1.61s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:48<00:00,  1.61s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:32<00:00,  3.08s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:13<00:00,  2.30it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:13<00:00,  2.25it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.45it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.53it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:47<00:00,  1.58s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:41<00:00,  3.39s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:11<00:00,  2.71it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [04:57<00:00,  9.92s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:20<00:00,  1.45it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.14it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:57<00:00,  1.91s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [05:06<00:00, 10.23s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:22<00:00,  1.34it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.21it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.09it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:40<00:00,  1.34s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.17it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.72it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.10it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  6.00it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:28<00:00,  1.05it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:27<00:00,  1.08it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  4.15it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.52it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.21it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:26<00:00,  1.11it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.66it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.67it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.01it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.99it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.98it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.60it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.62it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.46it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:21<00:00,  1.40it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:42<00:00,  1.40s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.06it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.97it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.19it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:12<00:00,  2.43it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.36it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:05<00:00,  2.19s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.46it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.50it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.56it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  7.96it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:29<00:00,  1.02it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:00<00:00,  4.03s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:16<00:00,  1.77it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:18<00:00,  1.66it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.57it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.12it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  3.94it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.86it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  7.91it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.57it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:28<00:00,  1.06it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:57<00:00,  3.93s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:32<00:00,  1.09s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:15<00:00,  2.53s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.20it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:11<00:00,  2.67it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [04:45<00:00,  9.51s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:39<00:00,  3.30s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:37<00:00,  1.27s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:18<00:00,  1.61it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:14<00:00,  4.50s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:47<00:00,  3.57s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:19<00:00,  1.50it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:20<00:00,  1.46it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [07:26<00:00, 14.88s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:40<00:00,  5.36s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:12<00:00,  2.33it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.69it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.43it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:22<00:00,  1.30it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:11<00:00,  2.68it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [05:07<00:00, 10.27s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:45<00:00,  1.52s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:44<00:00,  1.48s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:18<00:00,  1.66it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [03:06<00:00,  6.23s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.46it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:07<00:00,  4.26s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.84it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.93it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  3.84it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.65it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.22it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:11<00:00,  2.60it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:39<00:00,  1.33s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:37<00:00,  1.24s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.50it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:10<00:00,  2.97it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:17<00:00,  1.68it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:17<00:00,  1.70it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.87it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.79it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:25<00:00,  1.17it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:42<00:00,  1.41s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.90it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.62it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.62it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:04<00:00,  2.13s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.67it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.60it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:08<00:00,  2.29s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:10<00:00,  2.34s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.23it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  6.07it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.88it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.46it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  4.22it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:14<00:00,  2.04it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.46it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.02it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:02<00:00, 10.57it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.81it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.10it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.90it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:02<00:00, 10.39it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:10<00:00,  2.95it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.73it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.67it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.90it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:28<00:00,  2.94s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  4.17it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.73it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:14<00:00,  2.02it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.02it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:28<00:00,  2.95s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [06:50<00:00, 13.68s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:32<00:00,  1.10s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  4.10it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.28it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:23<00:00,  1.28it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.17it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.18it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.75it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [04:19<00:00,  8.66s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:13<00:00,  2.24it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:15<00:00,  1.93it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:15<00:00,  1.88it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:40<00:00,  3.34s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.54it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.30it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  4.13it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [04:32<00:00,  9.07s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:30<00:00,  5.02s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:45<00:00,  5.53s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:07<00:00,  3.91it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:51<00:00,  1.72s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.11it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:11<00:00,  2.67it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.41it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.57it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.20it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:23<00:00,  1.28it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [01:51<00:00,  3.71s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [20:45<00:00, 41.50s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:26<00:00,  1.13it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.36it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.33it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:18<00:00,  1.63it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:05<00:00,  5.29it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:16<00:00,  1.85it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:28<00:00,  1.05it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:28<00:00,  1.05it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.50it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  8.35it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:18<00:00,  1.59it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:18<00:00,  1.61it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.18it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:04<00:00,  7.33it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:22<00:00,  1.36it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:43<00:00,  1.47s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:08<00:00,  3.73it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.10it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:03<00:00,  9.28it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:31<00:00,  1.04s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [02:06<00:00,  4.22s/it]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:06<00:00,  4.92it/s]\n",
            "Feature Extraction: 100%|██████████| 30/30 [00:09<00:00,  3.16it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"TEST\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n",
        "\n",
        "def extract_tsfresh_features(X):\n",
        "    # Cria um dataframe a partir dos dados\n",
        "    df = pd.DataFrame(X)\n",
        "    df['id'] = df.index\n",
        "    \n",
        "    # Extrai as características usando tsfresh\n",
        "    extracted_features = extract_features(df, default_fc_parameters=MinimalFCParameters(), column_id='id')\n",
        "    return extracted_features\n",
        "\n",
        "def save_transformed_data(dataset):\n",
        "    # Carrega os dados\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset)\n",
        "    \n",
        "    # Extrai características dos dados de treino e teste\n",
        "    transformed_train = extract_tsfresh_features(features_train)\n",
        "    transformed_test = extract_tsfresh_features(features_test)\n",
        "    \n",
        "    # Cria a pasta de destino, se não existir\n",
        "    output_dir = './transform_data'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Salva os dados transformados em arquivos CSV\n",
        "    transformed_train.to_csv(os.path.join(output_dir, f'{dataset}_train_features.csv'), index=False)\n",
        "    transformed_test.to_csv(os.path.join(output_dir, f'{dataset}_test_features.csv'), index=False)\n",
        "    \n",
        "    # Salva as labels em arquivos CSV\n",
        "    pd.DataFrame(target_train, columns=['target']).to_csv(os.path.join(output_dir, f'{dataset}_train_labels.csv'), index=False)\n",
        "    pd.DataFrame(target_test, columns=['target']).to_csv(os.path.join(output_dir, f'{dataset}_test_labels.csv'), index=False)\n",
        "\n",
        "for dataset in univariate_equal_length:\n",
        "    save_transformed_data(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "select = ['Adiac', 'Beef','Car','CBF','Coffee','DiatomSizeReduction','ECG200','ECGFiveDays','FaceFour','GunPoint','Lightning2','Lightning7','MedicalImages','MoteStrain','OliveOil','SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'Trace', 'SyntheticControl','TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia EOGVerticalSignal: 0.4005524861878453\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia LargeKitchenAppliances: 0.496\n",
            "Best parameters for SVC(probability=True): {'C': 0.1, 'kernel': 'linear'}\n",
            "Acurácia ItalyPowerDemand: 0.9708454810495627\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia ShapeletSim: 0.49444444444444446\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia ECGFiveDays: 0.9860627177700348\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia Coffee: 1.0\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia NonInvasiveFetalECGThorax1: 0.9475826972010178\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia CricketX: 0.6410256410256411\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia TwoPatterns: 0.972\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia FordA: 0.8348484848484848\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia Plane: 0.9714285714285714\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'rbf'}\n",
            "Acurácia Trace: 0.86\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'rbf'}\n",
            "Acurácia PhalangesOutlinesCorrect: 0.6678321678321678\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'linear'}\n",
            "Acurácia EthanolLevel: 0.78\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'poly'}\n",
            "Acurácia GunPointAgeSpan: 0.8765822784810127\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia InlineSkate: 0.25272727272727274\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'rbf'}\n",
            "Acurácia EOGHorizontalSignal: 0.4116022099447514\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia ElectricDevices: 0.6178187005576449\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia FiftyWords: 0.6747252747252748\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia Lightning7: 0.6164383561643836\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia SemgHandGenderCh2: 0.9283333333333333\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia MixedShapesSmallTrain: 0.8540206185567011\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'poly'}\n",
            "Acurácia Wine: 0.6296296296296297\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'poly'}\n",
            "Acurácia MixedShapesRegularTrain: 0.9142268041237114\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia MiddlePhalanxOutlineAgeGroup: 0.564935064935065\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia Symbols: 0.7618090452261307\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia PowerCons: 0.9888888888888889\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia UMD: 0.8819444444444444\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia RefrigerationDevices: 0.464\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia DistalPhalanxOutlineCorrect: 0.6811594202898551\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia FacesUCR: 0.8195121951219512\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'sigmoid'}\n",
            "Acurácia SonyAIBORobotSurface1: 0.8069883527454242\n",
            "Best parameters for SVC(probability=True): {'C': 0.1, 'kernel': 'linear'}\n",
            "Acurácia BME: 0.9333333333333333\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'sigmoid'}\n",
            "Acurácia FaceFour: 0.8636363636363636\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia MiddlePhalanxTW: 0.5714285714285714\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia PigCVP: 0.15384615384615385\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'rbf'}\n",
            "Acurácia SyntheticControl: 0.9733333333333334\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'sigmoid'}\n",
            "Acurácia CBF: 0.9122222222222223\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'poly'}\n",
            "Acurácia ChlorineConcentration: 0.8908854166666667\n",
            "Best parameters for SVC(probability=True): {'C': 0.1, 'kernel': 'poly'}\n",
            "Acurácia SmoothSubspace: 0.8666666666666667\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia DistalPhalanxOutlineAgeGroup: 0.7122302158273381\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia UWaveGestureLibraryX: 0.7822445561139029\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'poly'}\n",
            "Acurácia ACSF1: 0.67\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia ProximalPhalanxOutlineAgeGroup: 0.8390243902439024\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'rbf'}\n",
            "Acurácia ProximalPhalanxOutlineCorrect: 0.8247422680412371\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia ToeSegmentation1: 0.618421052631579\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia UWaveGestureLibraryZ: 0.7135678391959799\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia Crop: 0.7543452380952381\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia DiatomSizeReduction: 0.9607843137254902\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia FordB: 0.6901234567901234\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'rbf'}\n",
            "Acurácia Earthquakes: 0.7482014388489209\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia SemgHandMovementCh2: 0.6644444444444444\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia CricketZ: 0.5897435897435898\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'rbf'}\n",
            "Acurácia HandOutlines: 0.9162162162162162\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia WormsTwoClass: 0.6103896103896104\n",
            "Best parameters for SVC(probability=True): {'C': 0.1, 'kernel': 'linear'}\n",
            "Acurácia Car: 0.8\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia Worms: 0.5714285714285714\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'sigmoid'}\n",
            "Acurácia Phoneme: 0.11234177215189874\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'poly'}\n",
            "Acurácia ShapesAll: 0.7883333333333333\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia Yoga: 0.8386666666666667\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia FreezerSmallTrain: 0.7007017543859649\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia Beef: 0.8333333333333334\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia Lightning2: 0.7213114754098361\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'rbf'}\n",
            "Acurácia SonyAIBORobotSurface2: 0.8100734522560336\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'sigmoid'}\n",
            "Acurácia SmallKitchenAppliances: 0.43733333333333335\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia GunPointOldVersusYoung: 1.0\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia CricketY: 0.6256410256410256\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'rbf'}\n",
            "Acurácia Herring: 0.59375\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia PigAirwayPressure: 0.038461538461538464\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia GunPointMaleVersusFemale: 0.9936708860759493\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia ECG5000: 0.9393333333333334\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'rbf'}\n",
            "Acurácia MiddlePhalanxOutlineCorrect: 0.6013745704467354\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia SemgHandSubjectCh2: 0.8444444444444444\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia Meat: 0.9666666666666667\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia ProximalPhalanxTW: 0.8\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia Rock: 0.82\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'rbf'}\n",
            "Acurácia BirdChicken: 0.6\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia Chinatown: 0.9620991253644315\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia ToeSegmentation2: 0.7769230769230769\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia TwoLeadECG: 0.9385425812115891\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'linear'}\n",
            "Acurácia Adiac: 0.7340153452685422\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia FreezerRegularTrain: 0.9859649122807017\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia Ham: 0.6571428571428571\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'linear'}\n",
            "Acurácia Strawberry: 0.972972972972973\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia UWaveGestureLibraryAll: 0.9595198213288666\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia ArrowHead: 0.8228571428571428\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia Haptics: 0.461038961038961\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia FaceAll: 0.8597633136094674\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'poly'}\n",
            "Acurácia Fish: 0.8685714285714285\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia CinCECGTorso: 0.7347826086956522\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia InsectEPGSmallTrain: 1.0\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia Wafer: 0.9957819597663855\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia DistalPhalanxTW: 0.6402877697841727\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia Mallat: 0.9552238805970149\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'poly'}\n",
            "Acurácia NonInvasiveFetalECGThorax2: 0.9506361323155216\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'rbf'}\n",
            "Acurácia InsectWingbeatSound: 0.6363636363636364\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia OSULeaf: 0.5619834710743802\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia ECG200: 0.89\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia HouseTwenty: 0.7563025210084033\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia StarLightCurves: 0.9477901894123361\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'linear'}\n",
            "Acurácia GunPoint: 0.9133333333333333\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'rbf'}\n",
            "Acurácia WordSynonyms: 0.6018808777429467\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia InsectEPGRegularTrain: 1.0\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'sigmoid'}\n",
            "Acurácia ScreenType: 0.368\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'sigmoid'}\n",
            "Acurácia BeetleFly: 0.75\n",
            "Best parameters for SVC(probability=True): {'C': 1000, 'kernel': 'sigmoid'}\n",
            "Acurácia Computers: 0.544\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'linear'}\n",
            "Acurácia OliveOil: 0.8333333333333334\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'linear'}\n",
            "Acurácia PigArtPressure: 0.18269230769230768\n",
            "Best parameters for SVC(probability=True): {'C': 10, 'kernel': 'poly'}\n",
            "Acurácia SwedishLeaf: 0.9184\n",
            "Best parameters for SVC(probability=True): {'C': 0.01, 'kernel': 'sigmoid'}\n",
            "Acurácia MoteStrain: 0.5391373801916933\n",
            "Best parameters for SVC(probability=True): {'C': 1, 'kernel': 'poly'}\n",
            "Acurácia UWaveGestureLibraryY: 0.6895589056393077\n",
            "Best parameters for SVC(probability=True): {'C': 100, 'kernel': 'rbf'}\n",
            "Acurácia MedicalImages: 0.7539473684210526\n"
          ]
        }
      ],
      "source": [
        "def load_transformed_data(dataset_name, data_dir='../Pos_Qualys/dransform_data'):\n",
        "    # Carregar os dados de treino e teste a partir dos arquivos CSV\n",
        "    train_features_path = os.path.join(data_dir, f'{dataset_name}_train_features.csv')\n",
        "    test_features_path = os.path.join(data_dir, f'{dataset_name}_test_features.csv')\n",
        "    train_labels_path = os.path.join(data_dir, f'{dataset_name}_train_labels.csv')\n",
        "    test_labels_path = os.path.join(data_dir, f'{dataset_name}_test_labels.csv')\n",
        "    \n",
        "    features_train = pd.read_csv(train_features_path)\n",
        "    features_test = pd.read_csv(test_features_path)\n",
        "    target_train = pd.read_csv(train_labels_path).values.ravel()\n",
        "    target_test = pd.read_csv(test_labels_path).values.ravel()\n",
        "    \n",
        "    return features_train, features_test, target_train, target_test\n",
        "\n",
        "accuracy_data = []\n",
        "for dataset_name in univariate_equal_length:\n",
        "    features_train, features_test, target_train, target_test = load_transformed_data(dataset_name)\n",
        "    \n",
        "    model_classifier = CombinedMetaClassifier()\n",
        "    model_classifier.fit(features_train, target_train)\n",
        "    y_hat = model_classifier.predict(features_test)\n",
        "    accuracy = accuracy_score(target_test, y_hat)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df_extf = pd.DataFrame(accuracy_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df_extf.to_csv(\"df_metaLearningSVC.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df_extf = pd.DataFrame(accuracy_data)\n",
        "accuracy_df_extf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Meta-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_ensemble_prediction(X, model):\n",
        "    # Generate meta-features for visualization\n",
        "    meta_features = np.column_stack([clf.predict_proba(X) for clf in model.base_classifiers])\n",
        "    meta_predictions = model.meta_clf.predict_proba(meta_features)\n",
        "    \n",
        "    # Criar um DataFrame para visualização\n",
        "    df_meta = pd.DataFrame(meta_features, columns=[f'Prob Class {i+1} from Clf{j+1}' \n",
        "                                                   for j in range(len(model.base_classifiers)) \n",
        "                                                   for i in range(meta_features.shape[1] // len(model.base_classifiers))])\n",
        "    df_meta['Meta Prediction Prob Class 1'] = meta_predictions[:, 0]\n",
        "    df_meta['Meta Prediction Prob Class 2'] = meta_predictions[:, 1]\n",
        "    df_meta['Meta Prediction'] = np.argmax(meta_predictions, axis=1) + 1  # Adicionar 1 para coincidir com a classe 1-indexada\n",
        "    \n",
        "    display(df_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de uso:\n",
        "visualize_ensemble_prediction(meta_ts, model_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### USING CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, ReLU, concatenate, Flatten, Dense, Add\n",
        "from tensorflow.keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_cnn(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    y_train = le.fit_transform(y_train)\n",
        "    y_test = le.transform(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_pooling_operations_vectorized(data, pooling_funcs):\n",
        "    pooled_features = []\n",
        "    for func in pooling_funcs:\n",
        "        if isinstance(func, tuple):\n",
        "            func, q = func\n",
        "            pooled_features.append(func(data, q, axis=-1))\n",
        "        else:\n",
        "            pooled_features.append(func(data, axis=-1))\n",
        "    return np.concatenate(pooled_features, axis=-1)\n",
        "\n",
        "def transform_series_cnn(X, wavelet='db1'):\n",
        "    n_sax_symbols = int(X.shape[-1] / 4)\n",
        "    n_paa_segments = int(X.shape[-1] / 4)\n",
        "\n",
        "    pooling_funcs = [\n",
        "        np.max,\n",
        "        np.min,\n",
        "        np.mean,\n",
        "        np.median,\n",
        "        np.std,\n",
        "        (np.percentile, 90)\n",
        "    ]\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=-1))\n",
        "\n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=-1, mode='constant')\n",
        "    X_dwt = np.concatenate((coeffs_cA, coeffs_cD), axis=-1)\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "\n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "\n",
        "    # Apply Pooling Operations\n",
        "    pooled_X = apply_pooling_operations_vectorized(X, pooling_funcs)\n",
        "    pooled_fft = apply_pooling_operations_vectorized(X_fft, pooling_funcs)\n",
        "    pooled_dwt = apply_pooling_operations_vectorized(X_dwt, pooling_funcs)\n",
        "    pooled_paa = apply_pooling_operations_vectorized(X_paa_, pooling_funcs)\n",
        "    pooled_sax = apply_pooling_operations_vectorized(X_sax_, pooling_funcs)\n",
        "\n",
        "    # Add an extra dimension to arrays that have only 1 dimension\n",
        "    pooled_X = np.expand_dims(pooled_X, axis=-1)\n",
        "    pooled_fft = np.expand_dims(pooled_fft, axis=-1)\n",
        "    pooled_dwt = np.expand_dims(pooled_dwt, axis=-1)\n",
        "    pooled_paa = np.expand_dims(pooled_paa, axis=-1)\n",
        "    pooled_sax = np.expand_dims(pooled_sax, axis=-1)\n",
        "\n",
        "    # Concatenate Features\n",
        "    concatenated_features = np.concatenate((pooled_X, pooled_fft, pooled_dwt, pooled_paa, pooled_sax), axis=-1)\n",
        "\n",
        "    return concatenated_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cnn_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # First convolutional block\n",
        "    conv1 = Conv1D(filters=32, kernel_size=2, activation='relu')(input_layer)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "    norm1 = BatchNormalization()(pool1)\n",
        "\n",
        "    # Second convolutional block\n",
        "    conv2 = Conv1D(filters=32, kernel_size=2, activation='relu')(input_layer)\n",
        "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "    norm2 = BatchNormalization()(pool2)\n",
        "\n",
        "    # Third convolutional block\n",
        "    conv3 = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
        "    pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
        "    norm3 = BatchNormalization()(pool3)\n",
        "\n",
        "    # Concatenate all features\n",
        "    concatenated = concatenate([norm1, norm2, norm3], axis=-1)\n",
        "\n",
        "    # Fully connected layers\n",
        "    flat = Flatten()(concatenated)\n",
        "    dense1 = Dense(units=64, activation='relu')(flat)\n",
        "    output_layer = Dense(units=1, activation='softmax')(dense1)  # Saída binária\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data\n",
        "X_train, X_test, target_train, target_test = load_data_cnn('CBF')\n",
        "\n",
        "#Training\n",
        "best_wl = choose_wavelet(X_train)\n",
        "features_train = transform_series_cnn(X_train, best_wl)\n",
        "\n",
        "#Testing\n",
        "features_test = transform_series_cnn(X_test, best_wl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape = features_train.shape[1:]\n",
        "model_classifier = cnn_model(input_shape)\n",
        "\n",
        "model_classifier.fit(features_train, target_train, epochs=20, batch_size=2)\n",
        "y_hat = model_classifier.predict(features_test)\n",
        "accuracy = accuracy_score(target_test, y_hat)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_transformations(X):\n",
        "    n_sax_symbols = int(X.shape[-1] / 4)\n",
        "    n_paa_segments = int(X.shape[-1] / 4)\n",
        "\n",
        "    # FFT Transformation\n",
        "    X_fft = np.abs(fft(X, axis=-1))\n",
        "\n",
        "    # DWT Transformation\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet='db1', axis=-1, mode='constant')\n",
        "    X_dwt = np.concatenate((coeffs_cA, coeffs_cD), axis=-1)\n",
        "\n",
        "    # PAA Transformation\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa = paa.inverse_transform(paa.fit_transform(X))\n",
        "\n",
        "    # SAX Transformation\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax = sax.inverse_transform(sax.fit_transform(X))\n",
        "\n",
        "    return [X_fft, X_dwt, X_paa, X_sax]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aeon.classification.deep_learning import IndividualInceptionClassifier\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_individual_classifiers(X_train, y_train, X_val, y_val):\n",
        "    classifiers = []\n",
        "    val_predictions = []\n",
        "    \n",
        "    transformed_X_train = apply_transformations(X_train)\n",
        "    transformed_X_val = apply_transformations(X_val)\n",
        "\n",
        "    for X_trans, x_val_trans in zip(transformed_X_train, transformed_X_val):\n",
        "        clf = IndividualInceptionClassifier(n_epochs=1500, batch_size=64, verbose=False, save_best_model=True)\n",
        "        clf.fit(X_trans, y_train)\n",
        "        classifiers.append(clf)\n",
        "        \n",
        "        # Prever no conjunto de validação\n",
        "        val_pred = clf.predict(x_val_trans)\n",
        "        val_predictions.append(val_pred)\n",
        "\n",
        "    # Converter lista de previsões para um array numpy\n",
        "    val_predictions = np.array(val_predictions).T  # Transpor para que cada linha corresponda a um exemplo\n",
        "\n",
        "    return classifiers, val_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "\n",
        "def train_meta_classifier(X_test, y_test, classifiers):\n",
        "    # Coletar previsões no conjunto de teste para treinar o meta-classificador\n",
        "    transformed_X_test = apply_transformations(X_test)\n",
        "\n",
        "    test_predictions = []\n",
        "    for clf, X_test_trans in zip(classifiers, transformed_X_test):\n",
        "        test_pred = clf.predict(X_test_trans)\n",
        "        test_predictions.append(test_pred)\n",
        "\n",
        "    # Converter lista de previsões para um array numpy\n",
        "    test_predictions = np.array(test_predictions).T  # Transpor para que cada linha corresponda a um exemplo\n",
        "\n",
        "    # Treinar o meta-classificador\n",
        "    meta_clf = RidgeClassifierCV()\n",
        "    meta_clf.fit(test_predictions, y_test)\n",
        "\n",
        "    # Prever com o meta-classificador\n",
        "    final_predictions = meta_clf.predict(test_predictions)\n",
        "\n",
        "    return final_predictions, meta_clf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data\n",
        "X_train, X_test, target_train, target_test = load_data_cnn('CBF')\n",
        "\n",
        "#Training\n",
        "#best_wl = choose_wavelet(X_train)\n",
        "features_train = apply_transformations(X_train)\n",
        "\n",
        "#Testing\n",
        "features_test = apply_transformations(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aeon.datasets import load_arrow_head\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Carregar dados de exemplo\n",
        "X, y = load_arrow_head(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinar os classificadores individuais\n",
        "classifiers, val_predictions = train_individual_classifiers(X_tr, y_tr, X_val, y_val)\n",
        "\n",
        "# Treinar e avaliar o meta-classificador\n",
        "final_predictions, meta_clf = train_meta_classifier(X_test, y_test, classifiers)\n",
        "\n",
        "# Avaliar o desempenho\n",
        "accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
