{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":689,"status":"ok","timestamp":1709140778883,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"OPQ2CvQ16qSn","outputId":"5cd708b2-cf09-4308-fc38-26c5750556aa"},"outputs":[],"source":["\"\"\"!pip install aeon\n","!pip install sktime\n","!pip install tsfresh\n","!pip install tslearn\n","!pip install PyWavelets\"\"\""]},{"cell_type":"markdown","metadata":{"id":"vb2wJ4B9U2pD"},"source":["### To Do list\n","\n","\n","*   Comparar os resultados do 1NN contra o SVM+RF\n","*   Comparar os resultados dos classificadores Feature Based com o SVM+RF\n","*   Comparar os resultados do MetaClf_Conc contra o MetaClf_Dict\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1709145813413,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"mALblC0a6_9B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from aeon.datasets import load_classification\n","from aeon.datasets.tsc_data_lists import univariate_equal_length\n","from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n","from aeon.classification.convolution_based import RocketClassifier\n","\n","import tsfresh\n","from tsfresh import extract_features, select_features\n","from tsfresh.feature_extraction import MinimalFCParameters\n","\n","from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n","from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n","\n","import os\n","import math\n","import pywt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from scipy.fftpack import fft\n","from scipy.stats import norm\n","\n","from numba import jit, njit\n","\n","from tqdm import tqdm\n","import timeit\n","from datetime import timedelta\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779251,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"YAZlCmp8MSBk","outputId":"b4a64cd9-58b0-4567-a89a-06f231eb79c3"},"outputs":[{"data":{"text/plain":["'# Transform data original\\ndef sax_transform(series, w, a):\\n    paa = [series[i:i + w].mean() for i in range(0, len(series), w)]\\n\\n    if np.std(paa) != 0:\\n        paa = (paa - np.mean(paa)) / np.std(paa)\\n    else:\\n        paa = paa - np.mean(paa)\\n\\n    breakpoints = norm.ppf(np.linspace(0, 1, a+1)[1:-1])\\n    sax_symbols = np.array(range(a))\\n    sax_representation = sax_symbols[np.digitize(paa, breakpoints)]\\n\\n    return sax_representation\\n\\ndef transform_data(X, num_features=10):\\n    a = 5\\n    w = int(X.shape[1] / num_features)  # Ajuste do tamanho da janela baseado no número de características desejado\\n\\n    X_sax = np.array([sax_transform(row, w, a) for row in X])\\n    X_fft = np.abs(fft(X, axis=1))\\n\\n    coeffs_cA, coeffs_cD = pywt.dwt(X, \\'db1\\', axis=1)\\n    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\\n\\n    X_paa = np.column_stack([X[:, i:i+2].mean(axis=1) for i in range(0, X.shape[1], 2)])\\n\\n    return {\\n        \"TS\": X,\\n        \"FFT\": X_fft,\\n        \"DWT\": X_dwt,\\n        \"PAA\": X_paa,\\n        \"SAX\": X_sax\\n    }'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"# Transform data original\n","def sax_transform(series, w, a):\n","    paa = [series[i:i + w].mean() for i in range(0, len(series), w)]\n","\n","    if np.std(paa) != 0:\n","        paa = (paa - np.mean(paa)) / np.std(paa)\n","    else:\n","        paa = paa - np.mean(paa)\n","\n","    breakpoints = norm.ppf(np.linspace(0, 1, a+1)[1:-1])\n","    sax_symbols = np.array(range(a))\n","    sax_representation = sax_symbols[np.digitize(paa, breakpoints)]\n","\n","    return sax_representation\n","\n","def transform_data(X, num_features=10):\n","    a = 5\n","    w = int(X.shape[1] / num_features)  # Ajuste do tamanho da janela baseado no número de características desejado\n","\n","    X_sax = np.array([sax_transform(row, w, a) for row in X])\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    X_paa = np.column_stack([X[:, i:i+2].mean(axis=1) for i in range(0, X.shape[1], 2)])\n","\n","    return {\n","        \"TS\": X,\n","        \"FFT\": X_fft,\n","        \"DWT\": X_dwt,\n","        \"PAA\": X_paa,\n","        \"SAX\": X_sax\n","    }\"\"\""]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779252,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"InL-RPCHCjWM"},"outputs":[],"source":["# Transform data using TimeSeriesScalerMeanVariance and concatenate all transformed data\n","def transform_data(X, num_features=10):\n","    n_sax_symbols = int(X.shape[1] / num_features)\n","    n_paa_segments = int(X.shape[1] / num_features)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","\n","    return data_concat"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779252,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"Qf4ep4cqMNh1","outputId":"64378fcc-6f82-4743-fe2c-f5bcde504dbc"},"outputs":[{"data":{"text/plain":["\"# Transform data using Extract and Select features\\ndef transform_data(X, num_features=10):\\n    n_sax_symbols = int(X.shape[1] / num_features)\\n    n_paa_segments = int(X.shape[1] / num_features)\\n\\n    X_fft = np.abs(fft(X, axis=1))\\n\\n    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\\n    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\\n\\n    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\\n    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\\n    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\\n\\n    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\\n    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\\n    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\\n\\n    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\\n    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\\n    data_concat.resize(data.shape[0], data.shape[1])\\n\\n    transformed_data = pd.DataFrame(data_concat)\\n\\n\\n    return transformed_data\""]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"# Transform data using Extract and Select features\n","def transform_data(X, num_features=10):\n","    n_sax_symbols = int(X.shape[1] / num_features)\n","    n_paa_segments = int(X.shape[1] / num_features)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","\n","    transformed_data = pd.DataFrame(data_concat)\n","\n","\n","    return transformed_data\"\"\""]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1709141696906,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"zB3SmlIRthyX"},"outputs":[],"source":["def select_model(option, random_state):\n","    if option == 'svm':\n","        return SVC(C = 100, gamma=0.01, kernel='rbf', probability=True)\n","    else:\n","        return RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=random_state)"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1709146106348,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"ACfkDWertiif"},"outputs":[],"source":["def train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option='random_forest', random_state=42):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    # Treinar um modelo para todos os dados transformados\n","    model = select_model(base_option, random_state)\n","    scores = []\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), ascii=True, desc=\"Traning Instances\"):\n","        X_train_fold, X_test_fold = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n","        model.fit(X_train_fold, y_train_fold)\n","        score = model.score(X_test_fold, y_test_fold)\n","        scores.append(score)\n","    avg_score = np.mean(scores)\n","\n","    # Preparar dados para o meta-classificador\n","    meta_features = []\n","    for X_trans in X_train_transformed:\n","        instance_features = []\n","        proba = model.predict_proba(X_trans.reshape(1, -1)) # Reshape para compatibilidade com predict_proba\n","        instance_features.extend(proba.flatten())\n","        meta_features.append(instance_features)\n","\n","    meta_features = np.array(meta_features)\n","\n","    # Treinar o meta-classificador\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","\n","    return model, meta_classifier\n"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1709146191764,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"p4fXHeGFtzBH"},"outputs":[],"source":["from tqdm import tqdm  # Importar a biblioteca tqdm\n","\n","def predict_with_meta_classifier(X_test, trained_base_model, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","\n","    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)\n","\n","        instance_features = []\n","        for X_trans in x_transformed:  # Iterar sobre as diferentes transformações\n","            proba = trained_base_model.predict_proba(X_trans.reshape(1, -1))\n","            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n","\n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n","\n","        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n","\n","    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n","    meta_features_test = np.array(meta_features_test)\n","\n","    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\n","    # np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n","\n","    return predictions\n"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1709141347382,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"j1sRMGzH8Y3z"},"outputs":[],"source":["univariate_list = list(univariate_equal_length)\n","univariate_list.sort()"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRJH8xsgC7ED","outputId":"1c9cfab9-7a04-4fee-9840-4bd963328ed4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Traning Instances: 390it [04:01,  1.61it/s]\n","Testing Instances: 100%|##########| 391/391 [00:14<00:00, 26.67it/s]\n","Traning Instances: 30it [00:00, 49.87it/s]\n","Testing Instances: 100%|##########| 30/30 [00:01<00:00, 25.57it/s]\n","Traning Instances: 60it [00:03, 15.32it/s]\n","Testing Instances: 100%|##########| 60/60 [00:02<00:00, 24.04it/s]\n","Traning Instances: 30it [00:00, 148.87it/s]\n","Testing Instances: 100%|##########| 900/900 [00:32<00:00, 27.39it/s]\n","Traning Instances: 28it [00:00, 223.56it/s]\n","Testing Instances: 100%|##########| 28/28 [00:01<00:00, 26.92it/s]\n","Traning Instances: 16it [00:00, 167.17it/s]\n","Testing Instances: 100%|##########| 306/306 [00:11<00:00, 26.45it/s]\n","Traning Instances: 100it [00:01, 68.80it/s]\n","Testing Instances: 100%|##########| 100/100 [00:03<00:00, 28.98it/s]\n","Traning Instances: 23it [00:00, 279.24it/s]\n","Testing Instances: 100%|##########| 861/861 [00:31<00:00, 27.59it/s]\n","Traning Instances: 24it [00:00, 90.68it/s]\n","Testing Instances: 100%|##########| 88/88 [00:02<00:00, 29.43it/s]\n","Traning Instances: 50it [00:00, 115.45it/s]\n","Testing Instances: 100%|##########| 150/150 [00:04<00:00, 30.29it/s]\n","Traning Instances: 60it [00:03, 17.70it/s]\n","Testing Instances: 100%|##########| 61/61 [00:02<00:00, 27.21it/s]\n","Traning Instances: 70it [00:05, 13.23it/s]\n","Testing Instances: 100%|##########| 73/73 [00:02<00:00, 28.81it/s]\n","Traning Instances: 381it [02:35,  2.44it/s]\n","Testing Instances: 100%|##########| 760/760 [00:24<00:00, 30.59it/s]\n","Traning Instances: 20it [00:00, 444.34it/s]\n","Testing Instances: 100%|##########| 1252/1252 [00:40<00:00, 30.70it/s]\n","Traning Instances: 30it [00:00, 58.89it/s]\n","Testing Instances: 100%|##########| 30/30 [00:01<00:00, 29.11it/s]\n","Traning Instances: 20it [00:00, 459.20it/s]\n","Testing Instances: 100%|##########| 601/601 [00:19<00:00, 31.06it/s]\n","Traning Instances: 27it [00:00, 387.24it/s]\n","Testing Instances: 100%|##########| 953/953 [00:30<00:00, 31.09it/s]\n","Traning Instances: 300it [00:40,  7.42it/s]\n","Testing Instances: 100%|##########| 300/300 [00:09<00:00, 30.07it/s]\n","Traning Instances: 100it [00:05, 18.07it/s]\n","Testing Instances: 100%|##########| 100/100 [00:03<00:00, 30.60it/s]\n","Traning Instances: 1000it [1:11:15,  4.28s/it]\n","Testing Instances: 100%|##########| 4000/4000 [02:13<00:00, 30.07it/s]\n"]}],"source":["accuracy_data = []\n","dataset_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour',\n","'GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n","\n","for dataset_name in dataset_list:\n","    train, train_labels = load_classification(dataset_name, split='TRAIN')\n","    test, test_labels = load_classification(dataset_name, split='test')\n","\n","    xtrain = train.reshape(train.shape[0], -1)\n","    xtest = test.reshape(test.shape[0], -1)\n","\n","    #data_train = transform_data(xtrain)\n","    #data_test = transform_data(xtest)\n","\n","    # Treino\n","    trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, train_labels, base_option='svm', meta_option='random_forest', random_state=42)\n","    # Teste\n","    predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n","    # Resultado\n","    test_accuracy_meta = np.mean(predictions_test_meta == test_labels)\n","\n","    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n","\n","accuracy_df = pd.DataFrame(accuracy_data)\n"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1709146549510,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"JQnLpU8lKAFq","outputId":"7d5aeb32-5e2d-4547-a1bf-a60f6d6d1665"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dataset Name</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adiac</td>\n","      <td>0.790281</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Beef</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Car</td>\n","      <td>0.683333</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CBF</td>\n","      <td>0.874444</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Coffee</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>DiatomSizeReduction</td>\n","      <td>0.924837</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ECG200</td>\n","      <td>0.910000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ECGFiveDays</td>\n","      <td>0.994193</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>FaceFour</td>\n","      <td>0.534091</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>GunPoint</td>\n","      <td>0.980000</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Lightning2</td>\n","      <td>0.836066</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Lightning7</td>\n","      <td>0.273973</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>MedicalImages</td>\n","      <td>0.677632</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>MoteStrain</td>\n","      <td>0.750000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>OliveOil</td>\n","      <td>0.633333</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>SonyAIBORobotSurface1</td>\n","      <td>0.843594</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>SonyAIBORobotSurface2</td>\n","      <td>0.895068</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>SyntheticControl</td>\n","      <td>0.946667</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Trace</td>\n","      <td>0.870000</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>TwoPatterns</td>\n","      <td>0.833250</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Dataset Name  Accuracy\n","0                   Adiac  0.790281\n","1                    Beef  0.833333\n","2                     Car  0.683333\n","3                     CBF  0.874444\n","4                  Coffee  1.000000\n","5     DiatomSizeReduction  0.924837\n","6                  ECG200  0.910000\n","7             ECGFiveDays  0.994193\n","8                FaceFour  0.534091\n","9                GunPoint  0.980000\n","10             Lightning2  0.836066\n","11             Lightning7  0.273973\n","12          MedicalImages  0.677632\n","13             MoteStrain  0.750000\n","14               OliveOil  0.633333\n","15  SonyAIBORobotSurface1  0.843594\n","16  SonyAIBORobotSurface2  0.895068\n","17       SyntheticControl  0.946667\n","18                  Trace  0.870000\n","19            TwoPatterns  0.833250"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["accuracy_df"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1709140783793,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"xXGPPjgI3-Wx"},"outputs":[],"source":["accuracy_df.to_csv('model_acc_SVM+RF+TDV1.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPgJSfuUhViHURafOkEnOEr","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
