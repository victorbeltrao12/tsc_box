{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":689,"status":"ok","timestamp":1709140778883,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"OPQ2CvQ16qSn","outputId":"5cd708b2-cf09-4308-fc38-26c5750556aa"},"outputs":[{"data":{"text/plain":["'!pip install aeon\\n!pip install sktime\\n!pip install tsfresh\\n!pip install tslearn\\n!pip install PyWavelets'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"!pip install aeon\n","!pip install sktime\n","!pip install tsfresh\n","!pip install tslearn\n","!pip install PyWavelets\"\"\""]},{"cell_type":"markdown","metadata":{"id":"vb2wJ4B9U2pD"},"source":["### To Do list\n","\n","\n","*   Comparar os resultados do 1NN contra o SVM+RF\n","*   Comparar os resultados dos classificadores Feature Based com o SVM+RF\n","*   Comparar os resultados do MetaClf_Conc contra o MetaClf_Dict\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1709145813413,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"mALblC0a6_9B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from aeon.datasets import load_classification\n","from aeon.datasets.tsc_data_lists import univariate_equal_length\n","from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n","from aeon.classification.convolution_based import RocketClassifier\n","\n","from tsfresh import extract_features, select_features\n","from tsfresh.feature_extraction import MinimalFCParameters\n","\n","from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n","from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n","\n","import os\n","import math\n","import pywt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import RidgeClassifierCV\n","\n","from scipy.fftpack import fft\n","from scipy.stats import norm\n","\n","\n","from tqdm import tqdm\n","import timeit\n","from datetime import timedelta\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["### Function Transform_data Original"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779251,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"YAZlCmp8MSBk","outputId":"b4a64cd9-58b0-4567-a89a-06f231eb79c3"},"outputs":[{"data":{"text/plain":["'def sax_transform(series, w, a):\\n    paa = [series[i:i + w].mean() for i in range(0, len(series), w)]\\n\\n    if np.std(paa) != 0:\\n        paa = (paa - np.mean(paa)) / np.std(paa)\\n    else:\\n        paa = paa - np.mean(paa)\\n\\n    breakpoints = norm.ppf(np.linspace(0, 1, a+1)[1:-1])\\n    sax_symbols = np.array(range(a))\\n    sax_representation = sax_symbols[np.digitize(paa, breakpoints)]\\n\\n    return sax_representation\\n\\ndef transform_data(X, num_features=10):\\n    a = 5\\n    w = int(X.shape[1] / num_features)  # Ajuste do tamanho da janela baseado no número de características desejado\\n\\n    X_sax = np.array([sax_transform(row, w, a) for row in X])\\n    X_fft = np.abs(fft(X, axis=1))\\n\\n    coeffs_cA, coeffs_cD = pywt.dwt(X, \\'db1\\', axis=1)\\n    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\\n\\n    X_paa = np.column_stack([X[:, i:i+2].mean(axis=1) for i in range(0, X.shape[1], 2)])\\n\\n    return {\\n        \"TS\": X,\\n        \"FFT\": X_fft,\\n        \"DWT\": X_dwt,\\n        \"PAA\": X_paa,\\n        \"SAX\": X_sax\\n    }'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Transform data original\n","\"\"\"def sax_transform(series, w, a):\n","    paa = [series[i:i + w].mean() for i in range(0, len(series), w)]\n","\n","    if np.std(paa) != 0:\n","        paa = (paa - np.mean(paa)) / np.std(paa)\n","    else:\n","        paa = paa - np.mean(paa)\n","\n","    breakpoints = norm.ppf(np.linspace(0, 1, a+1)[1:-1])\n","    sax_symbols = np.array(range(a))\n","    sax_representation = sax_symbols[np.digitize(paa, breakpoints)]\n","\n","    return sax_representation\n","\n","def transform_data(X, num_features=10):\n","    a = 5\n","    w = int(X.shape[1] / num_features)  # Ajuste do tamanho da janela baseado no número de características desejado\n","\n","    X_sax = np.array([sax_transform(row, w, a) for row in X])\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    X_paa = np.column_stack([X[:, i:i+2].mean(axis=1) for i in range(0, X.shape[1], 2)])\n","\n","    return {\n","        \"TS\": X,\n","        \"FFT\": X_fft,\n","        \"DWT\": X_dwt,\n","        \"PAA\": X_paa,\n","        \"SAX\": X_sax\n","    }\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Function Transform_data Concatenate"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779252,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"InL-RPCHCjWM"},"outputs":[{"data":{"text/plain":["\"def transform_data(X, num_features=10):\\n    n_sax_symbols = int(X.shape[1] / num_features)\\n    n_paa_segments = int(X.shape[1] / num_features)\\n\\n    X_fft = np.abs(fft(X, axis=1))\\n\\n    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\\n    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\\n\\n    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\\n    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\\n    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\\n\\n    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\\n    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\\n    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\\n\\n    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\\n    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\\n    data_concat.resize(data.shape[0], data.shape[1])\\n\\n    return data_concat\""]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Transform data using TimeSeriesScalerMeanVariance and concatenate all transformed data\n","\"\"\"def transform_data(X, num_features=10):\n","    n_sax_symbols = int(X.shape[1] / num_features)\n","    n_paa_segments = int(X.shape[1] / num_features)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","\n","    return data_concat\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Function Transform_data with normalization"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779252,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"Qf4ep4cqMNh1","outputId":"64378fcc-6f82-4743-fe2c-f5bcde504dbc"},"outputs":[],"source":["# Transform data atualizado\n","def transform_data(X, num_features=10):\n","    n_sax_symbols = int(X.shape[1] / num_features)\n","    n_paa_segments = int(X.shape[1] / num_features)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n","    data_X.resize(data_X.shape[0], data_X.shape[1])\n","    \n","    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n","    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n","    \n","    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n","    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n","    \n","    data_PAA = TimeSeriesScalerMeanVariance().fit_transform(X_paa)\n","    data_PAA.resize(data_PAA.shape[0], data_PAA.shape[1])\n","    \n","    data_SAX = TimeSeriesScalerMeanVariance().fit_transform(X_sax)\n","    data_SAX.resize(data_SAX.shape[0], data_SAX.shape[1])\n","    \n","\n","    return {\n","        \"TS\": data_X,\n","        \"FFT\": data_FFT,\n","        \"DWT\": data_DWT,\n","        \"PAA\": data_PAA,\n","        \"SAX\": data_SAX\n","    }"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1709141696906,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"zB3SmlIRthyX"},"outputs":[],"source":["def select_model(option, random_state):\n","    if option == '1nn':\n","        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1)\n","    elif option == '3nn':\n","        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=3)\n","    elif option == 'svm':\n","        return SVC(C=100, gamma=0.01, kernel='linear', probability=True)\n","    elif option == 'rd':\n","        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n","    else:\n","        return RandomForestClassifier(n_estimators=100,random_state=random_state)"]},{"cell_type":"markdown","metadata":{},"source":["### Train MetaModel Concatenate"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1709146106348,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"ACfkDWertiif"},"outputs":[{"data":{"text/plain":["'def train_with_meta_classifier(X_train, y_train, base_option=\\'svm\\', meta_option=\\'random_forest\\', random_state=42):\\n    X_train_transformed = transform_data(X_train)\\n\\n    loo = LeaveOneOut()\\n    loo.get_n_splits(X_train_transformed)\\n\\n    # Treinar um modelo para todos os dados transformados\\n    model = select_model(base_option, random_state)\\n    scores = []\\n    for train_index, test_index in tqdm(loo.split(X_train_transformed), ascii=True, desc=\"Traning Instances\"):\\n        X_train_fold, X_test_fold = X_train_transformed[train_index], X_train_transformed[test_index]\\n        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\\n        model.fit(X_train_fold, y_train_fold)\\n        score = model.score(X_test_fold, y_test_fold)\\n        scores.append(score)\\n    avg_score = np.mean(scores)\\n\\n    # Preparar dados para o meta-classificador\\n    meta_features = []\\n    for X_trans in X_train_transformed:\\n        instance_features = []\\n        proba = model.predict_proba(X_trans.reshape(1, -1)) # Reshape para compatibilidade com predict_proba\\n        instance_features.extend(proba.flatten())\\n        meta_features.append(instance_features)\\n\\n    meta_features = np.array(meta_features)\\n\\n    # Treinar o meta-classificador\\n    meta_classifier = select_model(meta_option, random_state)\\n    meta_classifier.fit(meta_features, y_train)\\n\\n    return model, meta_classifier\\n'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"def train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option='random_forest', random_state=42):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    # Treinar um modelo para todos os dados transformados\n","    model = select_model(base_option, random_state)\n","    scores = []\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), ascii=True, desc=\"Traning Instances\"):\n","        X_train_fold, X_test_fold = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n","        model.fit(X_train_fold, y_train_fold)\n","        score = model.score(X_test_fold, y_test_fold)\n","        scores.append(score)\n","    avg_score = np.mean(scores)\n","\n","    # Preparar dados para o meta-classificador\n","    meta_features = []\n","    for X_trans in X_train_transformed:\n","        instance_features = []\n","        proba = model.predict_proba(X_trans.reshape(1, -1)) # Reshape para compatibilidade com predict_proba\n","        instance_features.extend(proba.flatten())\n","        meta_features.append(instance_features)\n","\n","    meta_features = np.array(meta_features)\n","\n","    # Treinar o meta-classificador\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","\n","    return model, meta_classifier\n","\"\"\""]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1709146191764,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"p4fXHeGFtzBH"},"outputs":[{"data":{"text/plain":["'from tqdm import tqdm  # Importar a biblioteca tqdm\\n\\ndef predict_with_meta_classifier(X_test, trained_base_model, trained_meta_classifier):\\n    predictions = []\\n    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\\n\\n    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\\n        x_instance = X_test[i].reshape(1, -1)\\n        x_transformed = transform_data(x_instance)\\n\\n        instance_features = []\\n        for X_trans in x_transformed:  # Iterar sobre as diferentes transformações\\n            proba = trained_base_model.predict_proba(X_trans.reshape(1, -1))\\n            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\\n\\n        meta_feature = np.array(instance_features).reshape(1, -1)\\n        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\\n\\n        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\\n\\n    # Converter a lista de meta-recursos dos dados de teste em um array numpy\\n    meta_features_test = np.array(meta_features_test)\\n\\n    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\\n    # np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\\n\\n    return predictions\\n'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"from tqdm import tqdm  # Importar a biblioteca tqdm\n","\n","def predict_with_meta_classifier(X_test, trained_base_model, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","\n","    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)\n","\n","        instance_features = []\n","        for X_trans in x_transformed:  # Iterar sobre as diferentes transformações\n","            proba = trained_base_model.predict_proba(X_trans.reshape(1, -1))\n","            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n","\n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n","\n","        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n","\n","    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n","    meta_features_test = np.array(meta_features_test)\n","\n","    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\n","    # np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n","\n","    return predictions\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Train MetaModel using Dict variance"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n","    trained_models = {}  # Salvar modelos treinados para cada transformação\n","    \n","    X_train_transformed = transform_data(X_train)  # Transformar todo o conjunto de treino\n","\n","    # Treinar um modelo para cada transformação e salvar no dicionário\n","    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n","        model = select_model(base_option, random_state)\n","        loo = LeaveOneOut()\n","        for train_index, _ in loo.split(X_trans):\n","            model.fit(X_trans[train_index], y_train[train_index])\n","        trained_models[rep] = model\n","        \n","    # Preparar dados para o meta-classificador\n","    meta_features = []\n","    for i in range(X_train.shape[0]):\n","        instance_features = []\n","        for rep, model in trained_models.items():\n","            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n","            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n","        meta_features.append(instance_features)\n","    \n","    meta_features = np.array(meta_features)\n","    #np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\n","    \n","    # Treinar o meta-classificador\n","    meta_classifier = select_model(meta_option, random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","    \n","    return trained_models, meta_classifier, meta_features\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","    \n","    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)\n","        \n","        instance_features = []\n","        for rep, model in trained_base_models.items():  \n","            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))\n","            instance_features.extend(proba.flatten())  \n","        \n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  \n","        \n","        meta_features_test.append(meta_feature.flatten())  \n","    \n","    meta_features_test = np.array(meta_features_test)\n","\n","    return predictions, meta_features_test\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Base Models: 100%|##########| 5/5 [00:00<00:00, 23.83it/s]\n","Testing Instances: 100%|##########| 28/28 [00:00<00:00, 186.42it/s]\n"]}],"source":["train, train_labels = load_classification('Coffee', extract_path=\"./Temp/\", split='TRAIN')\n","test, test_labels = load_classification('Coffee', extract_path=\"./Temp/\", split='test')\n","xtrain = train.reshape(train.shape[0], -1)\n","xtest = test.reshape(test.shape[0], -1)\n","trained_base_models, meta_classifier, meta_feat = train_with_meta_classifier(xtrain, train_labels, base_option='svm', meta_option='rd', random_state=42)\n","predictions_test_meta, predict = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)"]},{"cell_type":"markdown","metadata":{},"source":["### Train/Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1709141347382,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"j1sRMGzH8Y3z"},"outputs":[],"source":["univariate_list = list(univariate_equal_length)\n","univariate_list.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n","dataset_full_list = ['Worms','FaceAll','SyntheticControl','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','TwoPatterns','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','SonyAIBORobotSurface1','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','DiatomSizeReduction','MixedShapesRegularTrain','Trace','ECGFiveDays','Lightning2','MoteStrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ECG200','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Lightning7','Yoga','FaceFour','CinCECGTorso','Beef','OliveOil','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','Coffee','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','Adiac','Car','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','GunPoint','Earthquakes','BirdChicken','HandOutlines','BeetleFly','SonyAIBORobotSurface2','CBF','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n","problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRJH8xsgC7ED","outputId":"1c9cfab9-7a04-4fee-9840-4bd963328ed4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Base Models: 100%|##########| 5/5 [06:45<00:00, 81.17s/it]\n","Testing Instances: 100%|##########| 391/391 [00:03<00:00, 117.15it/s]\n","Training Base Models: 100%|##########| 5/5 [06:24<00:00, 76.99s/it]\n","Testing Instances: 100%|##########| 391/391 [00:02<00:00, 164.67it/s]\n","Training Base Models: 100%|##########| 5/5 [06:29<00:00, 77.86s/it]\n","Testing Instances: 100%|##########| 391/391 [00:02<00:00, 168.66it/s]\n","Training Base Models: 100%|##########| 5/5 [06:20<00:00, 76.01s/it]\n","Testing Instances: 100%|##########| 391/391 [00:02<00:00, 178.32it/s]\n","Training Base Models: 100%|##########| 5/5 [06:28<00:00, 77.70s/it]\n","Testing Instances: 100%|##########| 391/391 [00:05<00:00, 71.00it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  8.10it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 155.50it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  8.16it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 159.07it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.87it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 173.78it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.71it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 156.79it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.46it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 70.26it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.35it/s]\n","Testing Instances: 100%|##########| 60/60 [00:00<00:00, 114.69it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.36it/s]\n","Testing Instances: 100%|##########| 60/60 [00:00<00:00, 140.52it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.41it/s]\n","Testing Instances: 100%|##########| 60/60 [00:00<00:00, 154.58it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.42it/s]\n","Testing Instances: 100%|##########| 60/60 [00:00<00:00, 144.49it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.42it/s]\n","Testing Instances: 100%|##########| 60/60 [00:00<00:00, 68.33it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 21.28it/s]\n","Testing Instances: 100%|##########| 900/900 [00:04<00:00, 200.83it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 20.94it/s]\n","Testing Instances: 100%|##########| 900/900 [00:04<00:00, 185.22it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 21.45it/s]\n","Testing Instances: 100%|##########| 900/900 [00:04<00:00, 195.09it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 21.35it/s]\n","Testing Instances: 100%|##########| 900/900 [00:04<00:00, 205.88it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 21.99it/s]\n","Testing Instances: 100%|##########| 900/900 [00:11<00:00, 75.06it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 26.27it/s]\n","Testing Instances: 100%|##########| 28/28 [00:00<00:00, 189.49it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 28.22it/s]\n","Testing Instances: 100%|##########| 28/28 [00:00<00:00, 181.18it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 28.08it/s]\n","Testing Instances: 100%|##########| 28/28 [00:00<00:00, 180.47it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 28.18it/s]\n","Testing Instances: 100%|##########| 28/28 [00:00<00:00, 168.47it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 28.73it/s]\n","Testing Instances: 100%|##########| 28/28 [00:00<00:00, 69.87it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 51.85it/s]\n","Testing Instances: 100%|##########| 306/306 [00:01<00:00, 176.17it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 46.53it/s]\n","Testing Instances: 100%|##########| 306/306 [00:01<00:00, 172.35it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 50.01it/s]\n","Testing Instances: 100%|##########| 306/306 [00:01<00:00, 173.60it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 47.82it/s]\n","Testing Instances: 100%|##########| 306/306 [00:01<00:00, 175.75it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 56.36it/s]\n","Testing Instances: 100%|##########| 306/306 [00:04<00:00, 72.74it/s]\n","Training Base Models: 100%|##########| 5/5 [00:31<00:00,  6.30s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 195.70it/s]\n","Training Base Models: 100%|##########| 5/5 [00:32<00:00,  6.44s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 197.14it/s]\n","Training Base Models: 100%|##########| 5/5 [00:32<00:00,  6.44s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 208.81it/s]\n","Training Base Models: 100%|##########| 5/5 [00:31<00:00,  6.33s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 206.85it/s]\n","Training Base Models: 100%|##########| 5/5 [00:32<00:00,  6.45s/it]\n","Testing Instances: 100%|##########| 100/100 [00:01<00:00, 75.01it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 41.82it/s]\n","Testing Instances: 100%|##########| 861/861 [00:04<00:00, 198.92it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 41.59it/s]\n","Testing Instances: 100%|##########| 861/861 [00:04<00:00, 197.95it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 43.01it/s]\n","Testing Instances: 100%|##########| 861/861 [00:04<00:00, 201.84it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 40.92it/s]\n","Testing Instances: 100%|##########| 861/861 [00:04<00:00, 198.06it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 35.90it/s]\n","Testing Instances: 100%|##########| 861/861 [00:12<00:00, 70.12it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 16.23it/s]\n","Testing Instances: 100%|##########| 88/88 [00:00<00:00, 173.39it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 16.34it/s]\n","Testing Instances: 100%|##########| 88/88 [00:00<00:00, 165.95it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 16.56it/s]\n","Testing Instances: 100%|##########| 88/88 [00:00<00:00, 177.81it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 17.04it/s]\n","Testing Instances: 100%|##########| 88/88 [00:00<00:00, 170.81it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 16.76it/s]\n","Testing Instances: 100%|##########| 88/88 [00:01<00:00, 71.03it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.42it/s]\n","Testing Instances: 100%|##########| 150/150 [00:00<00:00, 186.17it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.72it/s]\n","Testing Instances: 100%|##########| 150/150 [00:00<00:00, 189.69it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.73it/s]\n","Testing Instances: 100%|##########| 150/150 [00:00<00:00, 198.01it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.87it/s]\n","Testing Instances: 100%|##########| 150/150 [00:00<00:00, 205.43it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.28it/s]\n","Testing Instances: 100%|##########| 150/150 [00:01<00:00, 75.47it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.53it/s]\n","Testing Instances: 100%|##########| 61/61 [00:00<00:00, 149.60it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.52it/s]\n","Testing Instances: 100%|##########| 61/61 [00:00<00:00, 151.05it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.52it/s]\n","Testing Instances: 100%|##########| 61/61 [00:00<00:00, 134.98it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.52it/s]\n","Testing Instances: 100%|##########| 61/61 [00:00<00:00, 148.37it/s]\n","Training Base Models: 100%|##########| 5/5 [00:03<00:00,  1.52it/s]\n","Testing Instances: 100%|##########| 61/61 [00:00<00:00, 67.14it/s]\n","Training Base Models: 100%|##########| 5/5 [00:05<00:00,  1.03s/it]\n","Testing Instances: 100%|##########| 73/73 [00:00<00:00, 171.61it/s]\n","Training Base Models: 100%|##########| 5/5 [00:05<00:00,  1.00s/it]\n","Testing Instances: 100%|##########| 73/73 [00:00<00:00, 174.09it/s]\n","Training Base Models: 100%|##########| 5/5 [00:05<00:00,  1.00s/it]\n","Testing Instances: 100%|##########| 73/73 [00:00<00:00, 175.71it/s]\n","Training Base Models: 100%|##########| 5/5 [00:04<00:00,  1.00it/s]\n","Testing Instances: 100%|##########| 73/73 [00:00<00:00, 175.04it/s]\n","Training Base Models: 100%|##########| 5/5 [00:05<00:00,  1.01s/it]\n","Testing Instances: 100%|##########| 73/73 [00:01<00:00, 70.02it/s]\n","Training Base Models: 100%|##########| 5/5 [5:00:30<00:00, 3606.09s/it]\n","Testing Instances: 100%|##########| 760/760 [00:03<00:00, 193.29it/s]\n","Training Base Models: 100%|##########| 5/5 [4:59:49<00:00, 3597.83s/it]\n","Testing Instances: 100%|##########| 760/760 [00:03<00:00, 192.15it/s]\n","Training Base Models: 100%|##########| 5/5 [5:02:14<00:00, 3626.94s/it]\n","Testing Instances: 100%|##########| 760/760 [00:03<00:00, 198.70it/s]\n","Training Base Models: 100%|##########| 5/5 [5:01:32<00:00, 3618.52s/it]\n","Testing Instances: 100%|##########| 760/760 [00:03<00:00, 203.78it/s]\n","Training Base Models: 100%|##########| 5/5 [5:00:11<00:00, 3602.33s/it]\n","Testing Instances: 100%|##########| 760/760 [00:10<00:00, 75.33it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 21.43it/s]\n","Testing Instances: 100%|##########| 1252/1252 [00:05<00:00, 211.51it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 23.71it/s]\n","Testing Instances: 100%|##########| 1252/1252 [00:05<00:00, 210.39it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 23.91it/s]\n","Testing Instances: 100%|##########| 1252/1252 [00:05<00:00, 213.55it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 22.45it/s]\n","Testing Instances: 100%|##########| 1252/1252 [00:05<00:00, 216.53it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 24.13it/s]\n","Testing Instances: 100%|##########| 1252/1252 [00:15<00:00, 78.28it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 10.56it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 157.04it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 10.72it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 159.27it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 10.54it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 156.79it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 10.66it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 161.21it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 10.60it/s]\n","Testing Instances: 100%|##########| 30/30 [00:00<00:00, 69.10it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 44.37it/s]\n","Testing Instances: 100%|##########| 601/601 [00:02<00:00, 212.90it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 32.88it/s]\n","Testing Instances: 100%|##########| 601/601 [00:02<00:00, 210.96it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 44.91it/s]\n","Testing Instances: 100%|##########| 601/601 [00:02<00:00, 216.83it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 44.37it/s]\n","Testing Instances: 100%|##########| 601/601 [00:02<00:00, 212.22it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00, 43.49it/s]\n","Testing Instances: 100%|##########| 601/601 [00:07<00:00, 78.50it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  6.21it/s]\n","Testing Instances: 100%|##########| 953/953 [00:04<00:00, 212.38it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  6.26it/s]\n","Testing Instances: 100%|##########| 953/953 [00:04<00:00, 212.38it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  5.67it/s]\n","Testing Instances: 100%|##########| 953/953 [00:04<00:00, 215.95it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  5.82it/s]\n","Testing Instances: 100%|##########| 953/953 [00:04<00:00, 216.83it/s]\n","Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.59it/s]\n","Testing Instances: 100%|##########| 953/953 [00:12<00:00, 78.56it/s]\n","Training Base Models: 100%|##########| 5/5 [07:45<00:00, 93.13s/it] \n","Testing Instances: 100%|##########| 300/300 [00:01<00:00, 199.68it/s]\n","Training Base Models: 100%|##########| 5/5 [07:45<00:00, 93.01s/it] \n","Testing Instances: 100%|##########| 300/300 [00:01<00:00, 202.52it/s]\n","Training Base Models: 100%|##########| 5/5 [07:42<00:00, 92.47s/it] \n","Testing Instances: 100%|##########| 300/300 [00:01<00:00, 215.81it/s]\n","Training Base Models: 100%|##########| 5/5 [07:44<00:00, 92.81s/it] \n","Testing Instances: 100%|##########| 300/300 [00:01<00:00, 217.35it/s]\n","Training Base Models: 100%|##########| 5/5 [07:42<00:00, 92.46s/it] \n","Testing Instances: 100%|##########| 300/300 [00:03<00:00, 77.72it/s]\n","Training Base Models: 100%|##########| 5/5 [00:06<00:00,  1.27s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 177.54it/s]\n","Training Base Models: 100%|##########| 5/5 [00:06<00:00,  1.25s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 182.11it/s]\n","Training Base Models: 100%|##########| 5/5 [00:06<00:00,  1.26s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 188.86it/s]\n","Training Base Models: 100%|##########| 5/5 [00:06<00:00,  1.24s/it]\n","Testing Instances: 100%|##########| 100/100 [00:00<00:00, 188.19it/s]\n","Training Base Models: 100%|##########| 5/5 [00:06<00:00,  1.25s/it]\n","Testing Instances: 100%|##########| 100/100 [00:01<00:00, 74.27it/s]\n","Training Base Models:  80%|########  | 4/5 [19:48:31<4:41:07, 16867.11s/it] "]}],"source":["algos = ['1nn', '3nn','svm', 'rd', 'random_forest']\n","accuracy_data = []\n","\n","for dataset_name in dataset_full_list:\n","    train, train_labels = load_classification(dataset_name, extract_path=\"./Temp/\", split='TRAIN')\n","    test, test_labels = load_classification(dataset_name, extract_path=\"./Temp/\", split='test')\n","\n","    xtrain = train.reshape(train.shape[0], -1)\n","    xtest = test.reshape(test.shape[0], -1)\n","    \n","    # Treino\n","    \n","    for algo in algos:\n","        trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, train_labels, base_option='svm', meta_option=rd, random_state=42)\n","    # Teste\n","        predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n","    # Resultado\n","        test_accuracy_meta = np.mean(predictions_test_meta == test_labels)\n","    accuracy_data.append({'clf':{algo}, 'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n","\n","accuracy_df = pd.DataFrame(accuracy_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1709140783793,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"xXGPPjgI3-Wx"},"outputs":[],"source":["accuracy_df.to_csv('model_acc_SVM+RF+TDV2.csv', index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPgJSfuUhViHURafOkEnOEr","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
