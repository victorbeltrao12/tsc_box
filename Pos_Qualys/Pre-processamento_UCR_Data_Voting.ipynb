{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":689,"status":"ok","timestamp":1709140778883,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"OPQ2CvQ16qSn","outputId":"5cd708b2-cf09-4308-fc38-26c5750556aa"},"outputs":[{"data":{"text/plain":["'!pip install aeon\\n!pip install sktime\\n!pip install tsfresh\\n!pip install tslearn\\n!pip install PyWavelets'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"!pip install aeon\n","!pip install sktime\n","!pip install tsfresh\n","!pip install tslearn\n","!pip install PyWavelets\"\"\""]},{"cell_type":"markdown","metadata":{"id":"vb2wJ4B9U2pD"},"source":["### To Do list\n","\n","\n","*   Comparar os resultados do 1NN contra o SVM+RF\n","*   Comparar os resultados dos classificadores Feature Based com o SVM+RF\n","*   Comparar os resultados do MetaClf_Conc contra o MetaClf_Dict\n","\n"]},{"cell_type":"code","execution_count":143,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1709145813413,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"mALblC0a6_9B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from aeon.datasets import load_classification\n","#from aeon.datasets.tsc_data_lists import univariate_equal_length\n","from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n","\n","from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n","from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n","\n","import pywt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.svm import SVC\n","from sklearn.linear_model import RidgeClassifierCV, SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from scipy.fftpack import fft\n","from numba import jit\n","from tqdm import tqdm\n","import timeit\n","from datetime import timedelta\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":144,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779252,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"InL-RPCHCjWM"},"outputs":[{"data":{"text/plain":["\"# Transform data using TimeSeriesScalerMeanVariance and concatenate all transformed data\\n@jit\\ndef transform_data(X):\\n    n_sax_symbols = int(X.shape[1] / 4)\\n    n_paa_segments = int(X.shape[1] / 4)\\n\\n    X_fft = np.abs(fft(X, axis=1))\\n\\n    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\\n    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\\n\\n    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\\n    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\\n    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\\n\\n    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\\n    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\\n    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\\n\\n    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\\n    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\\n    data_concat.resize(data.shape[0], data.shape[1])\\n    \\n    return data_concat\""]},"execution_count":144,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"# Transform data using TimeSeriesScalerMeanVariance and concatenate all transformed data\n","@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","    \n","    return data_concat\"\"\""]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[],"source":["@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.fit_transform(X)\n","    X_paa_inv = paa.inverse_transform(X_paa_)\n","    X_paa = X_paa_inv.reshape(X_paa_inv.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.fit_transform(X)\n","    X_sax_inv = sax.inverse_transform(X_sax_)\n","    X_sax = X_sax_inv.reshape(X_sax_inv.shape[0], -1)\n","\n","    # Calculating statistics once\n","    data_mean = X.mean(axis=1).reshape(-1, 1)\n","    data_std = X.std(axis=1).reshape(-1, 1)\n","    data_max = X.max(axis=1).reshape(-1, 1)\n","    data_min = X.min(axis=1).reshape(-1, 1)\n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax, data_mean, data_std, data_max, data_min), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","\n","    return data_concat\n"]},{"cell_type":"code","execution_count":146,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1709141696906,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"zB3SmlIRthyX"},"outputs":[],"source":["@jit\n","def select_model(option, random_state):\n","    if option == '1nn':\n","        return KNeighborsTimeSeriesClassifier(distance='euclidean',\n","                                              n_neighbors=1,\n","                                              n_jobs=-1)\n","    elif option == '3nn':\n","        return KNeighborsTimeSeriesClassifier(distance='dtw',\n","                                              n_neighbors=3,\n","                                              n_jobs=-1)\n","    elif option == 'svm':\n","        return SVC(C = 100,\n","                   gamma=0.01,\n","                   kernel='linear',\n","                   probability=True)\n","    elif option == 'gbc':\n","        return GradientBoostingClassifier(n_estimators=5,\n","                                          random_state=random_state)\n","    elif option == 'nb':\n","        return GaussianNB()\n","    elif option == 'shape':\n","        return ShapeDTW(n_neighbors=1)\n","    elif option == 'ee':\n","        return ElasticEnsemble(n_jobs=-1,\n","                               random_state=random_state,\n","                               majority_vote=True)\n","    elif option == 'exrf':\n","        return ExtraTreesClassifier(n_estimators=200,\n","                                    criterion=\"entropy\",\n","                                    class_weight=\"balanced\",\n","                                    max_features=\"sqrt\",\n","                                    n_jobs=-1,\n","                                    random_state=None)\n","    elif option == 'rd':\n","        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n","    elif option == 'sgbd':\n","        return SGDClassifier(max_iter=1000, tol=1e-3, class_weight='balanced', n_jobs=-1, loss='perceptron')\n","    else:\n","        return RandomForestClassifier(n_estimators=200,\n","                                      criterion=\"entropy\",\n","                                      max_features=\"sqrt\",\n","                                      n_jobs=-1,\n","                                      random_state=None)"]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[],"source":["meta_distance_based = VotingClassifier(estimators=[\n","    ('nn', KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)),\n","    ('nndtw', KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnddtw', KNeighborsTimeSeriesClassifier(distance='ddtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnwdtw', KNeighborsTimeSeriesClassifier(distance='wdtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnwddtw', KNeighborsTimeSeriesClassifier(distance='wddtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnlcss', KNeighborsTimeSeriesClassifier(distance='lcss', n_neighbors=1, n_jobs=-1)),\n","    ('nnerp', KNeighborsTimeSeriesClassifier(distance='erp', n_neighbors=1, n_jobs=-1)),\n","    ('nnmsm', KNeighborsTimeSeriesClassifier(distance='msm', n_neighbors=1, n_jobs=-1))\n","    ], voting='hard')"]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":["from aeon.classification.interval_based import CanonicalIntervalForestClassifier, DrCIFClassifier, IntervalForestClassifier, RandomIntervalClassifier, RandomIntervalSpectralEnsembleClassifier, SupervisedIntervalClassifier, SupervisedTimeSeriesForest, TimeSeriesForestClassifier\n","\n","meta_interval_based = VotingClassifier(estimators=[\n","    ('drcif', DrCIFClassifier()),\n","    ('ifc', IntervalForestClassifier()),\n","    ('rif', RandomIntervalClassifier()),\n","    ('risec', RandomIntervalSpectralEnsembleClassifier()),\n","    ('sic', SupervisedIntervalClassifier()),\n","    ], voting='hard', verbose=1, n_jobs=-1)"]},{"cell_type":"code","execution_count":149,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1709146106348,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"ACfkDWertiif"},"outputs":[],"source":["@jit\n","def train_with_meta_classifier(X_train, y_train, base_option=meta_interval_based, meta_option='rd', random_state=42):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    # Treinar um modelo para todos os dados transformados\n","    model = select_model(base_option, random_state)\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), colour='red', desc=\"Training\"):\n","        X_train_fold, _ = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, _ = y_train[train_index], y_train[test_index]\n","        model.fit(X_train_fold, y_train_fold)\n","\n","    # Preparar dados para o meta-classificador\n","    meta_features = []\n","    for X_trans in X_train_transformed:\n","        instance_features = []\n","        proba = model.predict_proba(X_trans.reshape(1, -1)) # Reshape para compatibilidade com predict_proba\n","        proba /= np.sum(proba)\n","        instance_features.extend(proba.flatten())\n","        meta_features.append(instance_features)\n","\n","    meta_features = np.array(meta_features)\n","\n","    # Treinar o meta-classificador\n","    meta_classifier = select_model(meta_option, random_state=random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","\n","    return model, meta_classifier\n"]},{"cell_type":"code","execution_count":150,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1709146191764,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"p4fXHeGFtzBH"},"outputs":[],"source":["@jit\n","def predict_with_meta_classifier(X_test, trained_base_model, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","\n","    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)\n","\n","        instance_features = []\n","        for X_trans in x_transformed:  # Iterar sobre as diferentes transformações\n","            proba = trained_base_model.predict_proba(X_trans.reshape(1, -1))\n","            proba /= np.sum(proba)\n","            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n","\n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n","\n","        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n","\n","    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n","    meta_features_test = np.array(meta_features_test)\n","\n","    return predictions\n"]},{"cell_type":"code","execution_count":151,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1709141347382,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"j1sRMGzH8Y3z"},"outputs":[{"data":{"text/plain":["'\\nCaso o teste seja aplicado a todos os conjuntos de dados da UCR\\nunivariate_list = list(univariate_equal_length)\\nunivariate_list.sort()'"]},"execution_count":151,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","Caso o teste seja aplicado a todos os conjuntos de dados da UCR\n","univariate_list = list(univariate_equal_length)\n","univariate_list.sort()\"\"\""]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint','Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n","dataset_full_list= ['Worms','FaceAll','SyntheticControl','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','TwoPatterns','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','SonyAIBORobotSurface1','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','DiatomSizeReduction','MixedShapesRegularTrain','Trace','ECGFiveDays','Lightning2','MoteStrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ECG200','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Lightning7','Yoga','FaceFour','CinCECGTorso','Beef','OliveOil','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','Coffee','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','Adiac','Car','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','GunPoint','Earthquakes','BirdChicken','HandOutlines','BeetleFly','SonyAIBORobotSurface2','CBF','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n","problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll']"]},{"cell_type":"code","execution_count":153,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRJH8xsgC7ED","outputId":"1c9cfab9-7a04-4fee-9840-4bd963328ed4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training: 30it [00:08,  3.51it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:01<00:00, 27.44it/s]\n","Training: 60it [00:18,  3.20it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 60/60 [00:02<00:00, 27.51it/s]\n","Training: 30it [00:08,  3.65it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 900/900 [00:29<00:00, 30.78it/s]\n","Training: 28it [00:07,  3.74it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 28/28 [00:00<00:00, 29.25it/s]\n","Training: 16it [00:04,  3.75it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 306/306 [00:10<00:00, 29.40it/s]\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","datasetss = ['Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction']\n","accuracy_data = []\n","\n","for dataset_name in datasetss:\n","    train, train_labels = load_classification(dataset_name, split='TRAIN')\n","    test, test_labels = load_classification(dataset_name, split='test')\n","\n","    xtrain = train.reshape(train.shape[0], -1)\n","    xtest = test.reshape(test.shape[0], -1)\n","\n","    le = LabelEncoder()\n","    labels = le.fit_transform(train_labels)\n","    true_labels = le.transform(test_labels)\n","\n","    # Treino\n","    trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, labels, meta_option='sgbd', random_state=None)\n","    # Teste\n","    predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n","    # Resultado\n","    test_accuracy_meta = np.mean(predictions_test_meta == true_labels)\n","\n","    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n","\n","accuracy_df = pd.DataFrame(accuracy_data)\n"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dataset Name</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Beef</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Car</td>\n","      <td>0.633333</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CBF</td>\n","      <td>0.874444</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Coffee</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>DiatomSizeReduction</td>\n","      <td>0.911765</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Dataset Name  Accuracy\n","0                 Beef  0.700000\n","1                  Car  0.633333\n","2                  CBF  0.874444\n","3               Coffee  1.000000\n","4  DiatomSizeReduction  0.911765"]},"execution_count":156,"metadata":{},"output_type":"execute_result"}],"source":["accuracy_df"]},{"cell_type":"code","execution_count":155,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1709140783793,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"xXGPPjgI3-Wx"},"outputs":[],"source":["accuracy_df.to_csv('model_VTNNEnsemble+RF_CD.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPgJSfuUhViHURafOkEnOEr","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
