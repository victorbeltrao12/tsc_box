{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":689,"status":"ok","timestamp":1709140778883,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"OPQ2CvQ16qSn","outputId":"5cd708b2-cf09-4308-fc38-26c5750556aa"},"outputs":[{"data":{"text/plain":["'!pip install aeon\\n!pip install sktime\\n!pip install tsfresh\\n!pip install tslearn\\n!pip install PyWavelets'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"!pip install aeon\n","!pip install sktime\n","!pip install tsfresh\n","!pip install tslearn\n","!pip install PyWavelets\"\"\""]},{"cell_type":"markdown","metadata":{"id":"vb2wJ4B9U2pD"},"source":["### To Do list\n","\n","\n","*   Comparar os resultados do 1NN contra o SVM+RF\n","*   Comparar os resultados dos classificadores Feature Based com o SVM+RF\n","*   Comparar os resultados do MetaClf_Conc contra o MetaClf_Dict\n","\n"]},{"cell_type":"code","execution_count":433,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1709145813413,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"mALblC0a6_9B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from aeon.datasets import load_classification\n","from aeon.datasets.tsc_data_lists import univariate_equal_length\n","from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n","\n","from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n","from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n","\n","import pywt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.svm import SVC\n","from sklearn.linear_model import RidgeClassifierCV, SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from scipy.fftpack import fft\n","from numba import jit\n","from tqdm import tqdm\n","import timeit\n","from datetime import timedelta\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":434,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779252,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"InL-RPCHCjWM"},"outputs":[],"source":["# Transform data using TimeSeriesScalerMeanVariance and concatenate all transformed data\n","@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","    \n","    return data_concat"]},{"cell_type":"code","execution_count":435,"metadata":{},"outputs":[{"data":{"text/plain":["\"@jit\\ndef transform_data(X):\\n    n_sax_symbols = int(X.shape[1] / 4)\\n    n_paa_segments = int(X.shape[1] / 4)\\n\\n    X_fft = np.abs(fft(X, axis=1))\\n\\n    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\\n    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\\n\\n    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\\n    X_paa_ = paa.fit_transform(X)\\n    X_paa_inv = paa.inverse_transform(X_paa_)\\n    X_paa = X_paa_inv.reshape(X_paa_inv.shape[0], -1)\\n\\n    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\\n    X_sax_ = sax.fit_transform(X)\\n    X_sax_inv = sax.inverse_transform(X_sax_)\\n    X_sax = X_sax_inv.reshape(X_sax_inv.shape[0], -1)\\n\\n    # Calculating statistics once\\n    data_mean = X.mean(axis=1).reshape(-1, 1)\\n    data_std = X.std(axis=1).reshape(-1, 1)\\n    data_max = X.max(axis=1).reshape(-1, 1)\\n    data_min = X.min(axis=1).reshape(-1, 1)\\n    \\n\\n    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax, data_mean, data_std, data_max, data_min), axis=1)\\n    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\\n    data_concat.resize(data.shape[0], data.shape[1])\\n\\n    return data_concat\\n\""]},"execution_count":435,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.fit_transform(X)\n","    X_paa_inv = paa.inverse_transform(X_paa_)\n","    X_paa = X_paa_inv.reshape(X_paa_inv.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.fit_transform(X)\n","    X_sax_inv = sax.inverse_transform(X_sax_)\n","    X_sax = X_sax_inv.reshape(X_sax_inv.shape[0], -1)\n","\n","    # Calculating statistics once\n","    data_mean = X.mean(axis=1).reshape(-1, 1)\n","    data_std = X.std(axis=1).reshape(-1, 1)\n","    data_max = X.max(axis=1).reshape(-1, 1)\n","    data_min = X.min(axis=1).reshape(-1, 1)\n","    \n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax, data_mean, data_std, data_max, data_min), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","\n","    return data_concat\n","\"\"\""]},{"cell_type":"code","execution_count":436,"metadata":{},"outputs":[{"data":{"text/plain":["\"@jit\\ndef transform_data(X):\\n    n_sax_symbols = int(X.shape[1] / 4)\\n    n_paa_segments = int(X.shape[1] / 4)\\n\\n    X_fft = np.abs(fft(X, axis=1))\\n\\n    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\\n    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\\n\\n    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\\n    X_paa_ = paa.fit_transform(X)\\n    X_paa_inv = paa.inverse_transform(X_paa_)\\n    X_paa = X_paa_inv.reshape(X_paa_inv.shape[0], -1)\\n\\n    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\\n    X_sax_ = sax.fit_transform(X)\\n    X_sax_inv = sax.inverse_transform(X_sax_)\\n    X_sax = X_sax_inv.reshape(X_sax_inv.shape[0], -1)\\n\\n    # Calculating statistics for each transformation\\n    fft_mean = X_fft.mean(axis=1).reshape(-1, 1)\\n    fft_std = X_fft.std(axis=1).reshape(-1, 1)\\n    fft_max = X_fft.max(axis=1).reshape(-1, 1)\\n    fft_min = X_fft.min(axis=1).reshape(-1, 1)\\n\\n    dwt_mean = X_dwt.mean(axis=1).reshape(-1, 1)\\n    dwt_std = X_dwt.std(axis=1).reshape(-1, 1)\\n    dwt_max = X_dwt.max(axis=1).reshape(-1, 1)\\n    dwt_min = X_dwt.min(axis=1).reshape(-1, 1)\\n\\n    paa_mean = X_paa.mean(axis=1).reshape(-1, 1)\\n    paa_std = X_paa.std(axis=1).reshape(-1, 1)\\n    paa_max = X_paa.max(axis=1).reshape(-1, 1)\\n    paa_min = X_paa.min(axis=1).reshape(-1, 1)\\n\\n    sax_mean = X_sax.mean(axis=1).reshape(-1, 1)\\n    sax_std = X_sax.std(axis=1).reshape(-1, 1)\\n    sax_max = X_sax.max(axis=1).reshape(-1, 1)\\n    sax_min = X_sax.min(axis=1).reshape(-1, 1)\\n\\n    # Concatenating statistics with the transformed data\\n    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax,\\n                           fft_mean, fft_std, fft_max, fft_min,\\n                           dwt_mean, dwt_std, dwt_max, dwt_min,\\n                           paa_mean, paa_std, paa_max, paa_min,\\n                           sax_mean, sax_std, sax_max, sax_min), axis=1)\\n\\n    # Calculating statistics for all concatenated data\\n    data_mean = data.mean(axis=1).reshape(-1, 1)\\n    data_std = data.std(axis=1).reshape(-1, 1)\\n    data_max = data.max(axis=1).reshape(-1, 1)\\n    data_min = data.min(axis=1).reshape(-1, 1)\\n\\n    # Concatenating statistics with the transformed data\\n    data_concat = np.concatenate((data, data_mean, data_std, data_max, data_min), axis=1)\\n\\n    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data_concat)\\n    data_concat.resize(data_concat.shape[0], data_concat.shape[1])\\n\\n    return data_concat\\n\""]},"execution_count":436,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.fit_transform(X)\n","    X_paa_inv = paa.inverse_transform(X_paa_)\n","    X_paa = X_paa_inv.reshape(X_paa_inv.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.fit_transform(X)\n","    X_sax_inv = sax.inverse_transform(X_sax_)\n","    X_sax = X_sax_inv.reshape(X_sax_inv.shape[0], -1)\n","\n","    # Calculating statistics for each transformation\n","    fft_mean = X_fft.mean(axis=1).reshape(-1, 1)\n","    fft_std = X_fft.std(axis=1).reshape(-1, 1)\n","    fft_max = X_fft.max(axis=1).reshape(-1, 1)\n","    fft_min = X_fft.min(axis=1).reshape(-1, 1)\n","\n","    dwt_mean = X_dwt.mean(axis=1).reshape(-1, 1)\n","    dwt_std = X_dwt.std(axis=1).reshape(-1, 1)\n","    dwt_max = X_dwt.max(axis=1).reshape(-1, 1)\n","    dwt_min = X_dwt.min(axis=1).reshape(-1, 1)\n","\n","    paa_mean = X_paa.mean(axis=1).reshape(-1, 1)\n","    paa_std = X_paa.std(axis=1).reshape(-1, 1)\n","    paa_max = X_paa.max(axis=1).reshape(-1, 1)\n","    paa_min = X_paa.min(axis=1).reshape(-1, 1)\n","\n","    sax_mean = X_sax.mean(axis=1).reshape(-1, 1)\n","    sax_std = X_sax.std(axis=1).reshape(-1, 1)\n","    sax_max = X_sax.max(axis=1).reshape(-1, 1)\n","    sax_min = X_sax.min(axis=1).reshape(-1, 1)\n","\n","    # Concatenating statistics with the transformed data\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax,\n","                           fft_mean, fft_std, fft_max, fft_min,\n","                           dwt_mean, dwt_std, dwt_max, dwt_min,\n","                           paa_mean, paa_std, paa_max, paa_min,\n","                           sax_mean, sax_std, sax_max, sax_min), axis=1)\n","\n","    # Calculating statistics for all concatenated data\n","    data_mean = data.mean(axis=1).reshape(-1, 1)\n","    data_std = data.std(axis=1).reshape(-1, 1)\n","    data_max = data.max(axis=1).reshape(-1, 1)\n","    data_min = data.min(axis=1).reshape(-1, 1)\n","\n","    # Concatenating statistics with the transformed data\n","    data_concat = np.concatenate((data, data_mean, data_std, data_max, data_min), axis=1)\n","\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data_concat)\n","    data_concat.resize(data_concat.shape[0], data_concat.shape[1])\n","\n","    return data_concat\n","\"\"\""]},{"cell_type":"code","execution_count":453,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1709141696906,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"zB3SmlIRthyX"},"outputs":[],"source":["@jit\n","def select_model(option, random_state):\n","    if option == '1nn':\n","        return KNeighborsTimeSeriesClassifier(distance='euclidean',\n","                                              n_neighbors=1,\n","                                              n_jobs=-1)\n","    elif option == '3nn':\n","        return KNeighborsTimeSeriesClassifier(distance='dtw',\n","                                              n_neighbors=3,\n","                                              n_jobs=-1)\n","    elif option == 'svm':\n","        return SVC(C = 100,\n","                   gamma=0.01,\n","                   kernel='linear',\n","                   probability=True)\n","    elif option == 'gbc':\n","        return GradientBoostingClassifier(n_estimators=5,\n","                                          random_state=random_state)\n","    elif option == 'nb':\n","        return GaussianNB()\n","    elif option == 'exrf':\n","        return ExtraTreesClassifier(n_estimators=200,\n","                                    criterion=\"entropy\",\n","                                    max_features=\"sqrt\",\n","                                    oob_score=True,\n","                                    bootstrap=True,\n","                                    n_jobs=-1,\n","                                    random_state=None)\n","    elif option == 'rd':\n","        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n","    elif option == 'sgbd':\n","        return SGDClassifier(max_iter=1000, n_jobs=-1, loss='perceptron', penalty='elasticnet')\n","    else:\n","         return RandomForestClassifier(n_estimators=200,\n","                                      criterion=\"entropy\",\n","                                      max_features=\"gini\",\n","                                      n_jobs=-1,\n","                                      random_state=None)"]},{"cell_type":"code","execution_count":454,"metadata":{},"outputs":[],"source":["meta_distance_based = VotingClassifier(estimators=[\n","    ('nn', KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)),\n","    ('nndtw', KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnddtw', KNeighborsTimeSeriesClassifier(distance='ddtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnwdtw', KNeighborsTimeSeriesClassifier(distance='wdtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnwddtw', KNeighborsTimeSeriesClassifier(distance='wddtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnlcss', KNeighborsTimeSeriesClassifier(distance='lcss', n_neighbors=1, n_jobs=-1)),\n","    ('nnerp', KNeighborsTimeSeriesClassifier(distance='erp', n_neighbors=1, n_jobs=-1)),\n","    ('nnmsm', KNeighborsTimeSeriesClassifier(distance='msm', n_neighbors=1, n_jobs=-1))\n","    ], voting='hard')"]},{"cell_type":"code","execution_count":455,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1709146106348,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"ACfkDWertiif"},"outputs":[],"source":["@jit\n","def train_with_meta_classifier(X_train, y_train, base_option='exrf', meta_option='rd', random_state=42):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    # Treinar um modelo para todos os dados transformados\n","    model = select_model(base_option, random_state)\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), colour='red', desc=\"Training\"):\n","        X_train_fold, _ = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, _ = y_train[train_index], y_train[test_index]\n","        model.fit(X_train_fold, y_train_fold)\n","\n","    # Preparar dados para o meta-classificador\n","    meta_features = []\n","    for X_trans in X_train_transformed:\n","        instance_features = []\n","        proba = model.predict_proba(X_trans.reshape(1, -1)) # Reshape para compatibilidade com predict_proba\n","        proba /= np.sum(proba)\n","        instance_features.extend(proba.flatten())\n","        meta_features.append(instance_features)\n","\n","    meta_features = np.array(meta_features)\n","\n","    # Treinar o meta-classificador\n","    meta_classifier = select_model(meta_option, random_state=random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","\n","    return model, meta_classifier"]},{"cell_type":"code","execution_count":456,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1709146191764,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"p4fXHeGFtzBH"},"outputs":[],"source":["@jit\n","def predict_with_meta_classifier(X_test, trained_base_model, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","\n","    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)\n","\n","        instance_features = []\n","        for X_trans in x_transformed:  # Iterar sobre as diferentes transformações\n","            proba = trained_base_model.predict_proba(X_trans.reshape(1, -1))\n","            proba /= np.sum(proba)\n","            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n","\n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n","\n","        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n","\n","    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n","    meta_features_test = np.array(meta_features_test)\n","\n","    return predictions"]},{"cell_type":"code","execution_count":457,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1709141347382,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"j1sRMGzH8Y3z"},"outputs":[],"source":["\n","#Caso o teste seja aplicado a todos os conjuntos de dados da UCR\n","univariate_list = list(univariate_equal_length)\n","univariate_list.sort()"]},{"cell_type":"code","execution_count":458,"metadata":{},"outputs":[],"source":["dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint','Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n","dataset_full_list= ['Worms','FaceAll','SyntheticControl','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','TwoPatterns','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','SonyAIBORobotSurface1','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','DiatomSizeReduction','MixedShapesRegularTrain','Trace','ECGFiveDays','Lightning2','MoteStrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ECG200','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Lightning7','Yoga','FaceFour','CinCECGTorso','Beef','OliveOil','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','Coffee','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','Adiac','Car','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','GunPoint','Earthquakes','BirdChicken','HandOutlines','BeetleFly','SonyAIBORobotSurface2','CBF','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n","problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll']"]},{"cell_type":"code","execution_count":459,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRJH8xsgC7ED","outputId":"1c9cfab9-7a04-4fee-9840-4bd963328ed4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training: 100it [00:54,  1.85it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 100/100 [00:05<00:00, 18.50it/s]\n","Training: 390it [03:51,  1.68it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 391/391 [00:17<00:00, 22.93it/s]\n","Training: 36it [00:15,  2.29it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 175/175 [00:07<00:00, 23.32it/s]\n","Training: 30it [00:12,  2.43it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 150/150 [00:06<00:00, 23.31it/s]\n","Training: 30it [00:14,  2.02it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:01<00:00, 22.63it/s]\n","Training: 20it [00:09,  2.10it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 20/20 [00:01<00:00, 17.13it/s]\n","Training: 20it [00:08,  2.24it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 20/20 [00:00<00:00, 22.68it/s]\n","Training: 30it [00:12,  2.33it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 900/900 [00:34<00:00, 26.08it/s]\n","Training: 60it [00:27,  2.22it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 60/60 [00:02<00:00, 21.16it/s]\n","Training: 20it [00:08,  2.26it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 343/343 [00:12<00:00, 26.94it/s]\n","Training: 467it [04:36,  1.69it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 3840/3840 [02:44<00:00, 23.36it/s]\n","Training: 40it [00:19,  2.02it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1380/1380 [01:18<00:00, 17.53it/s]\n","Training: 28it [00:11,  2.36it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 28/28 [00:01<00:00, 22.81it/s]\n","Training: 250it [02:28,  1.68it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 250/250 [00:10<00:00, 22.97it/s]\n","Training: 390it [04:00,  1.62it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 390/390 [00:16<00:00, 22.94it/s]\n","Training: 390it [04:06,  1.58it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 390/390 [00:16<00:00, 23.51it/s]\n","Training: 390it [03:53,  1.67it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 390/390 [00:15<00:00, 24.97it/s]\n","Training: 7200it [3:43:08,  1.86s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 16800/16800 [09:32<00:00, 29.37it/s]\n","Training: 16it [00:05,  2.72it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 306/306 [00:12<00:00, 24.81it/s]\n","Training: 400it [03:04,  2.17it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 139/139 [00:05<00:00, 27.47it/s]\n","Training: 600it [05:05,  1.96it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 276/276 [00:10<00:00, 26.76it/s]\n","Training: 400it [03:07,  2.14it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 139/139 [00:05<00:00, 26.96it/s]\n","Training: 100it [00:38,  2.58it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 100/100 [00:03<00:00, 28.31it/s]\n","Training: 500it [03:25,  2.44it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 4500/4500 [02:30<00:00, 29.96it/s]\n","Training: 23it [00:07,  3.02it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 861/861 [00:28<00:00, 30.05it/s]\n","Training: 362it [04:39,  1.30it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 362/362 [00:16<00:00, 21.48it/s]\n","Training: 362it [04:42,  1.28it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 362/362 [00:16<00:00, 21.48it/s]\n","Training: 322it [02:38,  2.03it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 139/139 [00:05<00:00, 26.32it/s]\n","Training: 8926it [6:00:25,  2.42s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 7711/7711 [04:18<00:00, 29.82it/s]\n","Training: 504it [08:31,  1.01s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 500/500 [00:24<00:00, 20.78it/s]\n","Training: 560it [04:37,  2.02it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1690/1690 [00:57<00:00, 29.58it/s]\n","Training: 24it [00:08,  3.00it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 88/88 [00:03<00:00, 27.36it/s]\n","Training: 200it [01:18,  2.56it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 2050/2050 [01:09<00:00, 29.55it/s]\n","Training: 450it [04:23,  1.71it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 455/455 [00:15<00:00, 29.00it/s]\n","Training: 175it [01:11,  2.44it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 175/175 [00:06<00:00, 28.24it/s]\n","Training: 3601it [2:00:18,  2.00s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1320/1320 [00:47<00:00, 27.68it/s]\n","Training: 3636it [2:01:05,  2.00s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 810/810 [00:29<00:00, 27.57it/s]\n","Training: 150it [00:56,  2.64it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 2850/2850 [01:36<00:00, 29.55it/s]\n","Training: 28it [00:09,  3.07it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 2850/2850 [01:34<00:00, 30.25it/s]\n","Training: 50it [00:16,  3.07it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 150/150 [00:04<00:00, 30.89it/s]\n","Training: 135it [00:48,  2.77it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 316/316 [00:10<00:00, 30.67it/s]\n","Training: 135it [00:48,  2.79it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 316/316 [00:10<00:00, 31.28it/s]\n","Training: 136it [00:48,  2.78it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 315/315 [00:10<00:00, 30.82it/s]\n","Training: 109it [00:41,  2.63it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 105/105 [00:03<00:00, 28.69it/s]\n","Training: 1000it [32:27,  1.95s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 370/370 [00:21<00:00, 17.57it/s]\n","Training: 155it [01:09,  2.25it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 308/308 [00:12<00:00, 24.35it/s]\n","Training: 64it [00:23,  2.69it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 64/64 [00:02<00:00, 28.26it/s]\n","Training: 40it [00:15,  2.50it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 119/119 [00:05<00:00, 20.75it/s]\n","Training: 100it [00:45,  2.18it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 550/550 [00:25<00:00, 21.22it/s]\n","Training: 62it [00:22,  2.72it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 249/249 [00:08<00:00, 28.79it/s]\n","Training: 17it [00:05,  3.06it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 249/249 [00:08<00:00, 28.46it/s]\n","Training: 220it [01:31,  2.40it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1980/1980 [01:07<00:00, 29.51it/s]\n","Training: 67it [00:21,  3.18it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1029/1029 [00:31<00:00, 32.61it/s]\n","Training: 375it [03:14,  1.93it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 375/375 [00:14<00:00, 26.11it/s]\n","Training: 60it [00:21,  2.76it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 61/61 [00:02<00:00, 25.69it/s]\n","Training: 70it [00:25,  2.75it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 73/73 [00:02<00:00, 28.14it/s]\n","Training: 55it [00:21,  2.51it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 2345/2345 [01:33<00:00, 24.98it/s]\n","Training: 60it [00:21,  2.79it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 60/60 [00:02<00:00, 29.20it/s]\n","Training: 381it [02:32,  2.50it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 760/760 [00:24<00:00, 30.99it/s]\n","Training: 400it [02:37,  2.55it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 154/154 [00:05<00:00, 30.47it/s]\n","Training: 600it [04:21,  2.29it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 291/291 [00:09<00:00, 30.85it/s]\n","Training: 399it [02:36,  2.55it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 154/154 [00:04<00:00, 31.33it/s]\n","Training: 500it [05:17,  1.57it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 2425/2425 [01:38<00:00, 24.53it/s]\n","Training: 100it [00:41,  2.40it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 2425/2425 [01:37<00:00, 24.88it/s]\n","Training: 20it [00:06,  3.23it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1252/1252 [00:38<00:00, 32.50it/s]\n","Training: 1800it [53:13,  1.77s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1965/1965 [01:16<00:00, 25.78it/s]\n","Training: 1800it [51:29,  1.72s/it]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1965/1965 [01:16<00:00, 25.82it/s]\n","Training: 200it [01:21,  2.44it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 242/242 [00:08<00:00, 28.04it/s]\n","Training: 30it [00:10,  2.87it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:01<00:00, 28.51it/s]\n","Training: 1800it [17:10,  1.75it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 858/858 [00:27<00:00, 30.75it/s]\n","Training: 214it [02:03,  1.74it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1896/1896 [01:17<00:00, 24.44it/s]\n","Training: 104it [00:57,  1.80it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 208/208 [00:10<00:00, 20.53it/s]\n","Training: 104it [00:57,  1.82it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 208/208 [00:10<00:00, 19.81it/s]\n","Training: 104it [00:57,  1.81it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 208/208 [00:10<00:00, 20.48it/s]\n","Training: 105it [00:36,  2.86it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 105/105 [00:03<00:00, 29.98it/s]\n","Training: 180it [01:06,  2.71it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 180/180 [00:05<00:00, 30.47it/s]\n","Training: 400it [02:35,  2.57it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 205/205 [00:06<00:00, 30.77it/s]\n","Training: 600it [04:22,  2.28it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 291/291 [00:09<00:00, 30.91it/s]\n","Training: 400it [02:37,  2.53it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 205/205 [00:06<00:00, 31.40it/s]\n","Training: 375it [03:20,  1.87it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 375/375 [00:14<00:00, 26.00it/s]\n","Training: 20it [00:09,  2.22it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 50/50 [00:02<00:00, 17.23it/s]\n","Training: 375it [03:21,  1.86it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 375/375 [00:14<00:00, 26.30it/s]\n","Training: 300it [03:00,  1.66it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 600/600 [00:26<00:00, 22.40it/s]\n","Training: 450it [06:09,  1.22it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 450/450 [00:20<00:00, 22.12it/s]\n","Training: 450it [05:58,  1.26it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 450/450 [00:20<00:00, 22.15it/s]\n","Training: 20it [00:06,  3.06it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 180/180 [00:06<00:00, 28.26it/s]\n","Training: 600it [08:20,  1.20it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 600/600 [00:22<00:00, 27.07it/s]\n","Training: 375it [03:21,  1.86it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 375/375 [00:14<00:00, 26.12it/s]\n","Training: 150it [00:49,  3.06it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 150/150 [00:04<00:00, 31.52it/s]\n","Training: 20it [00:06,  3.24it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 601/601 [00:18<00:00, 32.59it/s]\n","Training: 27it [00:08,  3.23it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 953/953 [00:29<00:00, 32.29it/s]\n","Training: 1000it [14:24,  1.16it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 8236/8236 [05:32<00:00, 24.74it/s]\n","Training: 613it [04:32,  2.25it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 370/370 [00:12<00:00, 29.77it/s]\n","Training: 500it [03:43,  2.24it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 625/625 [00:20<00:00, 30.32it/s]\n","Training: 25it [00:08,  3.05it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 995/995 [00:34<00:00, 29.17it/s]\n","Training: 300it [01:55,  2.59it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 300/300 [00:09<00:00, 31.60it/s]\n","Training: 40it [00:13,  2.98it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 228/228 [00:07<00:00, 29.61it/s]\n","Training: 36it [00:12,  2.94it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 130/130 [00:04<00:00, 29.83it/s]\n","Training: 100it [00:37,  2.69it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 100/100 [00:03<00:00, 28.80it/s]\n","Training: 23it [00:07,  3.21it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 1139/1139 [00:35<00:00, 32.08it/s]\n","Training: 1000it [09:25,  1.77it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 4000/4000 [02:11<00:00, 30.39it/s]\n","Training: 36it [00:11,  3.09it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 144/144 [00:04<00:00, 30.69it/s]\n","Training: 896it [13:24,  1.11it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 3582/3582 [02:23<00:00, 24.91it/s]\n","Training: 896it [09:19,  1.60it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 3582/3582 [02:04<00:00, 28.72it/s]\n","Training: 896it [09:31,  1.57it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 3582/3582 [02:04<00:00, 28.75it/s]\n","Training: 896it [09:27,  1.58it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 3582/3582 [02:05<00:00, 28.61it/s]\n","Training: 1000it [07:12,  2.31it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 6164/6164 [03:20<00:00, 30.78it/s]\n","Training: 57it [00:19,  2.89it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 54/54 [00:01<00:00, 30.37it/s]\n","Training: 267it [02:00,  2.22it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 638/638 [00:22<00:00, 28.99it/s]\n","Training: 181it [01:22,  2.19it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 77/77 [00:03<00:00, 25.15it/s]\n","Training: 181it [01:17,  2.34it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 77/77 [00:03<00:00, 24.15it/s]\n","Training: 300it [02:10,  2.30it/s]\n","Testing: 100%|\u001b[32m##########\u001b[0m| 3000/3000 [01:46<00:00, 28.17it/s]\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","accuracy_data = []\n","\n","for dataset_name in univariate_list:\n","    train, train_labels = load_classification(dataset_name, split='TRAIN')\n","    test, test_labels = load_classification(dataset_name, split='test')\n","\n","    xtrain = train.reshape(train.shape[0], -1)\n","    xtest = test.reshape(test.shape[0], -1)\n","\n","    le = LabelEncoder()\n","    labels = le.fit_transform(train_labels)\n","    true_labels = le.transform(test_labels)\n","\n","    # Treino\n","    trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, labels, base_option='exrf', meta_option='rd', random_state=42)\n","    # Teste\n","    predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n","    # Resultado\n","    test_accuracy_meta = np.mean(predictions_test_meta == true_labels)\n","\n","    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n","\n","accuracy_df = pd.DataFrame(accuracy_data)\n"]},{"cell_type":"code","execution_count":460,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dataset Name</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ACSF1</td>\n","      <td>0.750000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Adiac</td>\n","      <td>0.693095</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ArrowHead</td>\n","      <td>0.714286</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BME</td>\n","      <td>0.993333</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beef</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>107</th>\n","      <td>Wine</td>\n","      <td>0.629630</td>\n","    </tr>\n","    <tr>\n","      <th>108</th>\n","      <td>WordSynonyms</td>\n","      <td>0.641066</td>\n","    </tr>\n","    <tr>\n","      <th>109</th>\n","      <td>Worms</td>\n","      <td>0.727273</td>\n","    </tr>\n","    <tr>\n","      <th>110</th>\n","      <td>WormsTwoClass</td>\n","      <td>0.792208</td>\n","    </tr>\n","    <tr>\n","      <th>111</th>\n","      <td>Yoga</td>\n","      <td>0.813333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>112 rows × 2 columns</p>\n","</div>"],"text/plain":["      Dataset Name  Accuracy\n","0            ACSF1  0.750000\n","1            Adiac  0.693095\n","2        ArrowHead  0.714286\n","3              BME  0.993333\n","4             Beef  0.800000\n","..             ...       ...\n","107           Wine  0.629630\n","108   WordSynonyms  0.641066\n","109          Worms  0.727273\n","110  WormsTwoClass  0.792208\n","111           Yoga  0.813333\n","\n","[112 rows x 2 columns]"]},"execution_count":460,"metadata":{},"output_type":"execute_result"}],"source":["accuracy_df"]},{"cell_type":"code","execution_count":461,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1709140783793,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"xXGPPjgI3-Wx"},"outputs":[],"source":["accuracy_df.to_csv('model_EXRF+RD+NOSTATS_OOB_CD.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPgJSfuUhViHURafOkEnOEr","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
