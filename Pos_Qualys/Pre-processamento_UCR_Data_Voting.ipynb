{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":689,"status":"ok","timestamp":1709140778883,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"OPQ2CvQ16qSn","outputId":"5cd708b2-cf09-4308-fc38-26c5750556aa"},"outputs":[],"source":["\"\"\"!pip install aeon\n","!pip install sktime\n","!pip install tsfresh\n","!pip install tslearn\n","!pip install PyWavelets\"\"\""]},{"cell_type":"markdown","metadata":{"id":"vb2wJ4B9U2pD"},"source":["### To Do list\n","\n","\n","*   Comparar os resultados do 1NN contra o SVM+RF\n","*   Comparar os resultados dos classificadores Feature Based com o SVM+RF\n","*   Comparar os resultados do MetaClf_Conc contra o MetaClf_Dict\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1709145813413,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"mALblC0a6_9B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from aeon.datasets import load_classification\n","from aeon.datasets.tsc_data_lists import univariate_equal_length\n","from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n","\n","from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n","from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n","\n","import pywt\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.svm import SVC\n","from sklearn.linear_model import RidgeClassifierCV, SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from scipy.fftpack import fft\n","from numba import jit\n","from tqdm import tqdm\n","import timeit\n","from datetime import timedelta\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709140779252,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"InL-RPCHCjWM"},"outputs":[],"source":["# Transform data using TimeSeriesScalerMeanVariance and concatenate all transformed data\n","@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n","    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n","    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n","\n","    data_clean = np.concatenate((X, X_fft, X_dwt), axis=1)\n","    data_conct = TimeSeriesScalerMeanVariance().fit_transform(data_clean)\n","    data_concat = np.concatenate((data_conct,X_paa, X_sax), axis=1)\n","    data_concat.resize(data_concat.shape[0], data_concat.shape[1])\n","    \n","    return data_concat"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.fit_transform(X)\n","    X_paa_inv = paa.inverse_transform(X_paa_)\n","    X_paa = X_paa_inv.reshape(X_paa_inv.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.fit_transform(X)\n","    X_sax_inv = sax.inverse_transform(X_sax_)\n","    X_sax = X_sax_inv.reshape(X_sax_inv.shape[0], -1)\n","\n","    # Calculating statistics once\n","    data_mean = X.mean(axis=1).reshape(-1, 1)\n","    data_std = X.std(axis=1).reshape(-1, 1)\n","    data_max = X.max(axis=1).reshape(-1, 1)\n","    data_min = X.min(axis=1).reshape(-1, 1)\n","    \n","\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax, data_mean, data_std, data_max, data_min), axis=1)\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data)\n","    data_concat.resize(data.shape[0], data.shape[1])\n","\n","    return data_concat\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"@jit\n","def transform_data(X):\n","    n_sax_symbols = int(X.shape[1] / 4)\n","    n_paa_segments = int(X.shape[1] / 4)\n","\n","    X_fft = np.abs(fft(X, axis=1))\n","\n","    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n","    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n","\n","    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n","    X_paa_ = paa.fit_transform(X)\n","    X_paa_inv = paa.inverse_transform(X_paa_)\n","    X_paa = X_paa_inv.reshape(X_paa_inv.shape[0], -1)\n","\n","    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n","    X_sax_ = sax.fit_transform(X)\n","    X_sax_inv = sax.inverse_transform(X_sax_)\n","    X_sax = X_sax_inv.reshape(X_sax_inv.shape[0], -1)\n","\n","    # Calculating statistics for each transformation\n","    fft_mean = X_fft.mean(axis=1).reshape(-1, 1)\n","    fft_std = X_fft.std(axis=1).reshape(-1, 1)\n","    fft_max = X_fft.max(axis=1).reshape(-1, 1)\n","    fft_min = X_fft.min(axis=1).reshape(-1, 1)\n","\n","    dwt_mean = X_dwt.mean(axis=1).reshape(-1, 1)\n","    dwt_std = X_dwt.std(axis=1).reshape(-1, 1)\n","    dwt_max = X_dwt.max(axis=1).reshape(-1, 1)\n","    dwt_min = X_dwt.min(axis=1).reshape(-1, 1)\n","\n","    paa_mean = X_paa.mean(axis=1).reshape(-1, 1)\n","    paa_std = X_paa.std(axis=1).reshape(-1, 1)\n","    paa_max = X_paa.max(axis=1).reshape(-1, 1)\n","    paa_min = X_paa.min(axis=1).reshape(-1, 1)\n","\n","    sax_mean = X_sax.mean(axis=1).reshape(-1, 1)\n","    sax_std = X_sax.std(axis=1).reshape(-1, 1)\n","    sax_max = X_sax.max(axis=1).reshape(-1, 1)\n","    sax_min = X_sax.min(axis=1).reshape(-1, 1)\n","\n","    # Concatenating statistics with the transformed data\n","    data = np.concatenate((X, X_fft, X_dwt, X_paa, X_sax,\n","                           fft_mean, fft_std, fft_max, fft_min,\n","                           dwt_mean, dwt_std, dwt_max, dwt_min,\n","                           paa_mean, paa_std, paa_max, paa_min,\n","                           sax_mean, sax_std, sax_max, sax_min), axis=1)\n","\n","    # Calculating statistics for all concatenated data\n","    data_mean = data.mean(axis=1).reshape(-1, 1)\n","    data_std = data.std(axis=1).reshape(-1, 1)\n","    data_max = data.max(axis=1).reshape(-1, 1)\n","    data_min = data.min(axis=1).reshape(-1, 1)\n","\n","    # Concatenating statistics with the transformed data\n","    data_concat = np.concatenate((data, data_mean, data_std, data_max, data_min), axis=1)\n","\n","    data_concat = TimeSeriesScalerMeanVariance().fit_transform(data_concat)\n","    data_concat.resize(data_concat.shape[0], data_concat.shape[1])\n","\n","    return data_concat\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1709141696906,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"zB3SmlIRthyX"},"outputs":[],"source":["@jit\n","def select_model(option, random_state):\n","    if option == '1nn':\n","        return KNeighborsTimeSeriesClassifier(distance='euclidean',\n","                                              n_neighbors=1,\n","                                              n_jobs=-1)\n","    elif option == '3nn':\n","        return KNeighborsTimeSeriesClassifier(distance='dtw',\n","                                              n_neighbors=3,\n","                                              n_jobs=-1)\n","    elif option == 'svm':\n","        return SVC(C = 100,\n","                   gamma=0.01,\n","                   kernel='linear',\n","                   probability=True)\n","    elif option == 'gbc':\n","        return GradientBoostingClassifier(n_estimators=5,\n","                                          random_state=random_state)\n","    elif option == 'nb':\n","        return GaussianNB()\n","    elif option == 'exrf':\n","        return ExtraTreesClassifier(n_estimators=200,\n","                                    criterion=\"entropy\",\n","                                    max_features=\"sqrt\",\n","                                    oob_score=True,\n","                                    bootstrap=True,\n","                                    n_jobs=-1,\n","                                    random_state=None)\n","    elif option == 'rd':\n","        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n","    elif option == 'sgbd':\n","        return SGDClassifier(max_iter=1000, n_jobs=-1, loss='perceptron', penalty='elasticnet')\n","    else:\n","         return RandomForestClassifier(n_estimators=200,\n","                                      criterion=\"entropy\",\n","                                      max_features=\"gini\",\n","                                      n_jobs=-1,\n","                                      random_state=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["meta_distance_based = VotingClassifier(estimators=[\n","    ('nn', KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)),\n","    ('nndtw', KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnddtw', KNeighborsTimeSeriesClassifier(distance='ddtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnwdtw', KNeighborsTimeSeriesClassifier(distance='wdtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnwddtw', KNeighborsTimeSeriesClassifier(distance='wddtw', n_neighbors=1, n_jobs=-1)),\n","    ('nnlcss', KNeighborsTimeSeriesClassifier(distance='lcss', n_neighbors=1, n_jobs=-1)),\n","    ('nnerp', KNeighborsTimeSeriesClassifier(distance='erp', n_neighbors=1, n_jobs=-1)),\n","    ('nnmsm', KNeighborsTimeSeriesClassifier(distance='msm', n_neighbors=1, n_jobs=-1))\n","    ], voting='hard')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1709146106348,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"ACfkDWertiif"},"outputs":[],"source":["@jit\n","def train_with_meta_classifier(X_train, y_train, base_option='exrf', meta_option='rd', random_state=42):\n","    X_train_transformed = transform_data(X_train)\n","\n","    loo = LeaveOneOut()\n","    loo.get_n_splits(X_train_transformed)\n","\n","    # Treinar um modelo para todos os dados transformados\n","    model = select_model(base_option, random_state)\n","    for train_index, test_index in tqdm(loo.split(X_train_transformed), colour='red', desc=\"Training\"):\n","        X_train_fold, _ = X_train_transformed[train_index], X_train_transformed[test_index]\n","        y_train_fold, _ = y_train[train_index], y_train[test_index]\n","        model.fit(X_train_fold, y_train_fold)\n","\n","    # Preparar dados para o meta-classificador\n","    meta_features = []\n","    for X_trans in X_train_transformed:\n","        instance_features = []\n","        proba = model.predict_proba(X_trans.reshape(1, -1)) # Reshape para compatibilidade com predict_proba\n","        proba /= np.sum(proba)\n","        instance_features.extend(proba.flatten())\n","        meta_features.append(instance_features)\n","\n","    meta_features = np.array(meta_features)\n","\n","    # Treinar o meta-classificador\n","    meta_classifier = select_model(meta_option, random_state=random_state)\n","    meta_classifier.fit(meta_features, y_train)\n","\n","    return model, meta_classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1709146191764,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"p4fXHeGFtzBH"},"outputs":[],"source":["@jit\n","def predict_with_meta_classifier(X_test, trained_base_model, trained_meta_classifier):\n","    predictions = []\n","    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n","\n","    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing\"):\n","        x_instance = X_test[i].reshape(1, -1)\n","        x_transformed = transform_data(x_instance)\n","\n","        instance_features = []\n","        for X_trans in x_transformed:  # Iterar sobre as diferentes transformações\n","            proba = trained_base_model.predict_proba(X_trans.reshape(1, -1))\n","            proba /= np.sum(proba)\n","            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n","\n","        meta_feature = np.array(instance_features).reshape(1, -1)\n","        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n","\n","        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n","\n","    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n","    meta_features_test = np.array(meta_features_test)\n","\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1709141347382,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"j1sRMGzH8Y3z"},"outputs":[],"source":["\n","#Caso o teste seja aplicado a todos os conjuntos de dados da UCR\n","univariate_list = list(univariate_equal_length)\n","univariate_list.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint','Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n","dataset_full_list= ['Worms','FaceAll','SyntheticControl','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','TwoPatterns','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','SonyAIBORobotSurface1','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','DiatomSizeReduction','MixedShapesRegularTrain','Trace','ECGFiveDays','Lightning2','MoteStrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ECG200','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Lightning7','Yoga','FaceFour','CinCECGTorso','Beef','OliveOil','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','Coffee','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','Adiac','Car','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','GunPoint','Earthquakes','BirdChicken','HandOutlines','BeetleFly','SonyAIBORobotSurface2','CBF','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n","problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRJH8xsgC7ED","outputId":"1c9cfab9-7a04-4fee-9840-4bd963328ed4"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","accuracy_data = []\n","\n","for dataset_name in univariate_list:\n","    train, train_labels = load_classification(dataset_name, split='TRAIN')\n","    test, test_labels = load_classification(dataset_name, split='test')\n","\n","    xtrain = train.reshape(train.shape[0], -1)\n","    xtest = test.reshape(test.shape[0], -1)\n","\n","    le = LabelEncoder()\n","    labels = le.fit_transform(train_labels)\n","    true_labels = le.transform(test_labels)\n","\n","    # Treino\n","    trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, labels, base_option='exrf', meta_option='rd', random_state=42)\n","    # Teste\n","    predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n","    # Resultado\n","    test_accuracy_meta = np.mean(predictions_test_meta == true_labels)\n","\n","    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n","\n","accuracy_df = pd.DataFrame(accuracy_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1709140783793,"user":{"displayName":"Victor Beltrao Duarte","userId":"05002922381121875242"},"user_tz":240},"id":"xXGPPjgI3-Wx"},"outputs":[],"source":["accuracy_df.to_csv('model_EXRF+RD+NOSTATS_OOB_CD.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPgJSfuUhViHURafOkEnOEr","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
