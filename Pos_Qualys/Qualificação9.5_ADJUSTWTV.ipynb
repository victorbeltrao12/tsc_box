{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "\"\"\"%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n",
        "\n",
        "from tsfresh import extract_features, select_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit\n",
        "from tqdm import tqdm\n",
        "import timeit\n",
        "from datetime import timedelta\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Função de transformação dos dados (2D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V0NO-NIJnp10"
      },
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n",
        "\n",
        "\n",
        "@jit\n",
        "def transform_data_math(X, wavelet):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.std(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.max(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.min(X_paa, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.std(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.max(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.min(X_sax, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
        "                         np.std(data_X, axis=1).reshape(-1,1),\n",
        "                         np.max(data_X, axis=1).reshape(-1,1),\n",
        "                         np.min(data_X, axis=1).reshape(-1,1),\n",
        "                         ])\n",
        "\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_FFT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_DWT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    return {\n",
        "        \"TS\": np.hstack([data_X, stats_X]),\n",
        "        \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
        "        \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
        "        \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
        "        \"SAX\": np.hstack([X_sax, stats_SAX])\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### Seleção do modelo extrator e modelo classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "vaW30f6-np11"
      },
      "outputs": [],
      "source": [
        "def select_model(option, random_state):\n",
        "    if option == '1nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
        "    elif option == '3nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
        "    elif option == 'svm':\n",
        "        return SVC(C = 1, gamma=0.1, kernel='linear', probability=True, cache_size=200, max_iter=-1, decision_function_shape='ovr', tol=1e-3)\n",
        "    elif option == 'gbc':\n",
        "        return GradientBoostingClassifier(n_estimators=5, random_state=random_state)\n",
        "    elif option == 'nb':\n",
        "        return AdaBoostClassifier(algorithm='SAMME', n_estimators=200)\n",
        "    elif option == 'exrf':\n",
        "        return ExtraTreesClassifier(n_estimators=200, criterion=\"entropy\", max_features=\"sqrt\", n_jobs=-1, random_state=None)\n",
        "    elif option == 'rd':\n",
        "        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    else:\n",
        "        return RandomForestClassifier(n_estimators=200, criterion=\"gini\", max_features=\"sqrt\", n_jobs=-1, random_state=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mNDe8USnp11"
      },
      "source": [
        "### Treino do modelos extrator e classificador - (CalibrationProba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='None', meta_option='None', random_state=42, wavelet=None):\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\n",
        "\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Base Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        scores = []\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
        "            scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
        "\n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, (model, _) in trained_models.items():\n",
        "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            proba /= np.sum(proba)\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "\n",
        "    meta_features = np.array(meta_features)\n",
        "\n",
        "    # Calibrar as probabilidades dos classificadores base\n",
        "    calibrated_classifiers = []\n",
        "    for rep, (model, _) in trained_models.items():\n",
        "        calibrated_classifier = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
        "        calibrated_classifier.fit(X_train_transformed[rep], y_train)\n",
        "        calibrated_classifiers.append((rep, calibrated_classifier))\n",
        "\n",
        "    # Preparar dados calibrados para o meta-classificador\n",
        "    calibrated_meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, calibrated_classifier in calibrated_classifiers:\n",
        "            proba = calibrated_classifier.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            proba /= np.sum(proba)\n",
        "            instance_features.extend(proba.flatten())\n",
        "        calibrated_meta_features.append(instance_features)\n",
        "\n",
        "    calibrated_meta_features = np.array(calibrated_meta_features)\n",
        "\n",
        "    # Treinar o meta-classificador (utilizando MLP como exemplo)\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(calibrated_meta_features, y_train)\n",
        "\n",
        "    return calibrated_classifiers, meta_classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0z4yRoAnp11"
      },
      "source": [
        "### Predicao do meta-classificador - (CalibrationProba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def predict_with_meta_classifier(X_test, calibrated_base_models, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing Instances\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, calibrated_classifier in calibrated_base_models:\n",
        "            proba = calibrated_classifier.predict_proba(x_transformed[rep][0].reshape(1, -1))  # Ajuste aqui para pegar o primeiro elemento\n",
        "            proba /= np.sum(proba)\n",
        "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
        "\n",
        "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict (MeanProba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combine_and_predict(X_transformed, trained_models):\n",
        "    num_instances = len(next(iter(X_transformed.values())))  # Number of instances from the first transformed data\n",
        "    num_classes = len(trained_models[next(iter(trained_models))].classes_)  # Number of classes from first model\n",
        "    combined_probabilities = np.zeros((num_instances, num_classes))\n",
        "\n",
        "    for transformation_type, X_trans in X_transformed.items():\n",
        "        model = trained_models[transformation_type]\n",
        "        proba = model.predict_proba(X_trans)  # Get probabilities for all instances\n",
        "        combined_probabilities += proba\n",
        "\n",
        "    combined_probabilities_reshaped = combined_probabilities.reshape(num_instances, -1, num_classes)\n",
        "    predicted_classes = np.argmax(combined_probabilities_reshaped, axis=1) + 1  # Adding 1 to start classes from 1 instead of 0\n",
        "    return predicted_classes\n",
        "\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='1nn', meta_option='rf', random_state=123, wavelet=None):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "        trained_models[rep] = model  # Salvar o modelo treinado\n",
        "\n",
        "    avg_proba = combine_and_predict(X_train_transformed, trained_models)\n",
        "    # Train meta-classifier\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(avg_proba, y_train)\n",
        "\n",
        "    return trained_models, meta_classifier\n",
        "\n",
        "def predict_with_meta_classifier(X_test, trained_models, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing Instances\"):\n",
        "        x_instance = X_test[i].reshape(1,-1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet)\n",
        "        avg_proba = combine_and_predict(x_transformed, trained_models)\n",
        "        meta_feature = avg_proba\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n",
        "        meta_features_test.append(meta_feature)\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7b562Qnp12"
      },
      "source": [
        "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ZRW1Zzql88iC"
      },
      "outputs": [],
      "source": [
        "dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace']\n",
        "dataset_full_list = ['Worms','FaceAll','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','MixedShapesRegularTrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Yoga','CinCECGTorso','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','Earthquakes','BirdChicken','HandOutlines','BeetleFly','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
        "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataunique = ['Beef', 'Car', 'CBF', 'Coffee','DiatomSizeReduction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8711111111111111\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "features_train, features_test, target_train, target_test = load_data('CBF')\n",
        "\n",
        "class BaggingClassifier:\n",
        "    def __init__(self, base_estimator, n_estimators=10, max_samples=1.0):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Amostragem com reposição dos dados\n",
        "            X_subset, y_subset = resample(X, y, replace=True, n_samples=int(self.max_samples * len(X)))\n",
        "            # Treinamento do classificador base\n",
        "            estimator = self.base_estimator.fit(X_subset, y_subset)\n",
        "            self.estimators.append(estimator)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Fazer previsões com cada classificador e retornar a maioria delas\n",
        "        predictions = np.array([estimator.predict(X) for estimator in self.estimators])\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "\n",
        "# Criar e treinar modelo Bagging usando árvore de decisão como base\n",
        "base_estimator = select_model('exrf', random_state=123)\n",
        "bagging_model = BaggingClassifier(base_estimator, n_estimators=10, max_samples=0.8)\n",
        "bagging_model.fit(features_train, target_train)\n",
        "\n",
        "# Fazer previsões\n",
        "y_pred = bagging_model.predict(features_test)\n",
        "\n",
        "# Avaliar desempenho\n",
        "accuracy = accuracy_score(target_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGKV11Rnp12",
        "outputId": "a82f769f-6037-4228-f0bf-31fe9408c66e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:44<00:00,  8.87s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:04<00:00,  6.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia Beef: 0.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [01:30<00:00, 18.16s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 60/60 [00:09<00:00,  6.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia Car: 0.6666666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:44<00:00,  8.81s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 900/900 [02:14<00:00,  6.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia CBF: 0.9011111111111111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:41<00:00,  8.29s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 28/28 [00:04<00:00,  6.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia Coffee: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:23<00:00,  4.73s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 306/306 [00:45<00:00,  6.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia DiatomSizeReduction: 0.9411764705882353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for dataset_name in dataunique:\n",
        "    Acc = []\n",
        "    dataset_accuracies = []\n",
        "    # Carregue os dados de treinamento e teste\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    best_wavelet = choose_wavelet(features_train)\n",
        "\n",
        "    trained_models, meta_classifier = train_with_meta_classifier(features_train, target_train, base_option='bagging_model', meta_option='rd', random_state=123, wavelet=best_wavelet)\n",
        "    \n",
        "    predictions = predict_with_meta_classifier(features_test, trained_models, meta_classifier, wavelet=best_wavelet)\n",
        "    \n",
        "    test_accuracy_meta = np.mean(predictions == target_test)\n",
        "    \n",
        "    dataset_accuracies.append(test_accuracy_meta)\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
        "    \n",
        "    Acc.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n",
        "\n",
        "accuracy_df = pd.DataFrame(Acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfFvUX1YU-xH"
      },
      "source": [
        "### Meta-Classificador - Hipótese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [900, 30]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[140], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Treinar o Ridge Classifier usando as pontuações de decisão dos modelos SVM nos dados de treinamento\u001b[39;00m\n\u001b[0;32m     24\u001b[0m ridge_classifier \u001b[38;5;241m=\u001b[39m RidgeClassifierCV()\n\u001b[1;32m---> 25\u001b[0m ridge_classifier\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39mcolumn_stack(\u001b[38;5;28mlist\u001b[39m(scores\u001b[38;5;241m.\u001b[39mvalues())), target_train)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Obter as pontuações de decisão dos modelos SVM nos dados de teste\u001b[39;00m\n\u001b[0;32m     28\u001b[0m scores_test \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM_ME_KedroPython311_20240208\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM_ME_KedroPython311_20240208\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2543\u001b[0m, in \u001b[0;36mRidgeClassifierCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2519\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Ridge classifier with cv.\u001b[39;00m\n\u001b[0;32m   2520\u001b[0m \n\u001b[0;32m   2521\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2540\u001b[0m \u001b[38;5;66;03m# `RidgeClassifier` does not accept \"sag\" or \"saga\" solver and thus support\u001b[39;00m\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;66;03m# csr, csc, and coo sparse matrices. By using solver=\"eigen\" we force to accept\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;66;03m# all sparse format.\u001b[39;00m\n\u001b[1;32m-> 2543\u001b[0m X, y, sample_weight, Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_data(X, y, sample_weight, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meigen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2545\u001b[0m \u001b[38;5;66;03m# If cv is None, gcv mode will be used and we used the binarized Y\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m \u001b[38;5;66;03m# since y will not be binarized in _RidgeGCV estimator.\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;66;03m# If cv is not None, a GridSearchCV with some RidgeClassifier\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m \u001b[38;5;66;03m# estimators are used where y will be binarized. Thus, we pass y\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m \u001b[38;5;66;03m# instead of the binarized Y.\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m target \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m y\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM_ME_KedroPython311_20240208\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1168\u001b[0m, in \u001b[0;36m_RidgeClassifierMixin._prepare_data\u001b[1;34m(self, X, y, sample_weight, solver)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate `X` and `y` and binarize `y`.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;124;03m    The binarized version of `y`.\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[38;5;241m.\u001b[39missparse(X), solver)\n\u001b[1;32m-> 1168\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1169\u001b[0m     X,\n\u001b[0;32m   1170\u001b[0m     y,\n\u001b[0;32m   1171\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1172\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1173\u001b[0m     y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1174\u001b[0m )\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_binarizer \u001b[38;5;241m=\u001b[39m LabelBinarizer(pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, neg_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1177\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_binarizer\u001b[38;5;241m.\u001b[39mfit_transform(y)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM_ME_KedroPython311_20240208\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM_ME_KedroPython311_20240208\\Lib\\site-packages\\sklearn\\utils\\validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1146\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1164\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AM_ME_KedroPython311_20240208\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [900, 30]"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Carregar o conjunto de dados\n",
        "features_train, features_test, target_train, target_test = load_data('CBF')\n",
        "\n",
        "# Treinar SVMs com diferentes kernels\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "svm_models = {}\n",
        "for kernel in kernels:\n",
        "    svm_models[kernel] = SVC(kernel=kernel)\n",
        "    svm_models[kernel].fit(features_train, target_train)\n",
        "\n",
        "# Obter previsões e pontuações de confiança dos SVMs\n",
        "predictions = {}\n",
        "scores = {}\n",
        "for kernel in kernels:\n",
        "    predictions[kernel] = svm_models[kernel].predict(features_test)\n",
        "    scores[kernel] = svm_models[kernel].decision_function(features_test)\n",
        "\n",
        "# Treinar o Ridge Classifier usando as pontuações de decisão dos modelos SVM nos dados de treinamento\n",
        "ridge_classifier = RidgeClassifierCV()\n",
        "ridge_classifier.fit(np.column_stack(list(scores.values())), target_train)\n",
        "\n",
        "# Obter as pontuações de decisão dos modelos SVM nos dados de teste\n",
        "scores_test = {}\n",
        "for kernel in kernels:\n",
        "    scores_test[kernel] = svm_models[kernel].decision_function(features_test)\n",
        "\n",
        "# Fazer a classificação final usando as pontuações de decisão dos modelos SVM nos dados de teste\n",
        "y_pred = ridge_classifier.predict(np.column_stack(list(scores_test.values())))\n",
        "\n",
        "# Avaliar o desempenho do modelo\n",
        "print('Acurácia:', accuracy_score(target_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de amostras em features_train: 30\n",
            "Número de amostras em target_train: 30\n",
            "Número de amostras em features_test: 900\n",
            "Número de amostras em target_test: 900\n"
          ]
        }
      ],
      "source": [
        "print(\"Número de amostras em features_train:\", len(features_train))\n",
        "print(\"Número de amostras em target_train:\", len(target_train))\n",
        "print(\"Número de amostras em features_test:\", len(features_test))\n",
        "print(\"Número de amostras em target_test:\", len(target_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
