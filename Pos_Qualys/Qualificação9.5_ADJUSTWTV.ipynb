{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n",
        "\n",
        "from tsfresh import extract_features, select_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit\n",
        "from tqdm import tqdm\n",
        "import timeit\n",
        "from datetime import timedelta\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'try:\\n    train_data = pd.read_parquet(\\'D:\\\\_MESTRADO\\\\_Meta_Learning\\\\MSC\\\\CSV_Parquet\\\\Car_TRAIN.parquet\\')\\n    test_data = pd.read_parquet(\\'D:\\\\_MESTRADO\\\\_Meta_Learning\\\\MSC\\\\CSV_Parquet\\\\Car_TRAIN.parquet\\')\\nexcept FileNotFoundError:\\n    print(\"Ensure the Parquet files are in the correct path.\")\\n    raise\\n\\n\\ndef load_data(dataset):\\n    # LabelEncoder para labels alvo\\n    le = LabelEncoder()\\n\\n    # Carregar conjunto de dados do repositório UCR\\n    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\\n    X_test, y_test = load_classification(dataset, split=\"test\")\\n\\n    # Formatar o conjunto de dados para 2D\\n    features_train = X_train.reshape(X_train.shape[0], -1)\\n    features_test = X_test.reshape(X_test.shape[0], -1)\\n\\n    # Ajustar e transformar as labels alvo\\n    target_train = le.fit_transform(y_train)\\n    target_test = le.transform(y_test)\\n\\n    return features_train, features_test, target_train, target_test\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"try:\n",
        "    train_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
        "    test_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure the Parquet files are in the correct path.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Função de transformação dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V0NO-NIJnp10"
      },
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n",
        "\n",
        "\n",
        "@jit\n",
        "def transform_data_math(X, wavelet):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.std(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.max(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.min(X_paa, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.std(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.max(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.min(X_sax, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
        "                         np.std(data_X, axis=1).reshape(-1,1),\n",
        "                         np.max(data_X, axis=1).reshape(-1,1),\n",
        "                         np.min(data_X, axis=1).reshape(-1,1),\n",
        "                         ])\n",
        "\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_FFT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_DWT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    return {\n",
        "        \"TS\": np.hstack([data_X, stats_X]),\n",
        "        \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
        "        \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
        "        \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
        "        \"SAX\": np.hstack([X_sax, stats_SAX])\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### Seleção do modelo extrator e modelo classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vaW30f6-np11"
      },
      "outputs": [],
      "source": [
        "def select_model(option, random_state):\n",
        "    if option == '1nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
        "    elif option == '3nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
        "    elif option == 'svm':\n",
        "        return SVC(C = 1000, gamma='auto', kernel='linear', probability=True)\n",
        "    elif option == 'gbc':\n",
        "        return GradientBoostingClassifier(n_estimators=5, random_state=random_state)\n",
        "    elif option == 'nb':\n",
        "        return GaussianNB()\n",
        "    elif option == 'exrf':\n",
        "        return ExtraTreesClassifier(n_estimators=200, criterion=\"entropy\", max_features=\"sqrt\", n_jobs=-1, random_state=None)\n",
        "    elif option == 'rd':\n",
        "        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    else:\n",
        "        return RandomForestClassifier(n_estimators=200, criterion=\"gini\", max_features=\"sqrt\", n_jobs=-1, random_state=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mNDe8USnp11"
      },
      "source": [
        "### Treino do modelos extrator e classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TgZU7fZxr0L3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'@jit\\ndef train_with_meta_classifier(X_train, y_train, base_option=\\'random_forest\\', meta_option=\\'rf\\', random_state=42, wavelet=None):\\n    num_classes = len(np.unique(y_train))\\n    trained_models = {}  # Salvar modelos treinados para cada transformação\\n    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\\n\\n    loo = LeaveOneOut()\\n\\n    # Treinar um modelo para cada transformação e salvar no dicionário\\n    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour=\\'red\\', desc=\"Training Base Models\"):\\n        model = select_model(base_option, random_state)\\n        scores = []\\n        for train_index, _ in loo.split(X_trans):\\n            model.fit(X_trans[train_index], y_train[train_index])\\n            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\\n            scores.append(score)\\n        avg_score = np.mean(scores)\\n        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\\n\\n    # Preparar dados para o meta-classificador\\n    meta_features = []\\n    for i in range(X_train.shape[0]):\\n        instance_features = []\\n        for rep, (model, _) in trained_models.items():\\n            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\\n            proba /= np.sum(proba)\\n            instance_features.extend(proba.flatten())\\n        meta_features.append(instance_features)\\n\\n    meta_features = np.array(meta_features)\\n\\n    # Treinar o meta-classificador (utilizando MLP como exemplo)\\n    meta_classifier = select_model(meta_option, random_state)\\n    meta_classifier.fit(meta_features, y_train)\\n\\n    return trained_models, meta_classifier\\n'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"@jit\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='rf', random_state=42, wavelet=None):\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\n",
        "\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Base Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        scores = []\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
        "            scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
        "\n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, (model, _) in trained_models.items():\n",
        "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            proba /= np.sum(proba)\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "\n",
        "    meta_features = np.array(meta_features)\n",
        "\n",
        "    # Treinar o meta-classificador (utilizando MLP como exemplo)\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "\n",
        "    return trained_models, meta_classifier\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "@jit\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='rf', random_state=42, wavelet=None):\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\n",
        "\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Base Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        scores = []\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
        "            scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
        "\n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, (model, _) in trained_models.items():\n",
        "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "\n",
        "    meta_features = np.array(meta_features)\n",
        "\n",
        "    # Calibrar as probabilidades dos classificadores base\n",
        "    calibrated_classifiers = []\n",
        "    for rep, (model, _) in trained_models.items():\n",
        "        calibrated_classifier = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
        "        calibrated_classifier.fit(X_train_transformed[rep], y_train)\n",
        "        calibrated_classifiers.append((rep, calibrated_classifier))\n",
        "\n",
        "    # Preparar dados calibrados para o meta-classificador\n",
        "    calibrated_meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, calibrated_classifier in calibrated_classifiers:\n",
        "            proba = calibrated_classifier.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        calibrated_meta_features.append(instance_features)\n",
        "\n",
        "    calibrated_meta_features = np.array(calibrated_meta_features)\n",
        "\n",
        "    # Treinar o meta-classificador (utilizando MLP como exemplo)\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(calibrated_meta_features, y_train)\n",
        "\n",
        "    return calibrated_classifiers, meta_classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0z4yRoAnp11"
      },
      "source": [
        "### Predicao do meta-classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QUeeYrisnp12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'@jit\\ndef predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier, wavelet=None):\\n    predictions = []\\n    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\\n\\n    for i in tqdm(range(len(X_test)), ascii=True, colour=\\'green\\', desc=\"Testing Instances\"):\\n        x_instance = X_test[i].reshape(1, -1)\\n        x_transformed = transform_data_math(x_instance, wavelet)\\n\\n        instance_features = []\\n        for rep, (model, _) in trained_base_models.items():  # Ajuste para percorrer os modelos treinados e os scores médios\\n            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))  # Ajuste aqui para pegar o primeiro elemento\\n            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\\n\\n        meta_feature = np.array(instance_features).reshape(1, -1)\\n        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\\n\\n        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\\n\\n    # Converter a lista de meta-recursos dos dados de teste em um array numpy\\n    meta_features_test = np.array(meta_features_test)\\n\\n    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\\n    np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\\n\\n    return predictions\\n'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"@jit\n",
        "def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing Instances\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, (model, _) in trained_base_models.items():  # Ajuste para percorrer os modelos treinados e os scores médios\n",
        "            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))  # Ajuste aqui para pegar o primeiro elemento\n",
        "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
        "\n",
        "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "\n",
        "    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\n",
        "    np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n",
        "\n",
        "    return predictions\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def predict_with_meta_classifier(X_test, calibrated_base_models, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing Instances\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, calibrated_classifier in calibrated_base_models:\n",
        "            proba = calibrated_classifier.predict_proba(x_transformed[rep][0].reshape(1, -1))  # Ajuste aqui para pegar o primeiro elemento\n",
        "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
        "\n",
        "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7b562Qnp12"
      },
      "source": [
        "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZRW1Zzql88iC"
      },
      "outputs": [],
      "source": [
        "dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace']\n",
        "dataset_full_list = ['Worms','FaceAll','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','MixedShapesRegularTrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Yoga','CinCECGTorso','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','Earthquakes','BirdChicken','HandOutlines','BeetleFly','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
        "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataunique = ['CBF']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGKV11Rnp12",
        "outputId": "a82f769f-6037-4228-f0bf-31fe9408c66e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models:  20%|\u001b[31m##        \u001b[0m| 1/5 [01:38<06:35, 98.85s/it]"
          ]
        }
      ],
      "source": [
        "for dataset_name in dataset_quali_list:\n",
        "    Acc = []\n",
        "    # Carregue os dados de treinamento e teste\n",
        "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
        "\n",
        "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    best_wavelet_train = choose_wavelet(X_train_flat)\n",
        "    #best_wavelet_test = choose_wavelet(X_test_flat)\n",
        "\n",
        "    dataset_accuracies = []\n",
        "    calibrate_classifiers, meta_classifier = train_with_meta_classifier(X_train_flat, y_train, base_option='svm', meta_option='rd', wavelet=best_wavelet_train)\n",
        "    predictions_test_meta = predict_with_meta_classifier(X_test_flat, calibrate_classifiers, meta_classifier, wavelet=best_wavelet_train)\n",
        "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
        "    dataset_accuracies.append(test_accuracy_meta)\n",
        "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
        "    \n",
        "    Acc.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n",
        "\n",
        "accuracy_df = pd.DataFrame(Acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48vIj_NYnp14"
      },
      "source": [
        "### Gráfico das diferenças de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtbzTbNjnp14"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_hat' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y_hat  \u001b[38;5;66;03m# depois da transformação\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y2 \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m      6\u001b[0m z1 \u001b[38;5;241m=\u001b[39m y_hat_ \u001b[38;5;66;03m#antes da transformação\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y_hat' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y1 = y_hat  # depois da transformação\n",
        "y2 = y_test\n",
        "\n",
        "z1 = y_hat_ #antes da transformação\n",
        "z2 = y_test\n",
        "\n",
        "#suavizar os dados do gráfico\n",
        "window_size = 15\n",
        "y1_smoothed = pd.Series(y1).rolling(window=window_size).mean()\n",
        "y2_smoothed = pd.Series(y2).rolling(window=window_size).mean()\n",
        "z1_smoothed = pd.Series(z1).rolling(window=window_size).mean()\n",
        "z2_smoothed = pd.Series(z2).rolling(window=window_size).mean()\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained')\n",
        "\n",
        "# Conjunto de validação do classificador\n",
        "axs[0].set_title('Antes da transformação')\n",
        "axs[0].plot(z1_smoothed, label='Treino')\n",
        "axs[0].plot(z2_smoothed, label='Teste')\n",
        "axs[0].set_xlabel('Tempo (s)')\n",
        "axs[0].set_ylabel('Treino')\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Conjunto de validação do meta-classificador\n",
        "axs[1].set_title('Depois da transformação')\n",
        "axs[1].plot(y1_smoothed, label='Treino')\n",
        "axs[1].plot(y2_smoothed, label='Teste')\n",
        "axs[1].set_xlabel('Tempo (s)')\n",
        "axs[1].set_ylabel('Treino')\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRjM29_Xnp14"
      },
      "outputs": [],
      "source": [
        "w1 = y_hat  # meta-classificador\n",
        "w2 = y_hat_ #classificação\n",
        "\n",
        "# Suavizar os dados do gráfico\n",
        "window_size = 15\n",
        "w1_smoothed = pd.Series(w1).rolling(window=window_size).mean()\n",
        "w2_smoothed = pd.Series(w2).rolling(window=window_size).mean()\n",
        "\n",
        "# Plotar os dados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(w1_smoothed, label='w1 (Classificação usando meta-caracteristicas)')\n",
        "plt.plot(w2_smoothed, label='w2 (classificação utilizando dados brutos)')\n",
        "plt.xlabel('Tempo (s)')\n",
        "plt.ylabel('Valores suavizados')\n",
        "plt.title('Comparação entre os resultados de um SVM')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBZ-XYnunp15"
      },
      "source": [
        "### Treino em loop de todas as opções de classificadores disponiveis no Select Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOyRjuwQnp15"
      },
      "outputs": [],
      "source": [
        "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'ee', 'shape', 'rf', 'rd']\n",
        "for algo in algos:\n",
        "\n",
        "    print(f'Meta-classificador com modelo extrator {algo.upper()}')\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "        trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option=algo)\n",
        "        # Testing\n",
        "        predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
        "        test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
        "\n",
        "        print(f'Acurácia do teste usando o meta-classificador com modelo extrator {algo}: {test_accuracy_meta}')\n",
        "    except Exception as e:\n",
        "        print(f\"Ocorreu um erro no teste com o {algo}: {e}\")\n",
        "    print(\"-------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfFvUX1YU-xH"
      },
      "source": [
        "### Meta-Classificador - Hipótese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVm8c033VD_u"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Criando um conjunto de dados de exemplo\n",
        "X, y = make_classification(n_samples=10000, n_features=37, random_state=42)\n",
        "\n",
        "# Dividindo o conjunto de dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definindo os classificadores base\n",
        "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "svm_classifier = SVC(kernel='linear', probability=True, random_state=42)\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Criando o meta-classificador usando votação por maioria\n",
        "meta_classifier = VotingClassifier(estimators=[\n",
        "    ('rf', rf_classifier),\n",
        "    ('svm', svm_classifier),\n",
        "    ('nb', nb_classifier)\n",
        "], voting='hard')\n",
        "\n",
        "# Treinando o meta-classificador\n",
        "meta_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões\n",
        "predictions = meta_classifier.predict(X_test)\n",
        "\n",
        "# Avaliando o desempenho do meta-classificador\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Acurácia do meta-classificador:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kRZ8rJZpE88"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class NeuralNetworkMetaClassifier:\n",
        "\n",
        "    def __init__(self, in_shape, lstm_cells, batch_size=4, epochs=10):\n",
        "        self.in_shape = in_shape\n",
        "        self.lstm_cells = lstm_cells\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        inputs = Input(shape=(1, self.in_shape))\n",
        "        lstm_layer = LSTM(self.lstm_cells, recurrent_regularizer=\"l1\")(inputs)\n",
        "        out = Dense(1, activation=\"sigmoid\")(lstm_layer)\n",
        "\n",
        "        self.meta_clf = Model(inputs=inputs, outputs=out)\n",
        "        self.meta_clf.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
        "\n",
        "        self.meta_clf.fit(X, y, batch_size=self.batch_size, epochs=self.epochs)\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = self.meta_clf.predict(X)\n",
        "        return np.round(pred).astype(int)\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        pred = self.meta_clf.predict(x.reshape(1, 1, -1))[0]\n",
        "        return int(np.round(pred))\n",
        "\n",
        "# Exemplo de como usar a classe NeuralNetworkMetaClassifier\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    meta_classifier = NeuralNetworkMetaClassifier(in_shape=20, lstm_cells=64, batch_size=32, epochs=10)\n",
        "    meta_classifier.fit(X_train.reshape(X_train.shape[0], 1, -1), y_train)\n",
        "\n",
        "    predictions = np.array([meta_classifier.predict_one(x) for x in X_test])\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(\"Acurácia do meta-classificador:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ru7Knb6G5Ca"
      },
      "source": [
        "### Usando Stacking do Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGsX3UJwHHEb"
      },
      "outputs": [],
      "source": [
        "# To install scikeras\n",
        "#%pip install scikeras[tensorflow]\n",
        "%pip install catboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5z7azT9IGzP"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBSRf_O7IV4N"
      },
      "outputs": [],
      "source": [
        "features_train, features_test, target_train, target_test = load_data(\"CBF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBCH6CUeN19b"
      },
      "outputs": [],
      "source": [
        "dataset_accuracies = []\n",
        "trained_base_models, meta_classifier = train_with_meta_classifier(features_train, target_train, base_option='exrf')\n",
        "predictions_test_meta = predict_with_meta_classifier(features_test, trained_base_models, meta_classifier)\n",
        "test_accuracy_meta = np.mean(predictions_test_meta == target_test)\n",
        "dataset_accuracies.append(test_accuracy_meta)\n",
        "print(f\"Acurácia{test_accuracy_meta}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyolvUbejMYm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
