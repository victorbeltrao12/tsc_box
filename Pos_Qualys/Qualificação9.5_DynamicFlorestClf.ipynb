{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "\"\"\"%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier\n",
        "\n",
        "from tsfresh import extract_features, select_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit\n",
        "import timeit\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "@staticmethod\n",
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Function transform data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n",
        "\n",
        "def transform_data_math(X, wavelet):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    df_PAA['id'] = range(len(df_PAA))\n",
        "    \n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "    df_SAX['id'] = range(len(df_SAX))\n",
        "\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "    df_X['id'] = range(len(df_X))\n",
        "\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "    df_FFT['id'] = range(len(df_FFT))\n",
        "\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "    df_DWT['id'] = range(len(df_DWT))\n",
        "    \n",
        "    return {'X': df_X, 'FFT': df_FFT, 'DWT': df_DWT, 'PAA': df_PAA, 'SAX': df_SAX}\n",
        "\n",
        "def extract_features_from_data(data):\n",
        "    extracted_features_list = []\n",
        "    for name, df in data.items():\n",
        "        features = extract_features(df, default_fc_parameters=MinimalFCParameters(), disable_progressbar=True, column_id='id')\n",
        "        extracted_features_list.append(features)\n",
        "    return extracted_features_list\n",
        "\n",
        "def select_best_features(extracted_features_dict):\n",
        "    best_features_list = []\n",
        "    for name, features in extracted_features_dict.items():\n",
        "        best_features = select_features(features)\n",
        "        best_features['Method'] = name\n",
        "        best_features_list.append(best_features)\n",
        "    concatenated_df = pd.concat(best_features_list, axis=1)\n",
        "    return concatenated_df.to_numpy() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### AmazonForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "        train_preds = [clf.predict(X) for clf in self.classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]  # Lista de classificadores\n",
        "\n",
        "        for clf in classifiers:\n",
        "            clf.fit(X, y) # Calculando os pesos com base na acurácia\n",
        "        train_preds = [clf.predict(X) for clf in classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies) ** 4\n",
        "        self.clf_weights /= np.sum(self.clf_weights) # Normalização dos pesos\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]  # Lista de classificadores\n",
        "\n",
        "        probs = [clf.predict_proba(X) for clf in classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7b562Qnp12"
      },
      "source": [
        "### Validando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRW1Zzql88iC"
      },
      "outputs": [],
      "source": [
        "dataset_quali_list = [ 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n",
        "dataset_full_list = ['Worms','FaceAll','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','MixedShapesRegularTrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Yoga','CinCECGTorso','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','Earthquakes','BirdChicken','HandOutlines','BeetleFly','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
        "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DynamicAmazonClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "\n",
        "accuracy_data = []\n",
        "for dataset_name in univariate_equal_length:\n",
        "    # Carregue os dados de treinamento e teste\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    best_wavelet = choose_wavelet(features_train)\n",
        "    \n",
        "    feature_extractor = CombinedDecisionForest()\n",
        "    feature_extractor.fit(features_train, target_train)\n",
        "    \n",
        "    train_features = feature_extractor.predict_proba(features_train)\n",
        "    test_features = feature_extractor.predict_proba(features_test)\n",
        "    \n",
        "    meta_model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    \n",
        "    meta_model.fit(train_features, target_train)\n",
        "    \n",
        "    predictions = meta_model.predict(test_features)\n",
        "    \n",
        "    accuracy = accuracy_score(target_test, predictions)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Accuracy {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df.to_csv('ModelAM_FeatureTest3.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
