{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier\n",
        "\n",
        "#from tsfresh import extract_features, select_features\n",
        "#from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit\n",
        "import timeit\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Function transform data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n",
        "\n",
        "def transform_data_math(X, wavelet):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    #df_PAA['id'] = range(len(df_PAA))\n",
        "    \n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "    #df_SAX['id'] = range(len(df_SAX))\n",
        "\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "    #df_X['id'] = range(len(df_X))\n",
        "\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "    #df_FFT['id'] = range(len(df_FFT))\n",
        "\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "    #df_DWT['id'] = range(len(df_DWT))\n",
        "    \n",
        "    return {'X': df_X, 'FFT': df_FFT, 'DWT': df_DWT, 'PAA': df_PAA, 'SAX': df_SAX}\n",
        "\n",
        "def extract_features_from_data(data):\n",
        "    extracted_features_list = []\n",
        "    for name, df in data.items():\n",
        "        features = extract_features(df, default_fc_parameters=MinimalFCParameters(), disable_progressbar=True, column_id='id')\n",
        "        extracted_features_list.append(features)\n",
        "    return extracted_features_list\n",
        "\n",
        "def select_best_features(extracted_features_dict):\n",
        "    best_features_list = []\n",
        "    for name, features in extracted_features_dict.items():\n",
        "        best_features = select_features(features)\n",
        "        best_features['Method'] = name\n",
        "        best_features_list.append(best_features)\n",
        "    concatenated_df = pd.concat(best_features_list, axis=1)\n",
        "    return concatenated_df.to_numpy() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    transformed_X = np.stack([df_X, df_FFT, df_DWT, df_PAA, df_SAX], axis=-1)\n",
        "    \n",
        "    return transformed_X\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate\n",
        "\n",
        "# Crie uma entrada para cada representação de dados\n",
        "input_X = Input(shape=(1, 128, 1))\n",
        "input_FFT = Input(shape=(1, 128, 1))\n",
        "input_DWT = Input(shape=(1, 128, 1))\n",
        "input_PAA = Input(shape=(1, 128, 1))\n",
        "input_SAX = Input(shape=(1, 128, 1))\n",
        "\n",
        "inputs = [input_X, input_FFT, input_DWT, input_PAA, input_SAX]\n",
        "\n",
        "# Crie uma sub-rede para cada entrada\n",
        "subnets = []\n",
        "for inp in inputs:\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inp)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    subnets.append(x)\n",
        "\n",
        "# Concatene as saídas das sub-redes\n",
        "x = concatenate(subnets)\n",
        "\n",
        "# Adicione algumas camadas densas\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "# Camada de saída\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Crie o modelo\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# Compile o modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import learn2learn as l2l\n",
        "\n",
        "# Defina a arquitetura da sua rede neural\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Crie uma instância do modelo e um otimizador\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Crie uma instância do algoritmo MAML\n",
        "maml = l2l.algorithms.MAML(model, lr=1e-3, first_order=False)\n",
        "\n",
        "# Suponha que temos um dataloader que carrega as tarefas\n",
        "for iteration, task in enumerate(dataloader):\n",
        "    learner = maml.clone()  # Crie um novo learner (i.e., uma nova instância do modelo)\n",
        "    \n",
        "    # \"task\" é uma tarefa, ou seja, um conjunto de dados de treinamento\n",
        "    X_train, y_train = task\n",
        "\n",
        "    # Adapte o modelo à tarefa\n",
        "    learner.adapt(torch.optim.Adam, X_train, y_train, n_steps=5)\n",
        "\n",
        "    # Calcule a perda no conjunto de dados de treinamento\n",
        "    train_preds = learner(X_train)\n",
        "    train_loss = loss(train_preds, y_train)\n",
        "\n",
        "    # Atualize os pesos do modelo\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### AmazonForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "        train_preds = [clf.predict(X) for clf in self.classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies)\n",
        "        self.clf_weights /= np.sum(self.clf_weights)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = [clf.predict_proba(X) for clf in self.classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]  # Lista de classificadores\n",
        "\n",
        "        for clf in classifiers:\n",
        "            clf.fit(X, y) # Calculando os pesos com base na acurácia\n",
        "        train_preds = [clf.predict(X) for clf in classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies) ** 4\n",
        "        self.clf_weights /= np.sum(self.clf_weights) # Normalização dos pesos\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]  # Lista de classificadores\n",
        "\n",
        "        probs = [clf.predict_proba(X) for clf in classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier(X_train, y_train, wavelet=None):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet=wavelet)  # Transformar todo o conjunto de treino\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in X_train_transformed.items():\n",
        "        model = SupervisedTimeSeriesForest()\n",
        "        model.fit(X_trans, y_train)\n",
        "        trained_models[rep] = model\n",
        "        \n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, model in trained_models.items():\n",
        "            proba = model.predict_proba(np.array(X_train_transformed[rep].iloc[i,:]).reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "    \n",
        "    meta_features = np.array(meta_features)\n",
        "    \n",
        "    meta_classifier = CombinedDecisionForest()\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "    \n",
        "    return trained_models, meta_classifier\n",
        "\n",
        "def predict_classifier(X_test, trained_base_model, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet=wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, X_trans in x_transformed.items():\n",
        "            proba = trained_base_model[rep].predict_proba(X_trans.values.reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())\n",
        "\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7b562Qnp12"
      },
      "source": [
        "### Validando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRW1Zzql88iC"
      },
      "outputs": [],
      "source": [
        "dataset_quali_list =  ['ElectricDevices']\n",
        "dataset_full_list = ['PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','MiddlePhalanxOutlineCorrect','Wafer','DistalPhalanxTW','CricketY','FacesUCR','FiftyWords','Mallat','Strawberry','ProximalPhalanxOutlineAgeGroup','MixedShapesRegularTrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Yoga','CinCECGTorso','ChlorineConcentration','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','BirdChicken','HandOutlines','BeetleFly','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
        "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runned = {'Adiac',\n",
        "'Beef',\n",
        "'BeetleFly',\n",
        "'BME',\n",
        "'Car',\n",
        "'CBF',\n",
        "'Coffee',\n",
        "'Computers',\n",
        "'Chinatown',\n",
        "'ChlorineConcentration',\n",
        "'DistalPhalanxOutlineCorrect',\n",
        "'DistalPhalanxTW',\n",
        "'ECG200',\n",
        "'ECG5000',\n",
        "'ECGFiveDays',\n",
        "'EOGHorizontalSignal',\n",
        "'Earthquakes',\n",
        "'EthanolLevel',\n",
        "'FaceFour',\n",
        "'FacesUCR',\n",
        "'GunPoint',\n",
        "'GunPointMaleVersusFemale',\n",
        "'HandOutlines',\n",
        "'Lightning2',\n",
        "'Lightning7',\n",
        "'MedicalImages',\n",
        "'MiddlePhalanxOutlineAgeGroup',\n",
        "'MoteStrain',\n",
        "'NonInvasiveFetalECGThorax1',\n",
        "'NonInvasiveFetalECGThorax2',\n",
        "'OliveOil',\n",
        "'PigAirwayPressure',\n",
        "'PowerCons',\n",
        "'SonyAIBORobotSurface1',\n",
        "'SonyAIBORobotSurface2',\n",
        "'ShapeletSim',\n",
        "'ShapesAll',\n",
        "'SmoothSubspace',\n",
        "'SyntheticControl',\n",
        "'Trace',\n",
        "'TwoPatterns',\n",
        "'SemgHandMovementCh2',\n",
        "'PigArtPressure',\n",
        "'Symbols',\n",
        "'ItalyPowerDemand',\n",
        "'PhalangesOutlinesCorrect',\n",
        "'LargeKitchenAppliances',\n",
        "'SemHandGenderCh2'\n",
        "'DiatomSizeReduction',\n",
        "'PigCVP',\n",
        "'FordB',\n",
        "'SemgHandSubjectCh2',\n",
        "'ScreenType',  \n",
        "'Yoga',  \n",
        "'UWaveGestureLibraryY',\n",
        "'SemgHandGenderCh2',  \n",
        "'ElectricDevices',\n",
        "'OSULeaf',\n",
        "'GunPointAgeSpan',\n",
        "'ToeSegmentation1',\n",
        "'ToeSegmentation2',\n",
        "'CricketZ',\n",
        "'Strawberry',\n",
        "'CricketY',\n",
        "'WordSynonyms',\n",
        "'RefrigerationDevices',  \n",
        "'MixedShapesSmallTrain',  \n",
        "'Wafer',\n",
        "'Worms',  \n",
        "'Phoneme',\n",
        "'GunPointOldVersusYoung',\n",
        "'UMD',  \n",
        "'InsectWingbeatSound',\n",
        "'TwoLeadECG',\n",
        "'Rock',\n",
        "'DistalPhalanxOutlineAgeGroup',\n",
        "'ACSF1',\n",
        "'FiftyWords',\n",
        "'HouseTwenty',\n",
        "'Meat',\n",
        "'FreezerRegularTrain',\n",
        "'Crop',\n",
        "'CricketX',\n",
        "'UWaveGestureLibraryX',\n",
        "'ProximalPhalanxTW',\n",
        "'SwedishLeaf',\n",
        "'FreezerSmallTrain',\n",
        "'ArrowHead',\n",
        "'Fish',\n",
        "'StarLightCurves',\n",
        "'InsectEPGSmallTrain',\n",
        "'WormsTwoClass',\n",
        "'CinCECGTorso',\n",
        "'MiddlePhalanxOutlineCorrect',\n",
        "'Haptics',\n",
        "'Ham',\n",
        "'SmallKitchenAppliances',\n",
        "'UWaveGestureLibraryAll',\n",
        "'ProximalPhalanxOutlineCorrect',\n",
        "'InsectEPGRegularTrain',\n",
        "'ProximalPhalanxOutlineAgeGroup'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_data = univariate_equal_length - runned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_data = []\n",
        "for dataset_name in selected_data:\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    best_wavelet = choose_wavelet(features_train)\n",
        "    \n",
        "    trained, clf = train_classifier(features_train, target_train, best_wavelet)\n",
        "    predictions = predict_classifier(features_test, trained, clf, best_wavelet)\n",
        "\n",
        "    accuracy = accuracy_score(target_test, predictions)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df\n",
        "accuracy_df.to_csv('ModelAM_STSF71.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DynamicAmazonClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "\n",
        "accuracy_data = []\n",
        "for dataset_name in univariate_equal_length:\n",
        "    # Carregue os dados de treinamento e teste\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    best_wavelet = choose_wavelet(features_train)\n",
        "    \n",
        "    feature_extractor = CombinedDecisionForest()\n",
        "    feature_extractor.fit(features_train, target_train)\n",
        "    \n",
        "    train_features = feature_extractor.predict_proba(features_train)\n",
        "    test_features = feature_extractor.predict_proba(features_test)\n",
        "    \n",
        "    meta_model = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    \n",
        "    meta_model.fit(train_features, target_train)\n",
        "    \n",
        "    predictions = meta_model.predict(test_features)\n",
        "    \n",
        "    accuracy = accuracy_score(target_test, predictions)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': accuracy})\n",
        "    \n",
        "    print(f\"Accuracy {dataset_name}: {accuracy}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df.to_csv('ModelAMM_FeatureWorking.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Representação temporal de uma transformada rápida de Fourier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Criação dos dados\n",
        "t = np.linspace(0, 1, 250, endpoint=False)\n",
        "a = 0.02\n",
        "f = 5\n",
        "x = 0.9 * np.sin(2 * np.pi * 3 * f * t + .1)\n",
        "x += a * np.cos(2 * np.pi * 312 * f * t + .1)\n",
        "x += a * np.cos(2 * np.pi * 2000 * f * t)\n",
        "x += a * np.sin(2 * np.pi * 5000 * f * t)\n",
        "x += 0.8 * np.cos(2 * np.pi * 2.3e4 * f * t)\n",
        "x += 0.3 * np.cos(2 * np.pi * 1.3e5 * f * t + .1)\n",
        "x += 0.3 * np.cos(2 * np.pi * 2.3e5 * f * t + .1)\n",
        "x += a * np.cos(2 * np.pi * 2e6 * f * t)\n",
        "\n",
        "# Transformada de Fourier\n",
        "y = np.fft.fft(x)\n",
        "freq = np.fft.fftfreq(len(x), d=t[1] - t[0])\n",
        "\n",
        "# Plotagem do gráfico\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plt.plot(t, x)\n",
        "plt.title('Série Temporal')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(freq, np.abs(y))\n",
        "plt.title('Espectro de Frequência')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
