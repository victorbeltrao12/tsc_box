{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%pip install aeon\\n%pip install tsfresh\\n%pip install tslearn\\n%pip install tensorflow\\n%pip install keras\\n%pip install pywavelets'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.interval_based import SupervisedTimeSeriesForest, TimeSeriesForestClassifier\n",
        "\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit\n",
        "import timeit\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "@staticmethod\n",
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Function transform data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n",
        "\n",
        "\n",
        "@jit\n",
        "def transform_data_math(X, wavelet):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='constant')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    df_PAA = pd.DataFrame(X_paa)\n",
        "    df_PAA['id'] = range(len(df_PAA))\n",
        "    \n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    df_SAX = pd.DataFrame(X_sax)\n",
        "    df_SAX['id'] = range(len(df_SAX))\n",
        "\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    df_X = pd.DataFrame(data_X)\n",
        "    df_X['id'] = range(len(df_X))\n",
        "\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    df_FFT = pd.DataFrame(data_FFT)\n",
        "    df_FFT['id'] = range(len(df_FFT))\n",
        "\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    df_DWT = pd.DataFrame(data_DWT)\n",
        "    df_DWT['id'] = range(len(df_DWT))\n",
        "    \n",
        "    extracted_features_dict = {}\n",
        "\n",
        "    # Loop through each transformed DataFrame and extract features\n",
        "    for name, df in [('X', df_X), ('FFT', df_FFT), ('DWT', df_DWT), ('PAA', df_PAA), ('SAX', df_SAX)]:\n",
        "        features = extract_features(df, default_fc_parameters=MinimalFCParameters(), disable_progressbar=True, column_id='id')\n",
        "        extracted_features_dict[name] = features\n",
        "    return extracted_features_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### AmazonForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedDecisionForest:\n",
        "    def __init__(self):\n",
        "        self.clf1 = RandomForestClassifier()\n",
        "        self.clf2 = ExtraTreesClassifier()\n",
        "        self.clf3 = SupervisedTimeSeriesForest()\n",
        "        self.clf4 = TimeSeriesForestClassifier()\n",
        "        self.clf_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]  # Lista de classificadores\n",
        "\n",
        "        for clf in classifiers:\n",
        "            clf.fit(X, y) # Calculando os pesos com base na acurácia\n",
        "        train_preds = [clf.predict(X) for clf in classifiers]\n",
        "        accuracies = [accuracy_score(y, preds) for preds in train_preds]\n",
        "\n",
        "        self.clf_weights = np.array(accuracies) ** 4\n",
        "        self.clf_weights /= np.sum(self.clf_weights) # Normalização dos pesos\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        classifiers = [self.clf1, self.clf2, self.clf3, self.clf4]  # Lista de classificadores\n",
        "\n",
        "        probs = [clf.predict_proba(X) for clf in classifiers]\n",
        "        combined_probs = np.sum([prob * weight for prob, weight in zip(probs, self.clf_weights)], axis=0)\n",
        "\n",
        "        return combined_probs / np.sum(combined_probs, axis=1, keepdims=True)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def train_with_meta_classifier(X_train, y_train, wavelet=None):\n",
        "    total_time = 0\n",
        "    start = timeit.default_timer()\n",
        "    extracted_features_dict = transform_data_math(X_train, wavelet=wavelet)  # Get extracted features\n",
        "    stop = timeit.default_timer()\n",
        "    total_time += stop - start\n",
        "    print('Extraction train data (seconds): ', total_time) # Get extracted features\n",
        "    # Prepare data for the meta-classifier\n",
        "    meta_features = np.hstack([extracted_features.values for extracted_features in extracted_features_dict.values()])\n",
        "    # Train a meta-classifier\n",
        "    meta_classifier = CombinedDecisionForest()\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "\n",
        "    return meta_classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def predict_with_meta_classifier(X_test, trained_meta_classifier, wavelet=None):\n",
        "    total_time = 0\n",
        "    start = timeit.default_timer()\n",
        "    extracted_features_dict = transform_data_math(X_test, wavelet=wavelet)  # Get extracted features\n",
        "    stop = timeit.default_timer()\n",
        "    total_time += stop - start\n",
        "    print('Extraction test data (seconds): ', total_time)\n",
        "    # Prepare data for the meta-classifier\n",
        "    meta_features_test = np.hstack([extracted_features.values for extracted_features in extracted_features_dict.values()])\n",
        "    # Train a meta-classifier\n",
        "    predictions = trained_meta_classifier.predict(meta_features_test)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7b562Qnp12"
      },
      "source": [
        "### Validando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZRW1Zzql88iC"
      },
      "outputs": [],
      "source": [
        "dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n",
        "dataset_full_list = ['Worms','FaceAll','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','MixedShapesRegularTrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Yoga','CinCECGTorso','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','Earthquakes','BirdChicken','HandOutlines','BeetleFly','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
        "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGKV11Rnp12",
        "outputId": "a82f769f-6037-4228-f0bf-31fe9408c66e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction train data (seconds):  43.788181200041436\n",
            "Extraction test data (seconds):  38.24764590000268\n",
            "Accuracy Adiac: 0.7953964194373402\n",
            "Extraction train data (seconds):  19.630803500069305\n",
            "Extraction test data (seconds):  19.46398389991373\n",
            "Accuracy Beef: 0.8333333333333334\n",
            "Extraction train data (seconds):  26.701001000008546\n",
            "Extraction test data (seconds):  26.299387399922125\n",
            "Accuracy Car: 0.7333333333333333\n",
            "Extraction train data (seconds):  15.778611799934879\n",
            "Extraction test data (seconds):  53.63676190003753\n",
            "Accuracy CBF: 0.9633333333333334\n",
            "Extraction train data (seconds):  19.824045700021088\n",
            "Extraction test data (seconds):  19.028037199983373\n",
            "Accuracy Coffee: 1.0\n",
            "Extraction train data (seconds):  18.19912780006416\n",
            "Extraction test data (seconds):  51.445221600006334\n",
            "Accuracy DiatomSizeReduction: 0.9509803921568627\n",
            "Extraction train data (seconds):  17.75305020005908\n",
            "Extraction test data (seconds):  17.657014700002037\n",
            "Accuracy ECG200: 0.84\n",
            "Extraction train data (seconds):  16.313168699969538\n",
            "Extraction test data (seconds):  54.486920599942096\n",
            "Accuracy ECGFiveDays: 0.9988385598141696\n",
            "Extraction train data (seconds):  17.619302000035532\n",
            "Extraction test data (seconds):  24.789637899957597\n",
            "Accuracy FaceFour: 0.9204545454545454\n",
            "Extraction train data (seconds):  17.05226429994218\n",
            "Extraction test data (seconds):  24.084239299991168\n",
            "Accuracy GunPoint: 0.9466666666666667\n",
            "Extraction train data (seconds):  30.53015130001586\n",
            "Extraction test data (seconds):  27.616311499965377\n",
            "Accuracy Lightning2: 0.7868852459016393\n",
            "Extraction train data (seconds):  22.09829739993438\n",
            "Extraction test data (seconds):  22.364415399963036\n",
            "Accuracy Lightning7: 0.7671232876712328\n",
            "Extraction train data (seconds):  28.07284100004472\n",
            "Extraction test data (seconds):  39.357211800059304\n",
            "Accuracy MedicalImages: 0.8118421052631579\n",
            "Extraction train data (seconds):  15.444504800019786\n",
            "Extraction test data (seconds):  50.74939410004299\n",
            "Accuracy MoteStrain: 0.8785942492012779\n",
            "Extraction train data (seconds):  20.783962899935432\n",
            "Extraction test data (seconds):  20.63864979997743\n",
            "Accuracy OliveOil: 0.9333333333333333\n",
            "Extraction train data (seconds):  15.260674400022253\n",
            "Extraction test data (seconds):  28.601315999985673\n",
            "Accuracy SonyAIBORobotSurface1: 0.8452579034941764\n",
            "Extraction train data (seconds):  15.339091000030749\n",
            "Extraction test data (seconds):  36.62191320001148\n",
            "Accuracy SonyAIBORobotSurface2: 0.8541448058761805\n",
            "Extraction train data (seconds):  20.72248190001119\n",
            "Extraction test data (seconds):  20.63070469989907\n",
            "Accuracy SyntheticControl: 0.9866666666666667\n",
            "Extraction train data (seconds):  23.513133400003426\n",
            "Extraction test data (seconds):  23.701269199955277\n",
            "Accuracy Trace: 0.99\n",
            "Extraction train data (seconds):  58.472150199930184\n",
            "Extraction test data (seconds):  198.16917040001135\n",
            "Accuracy TwoPatterns: 0.997\n"
          ]
        }
      ],
      "source": [
        "accuracy_data = []\n",
        "for dataset_name in dataset_quali_list:\n",
        "    # Carregue os dados de treinamento e teste\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    best_wavelet = choose_wavelet(features_train)\n",
        "\n",
        "    meta_classifier = train_with_meta_classifier(features_train, target_train, wavelet=best_wavelet)\n",
        "    \n",
        "    predictions = predict_with_meta_classifier(features_test, meta_classifier, wavelet=best_wavelet)\n",
        "    \n",
        "    test_accuracy_meta = np.mean(predictions == target_test)\n",
        "        \n",
        "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n",
        "    \n",
        "    print(f\"Accuracy {dataset_name}: {test_accuracy_meta}\")\n",
        "    \n",
        "accuracy_df = pd.DataFrame(accuracy_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset Name</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adiac</td>\n",
              "      <td>0.795396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beef</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Car</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CBF</td>\n",
              "      <td>0.963333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Coffee</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DiatomSizeReduction</td>\n",
              "      <td>0.950980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ECG200</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ECGFiveDays</td>\n",
              "      <td>0.998839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FaceFour</td>\n",
              "      <td>0.920455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>GunPoint</td>\n",
              "      <td>0.946667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Lightning2</td>\n",
              "      <td>0.786885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Lightning7</td>\n",
              "      <td>0.767123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>MedicalImages</td>\n",
              "      <td>0.811842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>MoteStrain</td>\n",
              "      <td>0.878594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>OliveOil</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SonyAIBORobotSurface1</td>\n",
              "      <td>0.845258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SonyAIBORobotSurface2</td>\n",
              "      <td>0.854145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SyntheticControl</td>\n",
              "      <td>0.986667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Trace</td>\n",
              "      <td>0.990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>TwoPatterns</td>\n",
              "      <td>0.997000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Dataset Name  Accuracy\n",
              "0                   Adiac  0.795396\n",
              "1                    Beef  0.833333\n",
              "2                     Car  0.733333\n",
              "3                     CBF  0.963333\n",
              "4                  Coffee  1.000000\n",
              "5     DiatomSizeReduction  0.950980\n",
              "6                  ECG200  0.840000\n",
              "7             ECGFiveDays  0.998839\n",
              "8                FaceFour  0.920455\n",
              "9                GunPoint  0.946667\n",
              "10             Lightning2  0.786885\n",
              "11             Lightning7  0.767123\n",
              "12          MedicalImages  0.811842\n",
              "13             MoteStrain  0.878594\n",
              "14               OliveOil  0.933333\n",
              "15  SonyAIBORobotSurface1  0.845258\n",
              "16  SonyAIBORobotSurface2  0.854145\n",
              "17       SyntheticControl  0.986667\n",
              "18                  Trace  0.990000\n",
              "19            TwoPatterns  0.997000"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df.to_csv('ResultsAmazonForest.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
