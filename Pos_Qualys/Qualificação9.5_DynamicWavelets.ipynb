{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l1AP99G_oHxu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%pip install aeon\\n%pip install tsfresh\\n%pip install tslearn\\n%pip install tensorflow\\n%pip install keras\\n%pip install pywavelets'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"%pip install aeon\n",
        "%pip install tsfresh\n",
        "%pip install tslearn\n",
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install pywavelets\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from aeon.classification.sklearn import RotationForestClassifier\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit, njit\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [],
      "source": [
        "@staticmethod\n",
        "def load_data(dataset):\n",
        "    # LabelEncoder para labels alvo\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Carregar conjunto de dados do repositório UCR\n",
        "    X_train, y_train = load_classification(dataset, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset, split=\"test\")\n",
        "\n",
        "    # Formatar o conjunto de dados para 2D\n",
        "    features_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    features_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # Ajustar e transformar as labels alvo\n",
        "    target_train = le.fit_transform(y_train)\n",
        "    target_test = le.transform(y_test)\n",
        "\n",
        "    return features_train, features_test, target_train, target_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Função de transformação dos dados (2D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "V0NO-NIJnp10"
      },
      "outputs": [],
      "source": [
        "def choose_wavelet(X):\n",
        "    min_variance = float('inf')\n",
        "    best_wavelet = None\n",
        "    candidate_wavelets = ['db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9']\n",
        "\n",
        "    for wavelet_type in candidate_wavelets:\n",
        "        _, coeffs_cD = pywt.dwt(X, wavelet_type, axis=1)\n",
        "        total_variance = np.var(coeffs_cD)\n",
        "\n",
        "        if total_variance < min_variance:\n",
        "            min_variance = total_variance\n",
        "            best_wavelet = wavelet_type\n",
        "    return str(best_wavelet)\n",
        "\n",
        "\n",
        "@jit\n",
        "def transform_data_math(X, wavelet):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, wavelet=wavelet, axis=1, mode='periodization')\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.std(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.max(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.min(X_paa, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.std(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.max(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.min(X_sax, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
        "                         np.std(data_X, axis=1).reshape(-1,1),\n",
        "                         np.max(data_X, axis=1).reshape(-1,1),\n",
        "                         np.min(data_X, axis=1).reshape(-1,1),\n",
        "                         ])\n",
        "\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_FFT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_DWT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    return {\n",
        "        \"TS\": np.hstack([data_X, stats_X]),\n",
        "        \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
        "        \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
        "        \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
        "        \"SAX\": np.hstack([X_sax, stats_SAX])\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### Seleção do modelo extrator e modelo classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vaW30f6-np11"
      },
      "outputs": [],
      "source": [
        "@staticmethod\n",
        "def select_model(option, random_state):\n",
        "    if option == '1nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
        "    elif option == '3nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
        "    elif option == 'svm':\n",
        "        return SVC(C = 1, gamma=0.1, kernel='linear', probability=True, cache_size=200, max_iter=-1, decision_function_shape='ovr', tol=1e-3)\n",
        "    elif option == 'gbc':\n",
        "        return GradientBoostingClassifier(n_estimators=5, random_state=random_state)\n",
        "    elif option == 'nb':\n",
        "        return GaussianNB()\n",
        "    elif option == 'lr':\n",
        "        return LogisticRegression(n_jobs=-1, max_iter=5000, solver=\"liblinear\", dual=True, penalty=\"l2\", random_state=random_state)\n",
        "    elif option == 'rrf':\n",
        "        return RotationForestClassifier(n_jobs=-1, random_state=None)\n",
        "    elif option == 'exrf':\n",
        "        return ExtraTreesClassifier(n_estimators=200, criterion=\"entropy\", max_features=\"sqrt\", n_jobs=-1, random_state=None)\n",
        "    elif option == 'rd':\n",
        "        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    else:\n",
        "        return RandomForestClassifier(n_estimators=200, criterion=\"gini\", max_features=\"sqrt\", n_jobs=-1, random_state=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mNDe8USnp11"
      },
      "source": [
        "### Treino do modelos extrator e classificador - (CalibrationProba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='None', meta_option='None', random_state=42, wavelet=None):\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\n",
        "\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Base Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        scores = []\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
        "            scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
        "\n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, (model, _) in trained_models.items():\n",
        "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        meta_features.append(instance_features)\n",
        "\n",
        "    meta_features = np.array(meta_features)\n",
        "\n",
        "    # Calibrar as probabilidades dos classificadores base\n",
        "    calibrated_classifiers = []\n",
        "    for rep, (model, _) in trained_models.items():\n",
        "        calibrated_classifier = CalibratedClassifierCV(model, method='sigmoid', cv='prefit', n_jobs=-1)\n",
        "        calibrated_classifier.fit(X_train_transformed[rep], y_train)\n",
        "        calibrated_classifiers.append((rep, calibrated_classifier))\n",
        "\n",
        "    # Preparar dados calibrados para o meta-classificador\n",
        "    calibrated_meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, calibrated_classifier in calibrated_classifiers:\n",
        "            proba = calibrated_classifier.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())\n",
        "        calibrated_meta_features.append(instance_features)\n",
        "\n",
        "    calibrated_meta_features = np.array(calibrated_meta_features)\n",
        "\n",
        "    # Treinar o meta-classificador (utilizando MLP como exemplo)\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(calibrated_meta_features, y_train)\n",
        "\n",
        "    return calibrated_classifiers, meta_classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0z4yRoAnp11"
      },
      "source": [
        "### Predicao do meta-classificador - (CalibrationProba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jit\n",
        "def predict_with_meta_classifier(X_test, calibrated_base_models, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing Instances\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet)\n",
        "\n",
        "        instance_features = []\n",
        "        for rep, calibrated_classifier in calibrated_base_models:\n",
        "            proba = calibrated_classifier.predict_proba(x_transformed[rep][0].reshape(1, -1))  # Ajuste aqui para pegar o primeiro elemento\n",
        "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
        "\n",
        "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Predict (ArgmaxProba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'def combine_and_predict(X_transformed, trained_models):\\n    num_instances = len(next(iter(X_transformed.values())))  # Number of instances from the first transformed data\\n    num_classes = len(trained_models[next(iter(trained_models))].classes_)  # Number of classes from first model\\n    combined_probabilities = np.zeros((num_instances, num_classes))\\n\\n    for transformation_type, X_trans in X_transformed.items():\\n        model = trained_models[transformation_type]\\n        proba = model.predict_proba(X_trans)  # Get probabilities for all instances\\n        combined_probabilities += proba\\n\\n    combined_probabilities_reshaped = combined_probabilities.reshape(num_instances, -1, num_classes)\\n    predicted_classes = np.argmax(combined_probabilities_reshaped, axis=1) + 1  # Adding 1 to start classes from 1 instead of 0\\n    return predicted_classes\\n\\ndef train_with_meta_classifier(X_train, y_train, base_option=\\'1nn\\', meta_option=\\'rf\\', random_state=123, wavelet=None):\\n    trained_models = {}  # Salvar modelos treinados para cada transformação\\n    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\\n    loo = LeaveOneOut()\\n\\n    # Treinar um modelo para cada transformação e salvar no dicionário\\n    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour=\\'red\\', desc=\"Training Models\"):\\n        model = select_model(base_option, random_state)\\n        for train_index, _ in loo.split(X_trans):\\n            model.fit(X_trans[train_index], y_train[train_index])\\n        trained_models[rep] = model  # Salvar o modelo treinado\\n\\n    avg_proba = combine_and_predict(X_train_transformed, trained_models)\\n    # Train meta-classifier\\n    meta_classifier = select_model(meta_option, random_state)\\n    meta_classifier.fit(avg_proba, y_train)\\n\\n    return trained_models, meta_classifier\\n\\ndef predict_with_meta_classifier(X_test, trained_models, trained_meta_classifier, wavelet=None):\\n    predictions = []\\n    meta_features_test = []\\n    for i in tqdm(range(len(X_test)), ascii=True, colour=\\'green\\', desc=\"Testing Instances\"):\\n        x_instance = X_test[i].reshape(1,-1)\\n        x_transformed = transform_data_math(x_instance, wavelet)\\n        avg_proba = combine_and_predict(x_transformed, trained_models)\\n        meta_feature = avg_proba\\n        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\\n        meta_features_test.append(meta_feature)\\n    meta_features_test = np.array(meta_features_test)\\n    return predictions\\n'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"def combine_and_predict(X_transformed, trained_models):\n",
        "    num_instances = len(next(iter(X_transformed.values())))  # Number of instances from the first transformed data\n",
        "    num_classes = len(trained_models[next(iter(trained_models))].classes_)  # Number of classes from first model\n",
        "    combined_probabilities = np.zeros((num_instances, num_classes))\n",
        "\n",
        "    for transformation_type, X_trans in X_transformed.items():\n",
        "        model = trained_models[transformation_type]\n",
        "        proba = model.predict_proba(X_trans)  # Get probabilities for all instances\n",
        "        combined_probabilities += proba\n",
        "\n",
        "    combined_probabilities_reshaped = combined_probabilities.reshape(num_instances, -1, num_classes)\n",
        "    predicted_classes = np.argmax(combined_probabilities_reshaped, axis=1) + 1  # Adding 1 to start classes from 1 instead of 0\n",
        "    return predicted_classes\n",
        "\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='1nn', meta_option='rf', random_state=123, wavelet=None):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train, wavelet)  # Transformar todo o conjunto de treino\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "        trained_models[rep] = model  # Salvar o modelo treinado\n",
        "\n",
        "    avg_proba = combine_and_predict(X_train_transformed, trained_models)\n",
        "    # Train meta-classifier\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(avg_proba, y_train)\n",
        "\n",
        "    return trained_models, meta_classifier\n",
        "\n",
        "def predict_with_meta_classifier(X_test, trained_models, trained_meta_classifier, wavelet=None):\n",
        "    predictions = []\n",
        "    meta_features_test = []\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Testing Instances\"):\n",
        "        x_instance = X_test[i].reshape(1,-1)\n",
        "        x_transformed = transform_data_math(x_instance, wavelet)\n",
        "        avg_proba = combine_and_predict(x_transformed, trained_models)\n",
        "        meta_feature = avg_proba\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n",
        "        meta_features_test.append(meta_feature)\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "    return predictions\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7b562Qnp12"
      },
      "source": [
        "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ZRW1Zzql88iC"
      },
      "outputs": [],
      "source": [
        "dataset_quali_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace']\n",
        "dataset_full_list = ['Worms','FaceAll','SemgHandMovementCh2','Herring','GunPointAgeSpan','SmoothSubspace','SemgHandSubjectCh2','LargeKitchenAppliances','Plane','Fish','ScreenType','PhalangesOutlinesCorrect','CricketZ','MiddlePhalanxOutlineAgeGroup','ECG5000','Chinatown','ShapeletSim','MiddlePhalanxTW','Symbols','EOGHorizontalSignal','Ham','UMD','HouseTwenty','MiddlePhalanxOutlineCorrect','Wafer','Rock','DistalPhalanxTW','CricketY','FacesUCR','FiftyWords','Mallat','Strawberry','SwedishLeaf','ProximalPhalanxOutlineAgeGroup','MixedShapesRegularTrain','SmallKitchenAppliances','GunPointOldVersusYoung','Wine','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','Yoga','CinCECGTorso','ChlorineConcentration','ArrowHead','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','InsectEPGSmallTrain','WormsTwoClass','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','DistalPhalanxOutlineCorrect','ItalyPowerDemand','InsectWingbeatSound','BME','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','Meat','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','Earthquakes','BirdChicken','HandOutlines','BeetleFly','ACSF1','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
        "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataunique = ['Beef', 'Car', 'CBF', 'Coffee','DiatomSizeReduction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGKV11Rnp12",
        "outputId": "a82f769f-6037-4228-f0bf-31fe9408c66e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:03<00:00,  1.26it/s]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 60/60 [00:03<00:00, 19.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia Car: 0.8833333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [05:39<00:00, 67.95s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 276/276 [00:10<00:00, 26.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia DistalPhalanxOutlineCorrect: 0.7391304347826086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:01<00:00,  3.31it/s]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 54/54 [00:02<00:00, 25.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia Wine: 0.6296296296296297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [01:13<00:00, 14.72s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 205/205 [00:08<00:00, 24.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia ProximalPhalanxTW: 0.8146341463414634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [11:28<00:00, 137.72s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 390/390 [00:19<00:00, 19.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia CricketZ: 0.441025641025641\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [01:42<00:00, 20.42s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 154/154 [00:06<00:00, 24.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia MiddlePhalanxTW: 0.6038961038961039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [07:16<00:00, 87.35s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 391/391 [00:52<00:00,  7.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia Adiac: 0.7519181585677749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 12.11it/s]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 900/900 [00:34<00:00, 25.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia CBF: 0.8822222222222222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [1:43:54<00:00, 1246.87s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 500/500 [00:30<00:00, 16.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia EthanolLevel: 0.712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:18<00:00,  3.64s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 2425/2425 [02:02<00:00, 19.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia MixedShapesSmallTrain: 0.8032989690721649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [18:54<00:00, 226.81s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 375/375 [00:17<00:00, 21.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia SmallKitchenAppliances: 0.4826666666666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:10<00:00,  2.11s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 180/180 [00:06<00:00, 26.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia PowerCons: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [02:29<00:00, 29.80s/it]\n",
            "Testing Instances: 100%|\u001b[32m##########\u001b[0m| 208/208 [00:49<00:00,  4.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia PigAirwayPressure: 0.18269230769230768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models:   0%|\u001b[31m          \u001b[0m| 0/5 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "for dataset_name in univariate_equal_length:\n",
        "    Acc = []\n",
        "    dataset_accuracies = []\n",
        "    # Carregue os dados de treinamento e teste\n",
        "    features_train, features_test, target_train, target_test = load_data(dataset_name)\n",
        "    best_wavelet = choose_wavelet(features_train)\n",
        "\n",
        "    trained_models, meta_classifier = train_with_meta_classifier(features_train, target_train, base_option='svm', meta_option='lr', random_state=123, wavelet=best_wavelet)\n",
        "    \n",
        "    predictions = predict_with_meta_classifier(features_test, trained_models, meta_classifier, wavelet=best_wavelet)\n",
        "    \n",
        "    test_accuracy_meta = np.mean(predictions == target_test)\n",
        "    \n",
        "    dataset_accuracies.append(test_accuracy_meta)\n",
        "    \n",
        "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
        "    \n",
        "    Acc.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n",
        "\n",
        "accuracy_df = pd.DataFrame(Acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_df.to_csv('model_SVM+LR+CProba.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "AP38ocldnp10",
        "48vIj_NYnp14",
        "fBZ-XYnunp15",
        "dfFvUX1YU-xH",
        "-ru7Knb6G5Ca"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
