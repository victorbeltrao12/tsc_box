{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_wfxopCnp1x"
      },
      "source": [
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1AP99G_oHxu",
        "outputId": "73d44148-db3b-4584-e7e9-c0d331a5c87b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!pip install aeon\\n!pip install tsfresh\\n!pip install tslearn'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"!pip install aeon\n",
        "!pip install tsfresh\n",
        "!pip install tslearn\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nuvyez8anp1y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from aeon.datasets import load_classification\n",
        "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
        "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n",
        "\n",
        "from tsfresh import extract_features, select_features\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
        "\n",
        "import pywt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "from numba import jit\n",
        "from tqdm import tqdm\n",
        "import timeit\n",
        "from datetime import timedelta\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Time Series Classifier Dynamic Series Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesClassifier:\n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.trained_models = {}\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Returns:\n",
        "            _type_: _description_\n",
        "        \"\"\"\n",
        "    @jit\n",
        "    def transform_data_math(X):\n",
        "        n_sax_symbols = int(X.shape[1] / 4)\n",
        "        n_paa_segments = int(X.shape[1] / 4)\n",
        "    \n",
        "        X_fft = np.abs(fft(X, axis=1))\n",
        "    \n",
        "        coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
        "        X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "    \n",
        "        paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "        X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "        X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "        stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
        "                               np.std(X_paa, axis=1).reshape(-1,1),\n",
        "                               np.max(X_paa, axis=1).reshape(-1,1),\n",
        "                               np.min(X_paa, axis=1).reshape(-1,1),\n",
        "                               ])\n",
        "    \n",
        "        sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "        X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "        X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "        stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
        "                               np.std(X_sax, axis=1).reshape(-1,1),\n",
        "                               np.max(X_sax, axis=1).reshape(-1,1),\n",
        "                               np.min(X_sax, axis=1).reshape(-1,1),\n",
        "                               ])\n",
        "    \n",
        "        data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "        data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "        stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
        "                             np.std(data_X, axis=1).reshape(-1,1),\n",
        "                             np.max(data_X, axis=1).reshape(-1,1),\n",
        "                             np.min(data_X, axis=1).reshape(-1,1),\n",
        "                             ])\n",
        "    \n",
        "        data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "        data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "        stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
        "                               np.std(data_FFT, axis=1).reshape(-1,1),\n",
        "                               np.max(data_FFT, axis=1).reshape(-1,1),\n",
        "                               np.min(data_FFT, axis=1).reshape(-1,1),\n",
        "                               ])\n",
        "    \n",
        "        data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "        data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "        stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
        "                               np.std(data_DWT, axis=1).reshape(-1,1),\n",
        "                               np.max(data_DWT, axis=1).reshape(-1,1),\n",
        "                               np.min(data_DWT, axis=1).reshape(-1,1),\n",
        "                               ])\n",
        "    \n",
        "        return {\n",
        "            \"TS\": np.hstack([data_X, stats_X]),\n",
        "            \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
        "            \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
        "            \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
        "            \"SAX\": np.hstack([X_sax, stats_SAX])\n",
        "        }\n",
        "    \n",
        "    @jit\n",
        "    def select_model(option, random_state):\n",
        "        if option == '1nn':\n",
        "            return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
        "        elif option == '3nn':\n",
        "            return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
        "        elif option == 'svm':\n",
        "            return SVC(C = 100, gamma=0.01, kernel='linear', probability=True)\n",
        "        elif option == 'gbc':\n",
        "            return GradientBoostingClassifier(n_estimators=5, random_state=random_state)\n",
        "        elif option == 'nb':\n",
        "            return GaussianNB()\n",
        "        elif option == 'shape':\n",
        "            return ShapeDTW(n_neighbors=1)\n",
        "        elif option == 'ee':\n",
        "            return ElasticEnsemble(n_jobs=-1,\n",
        "                                   random_state=random_state,\n",
        "                                   majority_vote=True)\n",
        "        elif option == 'exrf':\n",
        "            return ExtraTreesClassifier(n_estimators=200,\n",
        "                                        criterion=\"entropy\",\n",
        "                                        class_weight=\"balanced\",\n",
        "                                        max_features=\"sqrt\",\n",
        "                                        n_jobs=-1,\n",
        "                                        random_state=None)\n",
        "        elif option == 'rd':\n",
        "            return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "        else:\n",
        "            return RandomForestClassifier(n_estimators=200,\n",
        "                                          criterion=\"entropy\",\n",
        "                                          class_weight=\"balanced\",\n",
        "                                          max_features=\"sqrt\",\n",
        "                                          n_jobs=-1,\n",
        "                                          random_state=None)\n",
        "    \n",
        "    @jit\n",
        "    def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
        "        trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "        X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
        "        loo = LeaveOneOut()\n",
        "        num_classes = len(np.unique(y_train))  # Número de classes\n",
        "    \n",
        "        # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "        for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n",
        "            model = select_model(base_option, random_state)\n",
        "            scores = []\n",
        "            for train_index, _ in loo.split(X_trans):\n",
        "                model.fit(X_trans[train_index], y_train[train_index])\n",
        "                score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
        "                scores.append(score)\n",
        "            avg_score = np.mean(scores)\n",
        "            trained_models[rep] = model  # Salvar o modelo treinado\n",
        "    \n",
        "        # Preparar dados para o meta-classificador\n",
        "        meta_features = np.zeros((X_train.shape[0], num_classes))  # Inicializar um vetor para armazenar as somas de probabilidades para cada classe\n",
        "        for i in range(X_train.shape[0]):\n",
        "            for rep, model in trained_models.items():\n",
        "                proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "                meta_features[i] += proba.flatten()  # Adicione as probabilidades para cada classe\n",
        "    \n",
        "        # Treinar o meta-classificador\n",
        "        meta_classifier = select_model(meta_option, random_state)\n",
        "        meta_classifier.fit(meta_features, y_train)\n",
        "    \n",
        "        return trained_models, meta_classifier\n",
        "    \n",
        "    @jit\n",
        "    def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
        "        predictions = []\n",
        "        meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
        "    \n",
        "        for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n",
        "            x_instance = X_test[i].reshape(1, -1)\n",
        "            x_transformed = transform_data_math(x_instance)\n",
        "    \n",
        "            instance_features = np.zeros(trained_meta_classifier.n_features_in_)  # Inicialize um vetor para armazenar as características do meta-classificador\n",
        "            for rep, model in trained_base_models.items():\n",
        "                proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))\n",
        "                instance_features += proba.flatten()  # Adicione as probabilidades para cada classe\n",
        "    \n",
        "            meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "            predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
        "    \n",
        "            meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
        "    \n",
        "        # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
        "        meta_features_test = np.array(meta_features_test)\n",
        "    \n",
        "        # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\n",
        "        np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n",
        "    \n",
        "        return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctG_4yBOnp1z"
      },
      "source": [
        "### Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "imNQaGTDnp10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'X_train, y_train = load_classification(\\'CBF\\', split=\"TRAIN\")\\nX_test, y_test = load_classification(\\'CBF\\', split=\"test\")\\n\\n# Achatando os dados para 2D, pois alguns algoritmos esperam 2D\\nXtrain = X_train.reshape(X_train.shape[0], -1)\\nXtest = X_test.reshape(X_test.shape[0], -1)'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"try:\n",
        "    train_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
        "    test_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure the Parquet files are in the correct path.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "X_train = train_data.drop('target', axis=1).values\n",
        "y_train = train_data['target'].values\n",
        "\n",
        "X_test = test_data.drop('target', axis=1).values\n",
        "y_test = test_data['target'].values\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"X_train, y_train = load_classification('CBF', split=\"TRAIN\")\n",
        "X_test, y_test = load_classification('CBF', split=\"test\")\n",
        "\n",
        "# Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
        "Xtrain = X_train.reshape(X_train.shape[0], -1)\n",
        "Xtest = X_test.reshape(X_test.shape[0], -1)\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAtr74np10"
      },
      "source": [
        "### Função de transformação dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V0NO-NIJnp10"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def transform_data_math(X):\n",
        "    n_sax_symbols = int(X.shape[1] / 4)\n",
        "    n_paa_segments = int(X.shape[1] / 4)\n",
        "\n",
        "    X_fft = np.abs(fft(X, axis=1))\n",
        "\n",
        "    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
        "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
        "\n",
        "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
        "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
        "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
        "    stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.std(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.max(X_paa, axis=1).reshape(-1,1),\n",
        "                           np.min(X_paa, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
        "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
        "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
        "    stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.std(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.max(X_sax, axis=1).reshape(-1,1),\n",
        "                           np.min(X_sax, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
        "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
        "    stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
        "                         np.std(data_X, axis=1).reshape(-1,1),\n",
        "                         np.max(data_X, axis=1).reshape(-1,1),\n",
        "                         np.min(data_X, axis=1).reshape(-1,1),\n",
        "                         ])\n",
        "\n",
        "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
        "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
        "    stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_FFT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_FFT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
        "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
        "    stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.std(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.max(data_DWT, axis=1).reshape(-1,1),\n",
        "                           np.min(data_DWT, axis=1).reshape(-1,1),\n",
        "                           ])\n",
        "\n",
        "    return {\n",
        "        \"TS\": np.hstack([data_X, stats_X]),\n",
        "        \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
        "        \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
        "        \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
        "        \"SAX\": np.hstack([X_sax, stats_SAX])\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP38ocldnp10"
      },
      "source": [
        "### Seleção do modelo extrator e modelo classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vaW30f6-np11"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def select_model(option, random_state):\n",
        "    if option == '1nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
        "    elif option == '3nn':\n",
        "        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
        "    elif option == 'svm':\n",
        "        return SVC(C = 100, gamma=0.01, kernel='linear', probability=True)\n",
        "    elif option == 'gbc':\n",
        "        return GradientBoostingClassifier(n_estimators=5, random_state=random_state)\n",
        "    elif option == 'nb':\n",
        "        return GaussianNB()\n",
        "    elif option == 'shape':\n",
        "        return ShapeDTW(n_neighbors=1)\n",
        "    elif option == 'ee':\n",
        "        return ElasticEnsemble(n_jobs=-1,\n",
        "                               random_state=random_state,\n",
        "                               majority_vote=True)\n",
        "    elif option == 'exrf':\n",
        "        return ExtraTreesClassifier(n_estimators=500,\n",
        "                                    criterion=\"entropy\",\n",
        "                                    class_weight=\"balanced\",\n",
        "                                    max_features=\"sqrt\",\n",
        "                                    n_jobs=-1,\n",
        "                                    random_state=None)\n",
        "    elif option == 'rd':\n",
        "        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
        "    else:\n",
        "        return RandomForestClassifier(n_estimators=200,\n",
        "                                      criterion=\"entropy\",\n",
        "                                      class_weight=\"balanced\",\n",
        "                                      max_features=\"sqrt\",\n",
        "                                      n_jobs=-1,\n",
        "                                      random_state=None)\n",
        "        #return RandomForestClassifier(n_estimators=100,random_state=random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mNDe8USnp11"
      },
      "source": [
        "### Treino do modelos extrator e classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4sRNwBWAnp11",
        "outputId": "bb54d289-d593-4c48-ee95-ec2f5b6c8857"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'@jit\\ndef train_with_meta_classifier(X_train, y_train, base_option=\\'random_forest\\', meta_option=\\'1nn\\', random_state=42):\\n    trained_models = {}  # Salvar modelos treinados para cada transformação\\n    best_model_indices = []\\n    best_probabilities = []\\n    X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\\n    loo = LeaveOneOut()\\n\\n    # Treinar um modelo para cada transformação e salvar no dicionário\\n    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\\n        model = select_model(base_option, random_state)\\n        scores = []\\n        for train_index, _ in loo.split(X_trans):\\n            model.fit(X_trans[train_index], y_train[train_index])\\n            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\\n            scores.append(score)\\n        avg_score = np.mean(scores)\\n        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\\n\\n    # Preparar dados para o meta-classificador\\n    meta_features = []\\n    for i in range(X_train.shape[0]):\\n        instance_features = []\\n        for rep, (model, _) in trained_models.items():\\n            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\\n            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\\n\\n        meta_features.append(instance_features)\\n\\n        best_model_index = None\\n        best_accuracy = -1\\n\\n        # Iterar sobre os modelos treinados e suas respectivas probabilidades\\n        for rep, (model, _) in trained_models.items():\\n            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\\n            accuracy = np.max(proba)  # Calcula a precisão usando a probabilidade máxima\\n\\n            # Se a precisão desse modelo for a melhor até agora, armazene o índice do modelo e as probabilidades\\n            if accuracy > best_accuracy:\\n                best_accuracy = accuracy\\n                best_model_index = rep\\n                best_proba = proba.flatten()\\n\\n        # Adicione o índice do modelo que obteve a melhor precisão e as probabilidades correspondentes às listas\\n        best_model_indices.append(best_model_index)\\n        best_probabilities.append(best_proba)\\n\\n    # Converta as listas em arrays NumPy\\n    best_model_indices = np.array(best_model_indices)\\n    best_probabilities = np.array(best_probabilities)\\n\\n    # Crie uma lista para armazenar as meta_features usando apenas as melhores probabilidades\\n    best_meta_features = []\\n\\n    # Iterar sobre as melhores probabilidades para cada instância e adicionar apenas essas probabilidades às meta_features\\n    for i in range(len(best_probabilities)):\\n        best_meta_features.append(best_probabilities[i])\\n\\n    # Converta a lista em um array NumPy\\n    best_meta_features = np.array(best_meta_features)\\n\\n    meta_features = np.array(meta_features)\\n    np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\\n\\n    # Treinar o meta-classificador\\n    meta_classifier = select_model(meta_option, random_state)\\n    meta_classifier.fit(meta_features, y_train)\\n\\n    return trained_models, meta_classifier\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"@jit\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    best_model_indices = []\n",
        "    best_probabilities = []\n",
        "    X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        scores = []\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
        "            scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
        "\n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        instance_features = []\n",
        "        for rep, (model, _) in trained_models.items():\n",
        "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
        "\n",
        "        meta_features.append(instance_features)\n",
        "\n",
        "        best_model_index = None\n",
        "        best_accuracy = -1\n",
        "\n",
        "        # Iterar sobre os modelos treinados e suas respectivas probabilidades\n",
        "        for rep, (model, _) in trained_models.items():\n",
        "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            accuracy = np.max(proba)  # Calcula a precisão usando a probabilidade máxima\n",
        "\n",
        "            # Se a precisão desse modelo for a melhor até agora, armazene o índice do modelo e as probabilidades\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_model_index = rep\n",
        "                best_proba = proba.flatten()\n",
        "\n",
        "        # Adicione o índice do modelo que obteve a melhor precisão e as probabilidades correspondentes às listas\n",
        "        best_model_indices.append(best_model_index)\n",
        "        best_probabilities.append(best_proba)\n",
        "\n",
        "    # Converta as listas em arrays NumPy\n",
        "    best_model_indices = np.array(best_model_indices)\n",
        "    best_probabilities = np.array(best_probabilities)\n",
        "\n",
        "    # Crie uma lista para armazenar as meta_features usando apenas as melhores probabilidades\n",
        "    best_meta_features = []\n",
        "\n",
        "    # Iterar sobre as melhores probabilidades para cada instância e adicionar apenas essas probabilidades às meta_features\n",
        "    for i in range(len(best_probabilities)):\n",
        "        best_meta_features.append(best_probabilities[i])\n",
        "\n",
        "    # Converta a lista em um array NumPy\n",
        "    best_meta_features = np.array(best_meta_features)\n",
        "\n",
        "    meta_features = np.array(meta_features)\n",
        "    np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\n",
        "\n",
        "    # Treinar o meta-classificador\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "\n",
        "    return trained_models, meta_classifier\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wmiiKtQv8-W4"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
        "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
        "    X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
        "    loo = LeaveOneOut()\n",
        "    num_classes = len(np.unique(y_train))  # Número de classes\n",
        "\n",
        "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
        "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n",
        "        model = select_model(base_option, random_state)\n",
        "        scores = []\n",
        "        for train_index, _ in loo.split(X_trans):\n",
        "            model.fit(X_trans[train_index], y_train[train_index])\n",
        "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
        "            scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "        trained_models[rep] = model  # Salvar o modelo treinado\n",
        "\n",
        "    # Preparar dados para o meta-classificador\n",
        "    meta_features = np.zeros((X_train.shape[0], num_classes))  # Inicializar um vetor para armazenar as somas de probabilidades para cada classe\n",
        "    for i in range(X_train.shape[0]):\n",
        "        for rep, model in trained_models.items():\n",
        "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
        "            meta_features[i] += proba.flatten()  # Adicione as probabilidades para cada classe\n",
        "\n",
        "    # Treinar o meta-classificador\n",
        "    meta_classifier = select_model(meta_option, random_state)\n",
        "    meta_classifier.fit(meta_features, y_train)\n",
        "\n",
        "    return trained_models, meta_classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0z4yRoAnp11"
      },
      "source": [
        "### Predicao do meta-classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QUeeYrisnp12"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
        "    predictions = []\n",
        "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
        "\n",
        "    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n",
        "        x_instance = X_test[i].reshape(1, -1)\n",
        "        x_transformed = transform_data_math(x_instance)\n",
        "\n",
        "        instance_features = np.zeros(trained_meta_classifier.n_features_in_)  # Inicialize um vetor para armazenar as características do meta-classificador\n",
        "        for rep, model in trained_base_models.items():\n",
        "            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))\n",
        "            instance_features += proba.flatten()  # Adicione as probabilidades para cada classe\n",
        "\n",
        "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
        "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
        "\n",
        "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
        "\n",
        "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
        "    meta_features_test = np.array(meta_features_test)\n",
        "\n",
        "    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\n",
        "    np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7b562Qnp12"
      },
      "source": [
        "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4Lypnx1w70HV"
      },
      "outputs": [],
      "source": [
        "dataset_full_list = ['Crop', 'ElectricDevices', 'EthanolLevel', 'FordA', 'FordB', 'InlineSkate', 'Phoneme', 'PigAirwayPressure', 'PigArtPressure', 'PigCVP', 'ShapesAll', 'StarLightCurves', 'UWaveGestureLibraryAll', 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibrarZ' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGKV11Rnp12",
        "outputId": "69e5f145-2736-4ca2-efbe-4678d1f562a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models: 100%|##########| 5/5 [14:47<00:00, 177.52s/it]\n",
            "Testing Instances: 100%|##########| 250/250 [01:20<00:00,  3.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia Computers: 0.612\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Base Models:   0%|          | 0/5 [1:25:57<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m X_test_flat \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m dataset_accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 14\u001b[0m trained_base_models, meta_classifier \u001b[38;5;241m=\u001b[39m train_with_meta_classifier(X_train_flat, y_train, base_option\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexrf\u001b[39m\u001b[38;5;124m'\u001b[39m, meta_option\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m predictions_test_meta \u001b[38;5;241m=\u001b[39m predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n\u001b[0;32m     16\u001b[0m test_accuracy_meta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(predictions_test_meta \u001b[38;5;241m==\u001b[39m y_test)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:445\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[1;32m--> 445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:446\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:187\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_estimator\u001b[39m(\u001b[38;5;28mself\u001b[39m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make and configure a copy of the `estimator_` attribute.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Warning: This method should be used to properly instantiate new\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    sub-estimators.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_)\n\u001b[0;32m    188\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{p: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_params})\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\base.py:75\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mfound in :ref:`randomness`.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39m__sklearn_clone__()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe\u001b[38;5;241m=\u001b[39msafe)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\base.py:268\u001b[0m, in \u001b[0;36mBaseEstimator.__sklearn_clone__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\base.py:106\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[0;32m    103\u001b[0m             )\n\u001b[0;32m    105\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m--> 106\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m new_object_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    108\u001b[0m     new_object_params[name] \u001b[38;5;241m=\u001b[39m clone(param, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\base.py:194\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03mGet parameters for this estimator.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    193\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[0;32m    195\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\site-packages\\sklearn\\base.py:159\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# to represent\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m init_signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(init)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[0;32m    161\u001b[0m parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    162\u001b[0m     p\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m init_signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m p\u001b[38;5;241m.\u001b[39mVAR_KEYWORD\n\u001b[0;32m    165\u001b[0m ]\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\inspect.py:3280\u001b[0m, in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   3278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Signature\u001b[38;5;241m.\u001b[39mfrom_callable(obj, follow_wrapped\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[0;32m   3281\u001b[0m                                    \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\inspect.py:3028\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[1;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   3026\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   3029\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[0;32m   3030\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\inspect.py:2516\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[0;32m   2511\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[0;32m   2513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[0;32m   2514\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[0;32m   2515\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[1;32m-> 2516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[0;32m   2517\u001b[0m                                     skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg,\n\u001b[0;32m   2518\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n\u001b[0;32m   2520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[0;32m   2522\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
            "File \u001b[1;32md:\\Programas\\Anaconda\\envs\\AMMsc\\Lib\\inspect.py:2423\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[1;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(Parameter(name, annotation\u001b[38;5;241m=\u001b[39mannotation,\n\u001b[0;32m   2419\u001b[0m                                 kind\u001b[38;5;241m=\u001b[39m_VAR_KEYWORD))\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;66;03m# Is 'func' is a pure Python function - don't validate the\u001b[39;00m\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;66;03m# parameters list (for correct order and defaults), it should be OK.\u001b[39;00m\n\u001b[1;32m-> 2423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(parameters,\n\u001b[0;32m   2424\u001b[0m            return_annotation\u001b[38;5;241m=\u001b[39mannotations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m, _empty),\n\u001b[0;32m   2425\u001b[0m            __validate_parameters__\u001b[38;5;241m=\u001b[39mis_duck_function)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dataset_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MoteStrain', 'OliveOil','MedicalImages', 'Trace', 'TwoPatterns']\n",
        "#finalizado 'Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MoteStrain', 'OliveOil','MedicalImages', 'Trace'\n",
        "# Para cada conjunto de dados na lista\n",
        "for dataset_name in dataset_full_list:\n",
        "    # Carregue os dados de treinamento e teste\n",
        "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
        "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
        "\n",
        "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    dataset_accuracies = []\n",
        "    trained_base_models, meta_classifier = train_with_meta_classifier(X_train_flat, y_train, base_option='exrf', meta_option='rd')\n",
        "    predictions_test_meta = predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n",
        "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
        "    dataset_accuracies.append(test_accuracy_meta)\n",
        "\n",
        "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
        "\n",
        "#np.savetxt(\"Results_MSLOO_.csv\", dataset_accuracies, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48vIj_NYnp14"
      },
      "source": [
        "### Gráfico das diferenças de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtbzTbNjnp14"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_hat' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y_hat  \u001b[38;5;66;03m# depois da transformação\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y2 \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m      6\u001b[0m z1 \u001b[38;5;241m=\u001b[39m y_hat_ \u001b[38;5;66;03m#antes da transformação\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y_hat' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y1 = y_hat  # depois da transformação\n",
        "y2 = y_test\n",
        "\n",
        "z1 = y_hat_ #antes da transformação\n",
        "z2 = y_test\n",
        "\n",
        "#suavizar os dados do gráfico\n",
        "window_size = 15\n",
        "y1_smoothed = pd.Series(y1).rolling(window=window_size).mean()\n",
        "y2_smoothed = pd.Series(y2).rolling(window=window_size).mean()\n",
        "z1_smoothed = pd.Series(z1).rolling(window=window_size).mean()\n",
        "z2_smoothed = pd.Series(z2).rolling(window=window_size).mean()\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained')\n",
        "\n",
        "# Conjunto de validação do classificador\n",
        "axs[0].set_title('Antes da transformação')\n",
        "axs[0].plot(z1_smoothed, label='Treino')\n",
        "axs[0].plot(z2_smoothed, label='Teste')\n",
        "axs[0].set_xlabel('Tempo (s)')\n",
        "axs[0].set_ylabel('Treino')\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Conjunto de validação do meta-classificador\n",
        "axs[1].set_title('Depois da transformação')\n",
        "axs[1].plot(y1_smoothed, label='Treino')\n",
        "axs[1].plot(y2_smoothed, label='Teste')\n",
        "axs[1].set_xlabel('Tempo (s)')\n",
        "axs[1].set_ylabel('Treino')\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRjM29_Xnp14"
      },
      "outputs": [],
      "source": [
        "w1 = y_hat  # meta-classificador\n",
        "w2 = y_hat_ #classificação\n",
        "\n",
        "# Suavizar os dados do gráfico\n",
        "window_size = 15\n",
        "w1_smoothed = pd.Series(w1).rolling(window=window_size).mean()\n",
        "w2_smoothed = pd.Series(w2).rolling(window=window_size).mean()\n",
        "\n",
        "# Plotar os dados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(w1_smoothed, label='w1 (Classificação usando meta-caracteristicas)')\n",
        "plt.plot(w2_smoothed, label='w2 (classificação utilizando dados brutos)')\n",
        "plt.xlabel('Tempo (s)')\n",
        "plt.ylabel('Valores suavizados')\n",
        "plt.title('Comparação entre os resultados de um SVM')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBZ-XYnunp15"
      },
      "source": [
        "### Treino em loop de todas as opções de classificadores disponiveis no Select Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOyRjuwQnp15"
      },
      "outputs": [],
      "source": [
        "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'ee', 'shape', 'rf', 'rd']\n",
        "for algo in algos:\n",
        "\n",
        "    print(f'Meta-classificador com modelo extrator {algo.upper()}')\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "        trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option=algo)\n",
        "        # Testing\n",
        "        predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
        "        test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
        "\n",
        "        print(f'Acurácia do teste usando o meta-classificador com modelo extrator {algo}: {test_accuracy_meta}')\n",
        "    except Exception as e:\n",
        "        print(f\"Ocorreu um erro no teste com o {algo}: {e}\")\n",
        "    print(\"-------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_wfxopCnp1x",
        "ctG_4yBOnp1z",
        "QuvAtr74np10",
        "AP38ocldnp10",
        "5mNDe8USnp11",
        "e0z4yRoAnp11",
        "jk7b562Qnp12",
        "48vIj_NYnp14",
        "fBZ-XYnunp15"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
