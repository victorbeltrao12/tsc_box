{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_wfxopCnp1x"
   },
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1AP99G_oHxu",
    "outputId": "73d44148-db3b-4584-e7e9-c0d331a5c87b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%pip install aeon\n",
    "%pip install tsfresh\n",
    "%pip install tslearn\n",
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "nuvyez8anp1y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from aeon.datasets import load_classification\n",
    "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
    "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
    "\n",
    "import pywt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctG_4yBOnp1z"
   },
   "source": [
    "### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "imNQaGTDnp10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train, y_train = load_classification(\\'CBF\\', split=\"TRAIN\")\\nX_test, y_test = load_classification(\\'CBF\\', split=\"test\")\\n\\n# Achatando os dados para 2D, pois alguns algoritmos esperam 2D\\nXtrain = X_train.reshape(X_train.shape[0], -1)\\nXtest = X_test.reshape(X_test.shape[0], -1)'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"try:\n",
    "    train_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
    "    test_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure the Parquet files are in the correct path.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "X_train = train_data.drop('target', axis=1).values\n",
    "y_train = train_data['target'].values\n",
    "\n",
    "X_test = test_data.drop('target', axis=1).values\n",
    "y_test = test_data['target'].values\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"X_train, y_train = load_classification('CBF', split=\"TRAIN\")\n",
    "X_test, y_test = load_classification('CBF', split=\"test\")\n",
    "\n",
    "# Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "Xtrain = X_train.reshape(X_train.shape[0], -1)\n",
    "Xtest = X_test.reshape(X_test.shape[0], -1)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuvAtr74np10"
   },
   "source": [
    "### Função de transformação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "V0NO-NIJnp10"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def transform_data_math(X):\n",
    "    n_sax_symbols = int(X.shape[1] / 4)\n",
    "    n_paa_segments = int(X.shape[1] / 4)\n",
    "\n",
    "    X_fft = np.abs(fft(X, axis=1))\n",
    "\n",
    "    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
    "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
    "\n",
    "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
    "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
    "    stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
    "                           np.std(X_paa, axis=1).reshape(-1,1),\n",
    "                           np.max(X_paa, axis=1).reshape(-1,1),\n",
    "                           np.min(X_paa, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
    "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
    "    stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
    "                           np.std(X_sax, axis=1).reshape(-1,1),\n",
    "                           np.max(X_sax, axis=1).reshape(-1,1),\n",
    "                           np.min(X_sax, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
    "    stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
    "                         np.std(data_X, axis=1).reshape(-1,1),\n",
    "                         np.max(data_X, axis=1).reshape(-1,1),\n",
    "                         np.min(data_X, axis=1).reshape(-1,1),\n",
    "                         ])\n",
    "\n",
    "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
    "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
    "    stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
    "                           np.std(data_FFT, axis=1).reshape(-1,1),\n",
    "                           np.max(data_FFT, axis=1).reshape(-1,1),\n",
    "                           np.min(data_FFT, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
    "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
    "    stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
    "                           np.std(data_DWT, axis=1).reshape(-1,1),\n",
    "                           np.max(data_DWT, axis=1).reshape(-1,1),\n",
    "                           np.min(data_DWT, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    return {\n",
    "        \"TS\": np.hstack([data_X, stats_X]),\n",
    "        \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
    "        \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
    "        \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
    "        \"SAX\": np.hstack([X_sax, stats_SAX])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP38ocldnp10"
   },
   "source": [
    "### Seleção do modelo extrator e modelo classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "vaW30f6-np11"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def select_model(option, random_state):\n",
    "    if option == '1nn':\n",
    "        return KNeighborsTimeSeriesClassifier(distance='euclidean',\n",
    "                                              n_neighbors=1,\n",
    "                                              n_jobs=-1)\n",
    "    elif option == '3nn':\n",
    "        return KNeighborsTimeSeriesClassifier(distance='msm',\n",
    "                                              n_neighbors=3,\n",
    "                                              weights='distance',\n",
    "                                              n_jobs=-1)\n",
    "    elif option == 'svm':\n",
    "        return SVC(C = 100,\n",
    "                   gamma=0.01,\n",
    "                   kernel='linear',\n",
    "                   probability=True)\n",
    "    elif option == 'gbc':\n",
    "        return GradientBoostingClassifier(n_estimators=5,\n",
    "                                          random_state=random_state)\n",
    "    elif option == 'nb':\n",
    "        return GaussianNB()\n",
    "    elif option == 'shape':\n",
    "        return ShapeDTW(n_neighbors=1)\n",
    "    elif option == 'ee':\n",
    "        return ElasticEnsemble(n_jobs=-1,\n",
    "                               random_state=random_state,\n",
    "                               majority_vote=True)\n",
    "    elif option == 'exrf':\n",
    "        return ExtraTreesClassifier(n_estimators=250,\n",
    "                                    criterion=\"entropy\",\n",
    "                                    class_weight=\"balanced\",\n",
    "                                    max_features=\"sqrt\",\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=None)\n",
    "    elif option == 'rd':\n",
    "        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))   \n",
    "    else:\n",
    "        return RandomForestClassifier(n_estimators=200,\n",
    "                                      criterion=\"entropy\",\n",
    "                                      class_weight=\"balanced\",\n",
    "                                      max_features=\"sqrt\",\n",
    "                                      n_jobs=-1,\n",
    "                                      random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mNDe8USnp11"
   },
   "source": [
    "### Treino do modelos extrator e classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4sRNwBWAnp11",
    "outputId": "bb54d289-d593-4c48-ee95-ec2f5b6c8857"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@jit\\ndef train_with_meta_classifier(X_train, y_train, base_option=\\'random_forest\\', meta_option=\\'1nn\\', random_state=42):\\n    trained_models = {}  # Salvar modelos treinados para cada transformação\\n    best_model_indices = []\\n    best_probabilities = []\\n    X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\\n    loo = LeaveOneOut()\\n\\n    # Treinar um modelo para cada transformação e salvar no dicionário\\n    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\\n        model = select_model(base_option, random_state)\\n        scores = []\\n        for train_index, _ in loo.split(X_trans):\\n            model.fit(X_trans[train_index], y_train[train_index])\\n            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\\n            scores.append(score)\\n        avg_score = np.mean(scores)\\n        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\\n\\n    # Preparar dados para o meta-classificador\\n    meta_features = []\\n    for i in range(X_train.shape[0]):\\n        instance_features = []\\n        for rep, (model, _) in trained_models.items():\\n            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\\n            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\\n\\n        meta_features.append(instance_features)\\n\\n        best_model_index = None\\n        best_accuracy = -1\\n\\n        # Iterar sobre os modelos treinados e suas respectivas probabilidades\\n        for rep, (model, _) in trained_models.items():\\n            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\\n            accuracy = np.max(proba)  # Calcula a precisão usando a probabilidade máxima\\n\\n            # Se a precisão desse modelo for a melhor até agora, armazene o índice do modelo e as probabilidades\\n            if accuracy > best_accuracy:\\n                best_accuracy = accuracy\\n                best_model_index = rep\\n                best_proba = proba.flatten()\\n\\n        # Adicione o índice do modelo que obteve a melhor precisão e as probabilidades correspondentes às listas\\n        best_model_indices.append(best_model_index)\\n        best_probabilities.append(best_proba)\\n\\n    # Converta as listas em arrays NumPy\\n    best_model_indices = np.array(best_model_indices)\\n    best_probabilities = np.array(best_probabilities)\\n\\n    # Crie uma lista para armazenar as meta_features usando apenas as melhores probabilidades\\n    best_meta_features = []\\n\\n    # Iterar sobre as melhores probabilidades para cada instância e adicionar apenas essas probabilidades às meta_features\\n    for i in range(len(best_probabilities)):\\n        best_meta_features.append(best_probabilities[i])\\n\\n    # Converta a lista em um array NumPy\\n    best_meta_features = np.array(best_meta_features)\\n\\n    meta_features = np.array(meta_features)\\n    np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\\n\\n    # Treinar o meta-classificador\\n    meta_classifier = select_model(meta_option, random_state)\\n    meta_classifier.fit(meta_features, y_train)\\n\\n    return trained_models, meta_classifier\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"@jit\n",
    "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
    "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
    "    best_model_indices = []\n",
    "    best_probabilities = []\n",
    "    X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
    "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n",
    "        model = select_model(base_option, random_state)\n",
    "        scores = []\n",
    "        for train_index, _ in loo.split(X_trans):\n",
    "            model.fit(X_trans[train_index], y_train[train_index])\n",
    "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
    "\n",
    "    # Preparar dados para o meta-classificador\n",
    "    meta_features = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        instance_features = []\n",
    "        for rep, (model, _) in trained_models.items():\n",
    "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
    "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
    "\n",
    "        meta_features.append(instance_features)\n",
    "\n",
    "        best_model_index = None\n",
    "        best_accuracy = -1\n",
    "\n",
    "        # Iterar sobre os modelos treinados e suas respectivas probabilidades\n",
    "        for rep, (model, _) in trained_models.items():\n",
    "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
    "            accuracy = np.max(proba)  # Calcula a precisão usando a probabilidade máxima\n",
    "\n",
    "            # Se a precisão desse modelo for a melhor até agora, armazene o índice do modelo e as probabilidades\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_index = rep\n",
    "                best_proba = proba.flatten()\n",
    "\n",
    "        # Adicione o índice do modelo que obteve a melhor precisão e as probabilidades correspondentes às listas\n",
    "        best_model_indices.append(best_model_index)\n",
    "        best_probabilities.append(best_proba)\n",
    "\n",
    "    # Converta as listas em arrays NumPy\n",
    "    best_model_indices = np.array(best_model_indices)\n",
    "    best_probabilities = np.array(best_probabilities)\n",
    "\n",
    "    # Crie uma lista para armazenar as meta_features usando apenas as melhores probabilidades\n",
    "    best_meta_features = []\n",
    "\n",
    "    # Iterar sobre as melhores probabilidades para cada instância e adicionar apenas essas probabilidades às meta_features\n",
    "    for i in range(len(best_probabilities)):\n",
    "        best_meta_features.append(best_probabilities[i])\n",
    "\n",
    "    # Converta a lista em um array NumPy\n",
    "    best_meta_features = np.array(best_meta_features)\n",
    "\n",
    "    meta_features = np.array(meta_features)\n",
    "    np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\n",
    "\n",
    "    # Treinar o meta-classificador\n",
    "    meta_classifier = select_model(meta_option, random_state)\n",
    "    meta_classifier.fit(meta_features, y_train)\n",
    "\n",
    "    return trained_models, meta_classifier\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "wmiiKtQv8-W4"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
    "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
    "    X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(X_train_transformed)\n",
    "    num_classes = len(np.unique(y_train))  # Número de classes\n",
    "\n",
    "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
    "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Models\", colour='red'):\n",
    "        model = select_model(base_option, random_state)\n",
    "        for train_index, _ in loo.split(X_trans):\n",
    "            model.fit(X_trans[train_index], y_train[train_index])\n",
    "        trained_models[rep] = model  # Salvar o modelo treinado\n",
    "\n",
    "    # Preparar dados para o meta-classificador\n",
    "    meta_features = np.zeros((X_train.shape[0], num_classes))  # Inicializar um vetor para armazenar as somas de probabilidades para cada classe\n",
    "    for i in range(0, X_train.shape[0]):\n",
    "        for rep, model in trained_models.items():\n",
    "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
    "            proba /= np.sum(proba)\n",
    "            meta_features[i] += proba.flatten()  # Adicione as probabilidades para cada classe\n",
    "\n",
    "    # Treinar o meta-classificador\n",
    "    meta_classifier = select_model(meta_option, random_state)\n",
    "    meta_classifier.fit(meta_features, y_train)\n",
    "\n",
    "    return trained_models, meta_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0z4yRoAnp11"
   },
   "source": [
    "### Predicao do meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "QUeeYrisnp12"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
    "    predictions = []\n",
    "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
    "\n",
    "    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing\", colour='green'):\n",
    "        x_instance = X_test[i].reshape(1, -1)\n",
    "        x_transformed = transform_data_math(x_instance)\n",
    "\n",
    "        instance_features = np.zeros(trained_meta_classifier.n_features_in_)  # Inicialize um vetor para armazenar as características do meta-classificador\n",
    "        for rep, model in trained_base_models.items():\n",
    "            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))\n",
    "            instance_features += proba.flatten()  # Adicione as probabilidades para cada classe\n",
    "\n",
    "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
    "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
    "\n",
    "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
    "\n",
    "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
    "    meta_features_test = np.array(meta_features_test)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk7b562Qnp12"
   },
   "source": [
    "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_quali_list = [ 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n",
    "dataset_full_list = ['MixedShapesRegularTrain','SmallKitchenAppliances','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','CinCECGTorso','ChlorineConcentration','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','WormsTwoClass','DistalPhalanxOutlineCorrect','InsectWingbeatSound','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','HandOutlines','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
    "rapidos = ['SwedishLeaf', 'ProximalPhalanxOutlineAgeGroup', 'GunPointOldVersusYoung', 'Wine', 'Yoga', 'ArrowHead', 'InsectEPGSmallTrain','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','ItalyPowerDemand','BME','Meat','Earthquakes','BirdChicken','BeetleFly','ACSF1']\n",
    "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFGKV11Rnp12",
    "outputId": "69e5f145-2736-4ca2-efbe-4678d1f562a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 66.91it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:00<00:00, 128.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Beef: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 25.34it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 60/60 [00:00<00:00, 109.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Car: 0.7333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 81.20it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 900/900 [00:04<00:00, 185.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia CBF: 0.8822222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 51.21it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 28/28 [00:00<00:00, 169.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Coffee: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 100.84it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 306/306 [00:02<00:00, 144.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia DiatomSizeReduction: 0.934640522875817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 17.67it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 100/100 [00:00<00:00, 188.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECG200: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 80.67it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 861/861 [00:04<00:00, 183.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECGFiveDays: 0.8536585365853658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 85.73it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 88/88 [00:00<00:00, 135.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia FaceFour: 0.7954545454545454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 35.68it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 150/150 [00:00<00:00, 159.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia GunPoint: 0.9133333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 25.43it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 61/61 [00:00<00:00, 104.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning2: 0.819672131147541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 27.25it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 73/73 [00:00<00:00, 146.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning7: 0.6027397260273972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:01<00:00,  4.76it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 760/760 [00:04<00:00, 156.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MedicalImages: 0.6986842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 51.05it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 1252/1252 [00:06<00:00, 198.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MoteStrain: 0.8490415335463258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 48.16it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:00<00:00, 115.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia OliveOil: 0.8666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 92.62it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 601/601 [00:02<00:00, 202.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SonyAIBORobotSurface1: 0.7104825291181365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 42.83it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 953/953 [00:04<00:00, 204.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SonyAIBORobotSurface2: 0.8730325288562435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00,  6.52it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 300/300 [00:01<00:00, 163.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SyntheticControl: 0.9633333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 17.11it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 100/100 [00:00<00:00, 148.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Trace: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:04<00:00,  1.12it/s]\n",
      "Testing: 100%|\u001b[32m##########\u001b[0m| 4000/4000 [00:35<00:00, 113.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia TwoPatterns: 0.9315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "accuracy_data = []\n",
    "for dataset_name in dataset_quali_list:\n",
    "    # Carregue os dados de treinamento e teste\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "\n",
    "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labels = le.fit_transform(y_train)\n",
    "    true_labels = le.transform(y_test)\n",
    "\n",
    "    dataset_accuracies = []\n",
    "    trained_base_models, meta_classifier = train_with_meta_classifier(X_train_flat, labels, base_option='1nn', meta_option='rd')\n",
    "    predictions_test_meta = predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n",
    "    test_accuracy_meta = np.mean(predictions_test_meta == true_labels)\n",
    "    dataset_accuracies.append(test_accuracy_meta)\n",
    "\n",
    "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
    "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beef</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Car</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBF</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DiatomSizeReduction</td>\n",
       "      <td>0.934641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG200</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ECGFiveDays</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FaceFour</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GunPoint</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lightning2</td>\n",
       "      <td>0.819672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lightning7</td>\n",
       "      <td>0.602740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MedicalImages</td>\n",
       "      <td>0.698684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MoteStrain</td>\n",
       "      <td>0.849042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OliveOil</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SonyAIBORobotSurface1</td>\n",
       "      <td>0.710483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SonyAIBORobotSurface2</td>\n",
       "      <td>0.873033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SyntheticControl</td>\n",
       "      <td>0.963333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Trace</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TwoPatterns</td>\n",
       "      <td>0.931500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Dataset Name  Accuracy\n",
       "0                    Beef  0.666667\n",
       "1                     Car  0.733333\n",
       "2                     CBF  0.882222\n",
       "3                  Coffee  1.000000\n",
       "4     DiatomSizeReduction  0.934641\n",
       "5                  ECG200  0.870000\n",
       "6             ECGFiveDays  0.853659\n",
       "7                FaceFour  0.795455\n",
       "8                GunPoint  0.913333\n",
       "9              Lightning2  0.819672\n",
       "10             Lightning7  0.602740\n",
       "11          MedicalImages  0.698684\n",
       "12             MoteStrain  0.849042\n",
       "13               OliveOil  0.866667\n",
       "14  SonyAIBORobotSurface1  0.710483\n",
       "15  SonyAIBORobotSurface2  0.873033\n",
       "16       SyntheticControl  0.963333\n",
       "17                  Trace  0.790000\n",
       "18            TwoPatterns  0.931500"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_df.to_csv('model_votingCLF+NN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48vIj_NYnp14"
   },
   "source": [
    "### Gráfico das diferenças de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtbzTbNjnp14"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y1 = y_hat  # depois da transformação\n",
    "y2 = y_test\n",
    "\n",
    "z1 = y_hat_ #antes da transformação\n",
    "z2 = y_test\n",
    "\n",
    "#suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "y1_smoothed = pd.Series(y1).rolling(window=window_size).mean()\n",
    "y2_smoothed = pd.Series(y2).rolling(window=window_size).mean()\n",
    "z1_smoothed = pd.Series(z1).rolling(window=window_size).mean()\n",
    "z2_smoothed = pd.Series(z2).rolling(window=window_size).mean()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained')\n",
    "\n",
    "# Conjunto de validação do classificador\n",
    "axs[0].set_title('Antes da transformação')\n",
    "axs[0].plot(z1_smoothed, label='Treino')\n",
    "axs[0].plot(z2_smoothed, label='Teste')\n",
    "axs[0].set_xlabel('Tempo (s)')\n",
    "axs[0].set_ylabel('Treino')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Conjunto de validação do meta-classificador\n",
    "axs[1].set_title('Depois da transformação')\n",
    "axs[1].plot(y1_smoothed, label='Treino')\n",
    "axs[1].plot(y2_smoothed, label='Teste')\n",
    "axs[1].set_xlabel('Tempo (s)')\n",
    "axs[1].set_ylabel('Treino')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRjM29_Xnp14"
   },
   "outputs": [],
   "source": [
    "w1 = y_hat  # meta-classificador\n",
    "w2 = y_hat_ #classificação\n",
    "\n",
    "# Suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "w1_smoothed = pd.Series(w1).rolling(window=window_size).mean()\n",
    "w2_smoothed = pd.Series(w2).rolling(window=window_size).mean()\n",
    "\n",
    "# Plotar os dados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(w1_smoothed, label='w1 (Classificação usando meta-caracteristicas)')\n",
    "plt.plot(w2_smoothed, label='w2 (classificação utilizando dados brutos)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Valores suavizados')\n",
    "plt.title('Comparação entre os resultados de um SVM')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBZ-XYnunp15"
   },
   "source": [
    "### Treino em loop de todas as opções de classificadores disponiveis no Select Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOyRjuwQnp15"
   },
   "outputs": [],
   "source": [
    "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'ee', 'shape', 'rf', 'rd']\n",
    "for algo in algos:\n",
    "\n",
    "    print(f'Meta-classificador com modelo extrator {algo.upper()}')\n",
    "\n",
    "    # Training\n",
    "    try:\n",
    "        trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option=algo)\n",
    "        # Testing\n",
    "        predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
    "        test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "\n",
    "        print(f'Acurácia do teste usando o meta-classificador com modelo extrator {algo}: {test_accuracy_meta}')\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro no teste com o {algo}: {e}\")\n",
    "    print(\"-------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w_wfxopCnp1x",
    "ctG_4yBOnp1z",
    "QuvAtr74np10",
    "AP38ocldnp10",
    "5mNDe8USnp11",
    "e0z4yRoAnp11",
    "jk7b562Qnp12",
    "48vIj_NYnp14",
    "fBZ-XYnunp15"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
