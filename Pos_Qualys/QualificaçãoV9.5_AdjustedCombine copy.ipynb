{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_wfxopCnp1x"
   },
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1AP99G_oHxu",
    "outputId": "73d44148-db3b-4584-e7e9-c0d331a5c87b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "%pip install aeon\n",
    "%pip install tsfresh\n",
    "%pip install tslearn\n",
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "%pip install xgboost\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "nuvyez8anp1y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "from scipy.fftpack import fft\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from more_itertools import powerset\n",
    "\n",
    "from aeon.datasets import load_classification\n",
    "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut, StratifiedKFold, train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import copy as cp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Classifier Dynamic Series Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesClassifier:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.trained_models = {}\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "    @staticmethod\n",
    "    def transform_data_math(X):\n",
    "        n_sax_symbols = int(X.shape[1] / 4)\n",
    "        n_paa_segments = int(X.shape[1] / 4)\n",
    "    \n",
    "        X_fft = np.abs(fft(X, axis=1))\n",
    "    \n",
    "        coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
    "        X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
    "    \n",
    "        paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "        X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
    "        X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
    "        stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
    "                               np.std(X_paa, axis=1).reshape(-1,1),\n",
    "                               np.max(X_paa, axis=1).reshape(-1,1),\n",
    "                               np.min(X_paa, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "        X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
    "        X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
    "        stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
    "                               np.std(X_sax, axis=1).reshape(-1,1),\n",
    "                               np.max(X_sax, axis=1).reshape(-1,1),\n",
    "                               np.min(X_sax, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "        data_X.resize(data_X.shape[0], data_X.shape[1])\n",
    "        stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
    "                             np.std(data_X, axis=1).reshape(-1,1),\n",
    "                             np.max(data_X, axis=1).reshape(-1,1),\n",
    "                             np.min(data_X, axis=1).reshape(-1,1),\n",
    "                             ])\n",
    "    \n",
    "        data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
    "        data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
    "        stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
    "                               np.std(data_FFT, axis=1).reshape(-1,1),\n",
    "                               np.max(data_FFT, axis=1).reshape(-1,1),\n",
    "                               np.min(data_FFT, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
    "        data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
    "        stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
    "                               np.std(data_DWT, axis=1).reshape(-1,1),\n",
    "                               np.max(data_DWT, axis=1).reshape(-1,1),\n",
    "                               np.min(data_DWT, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        return {\n",
    "            \"TS\": np.hstack([data_X, stats_X]),\n",
    "            \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
    "            \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
    "            \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
    "            \"SAX\": np.hstack([X_sax, stats_SAX])\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def select_model(option, random_state):\n",
    "        if option == '1nn':\n",
    "            return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
    "        elif option == '3nn':\n",
    "            return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
    "        elif option == 'svm':\n",
    "            return SVC(C = 100, gamma=0.01, kernel='linear', probability=True)\n",
    "        elif option == 'gbc':\n",
    "            return GradientBoostingClassifier(n_estimators=5, random_state=random_state)\n",
    "        elif option == 'nb':\n",
    "            return GaussianNB()\n",
    "        elif option == 'shape':\n",
    "            return ShapeDTW(n_neighbors=1)\n",
    "        elif option == 'ee':\n",
    "            return ElasticEnsemble(n_jobs=-1,\n",
    "                                   random_state=random_state,\n",
    "                                   majority_vote=True)\n",
    "        elif option == 'exrf':\n",
    "            return ExtraTreesClassifier(n_estimators=200,\n",
    "                                        criterion=\"entropy\",\n",
    "                                        class_weight=\"balanced\",\n",
    "                                        max_features=\"sqrt\",\n",
    "                                        oob_score=True,\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=None)\n",
    "        elif option == 'rd':\n",
    "            return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        else:\n",
    "            return RandomForestClassifier(n_estimators=200,\n",
    "                                          criterion=\"gini\",\n",
    "                                          class_weight=\"balanced_subsample\",\n",
    "                                          max_features=\"sqrt\",\n",
    "                                          n_jobs=-1,\n",
    "                                          random_state=None)\n",
    "    \n",
    "    def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
    "        trained_models = {}  # Salvar modelos treinados para cada transformação\n",
    "        X_train_transformed = TimeSeriesClassifier.transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
    "        loo = LeaveOneOut()\n",
    "        num_classes = len(np.unique(y_train))  # Número de classes\n",
    "\n",
    "        level_0_classifiers = dict()\n",
    "        level_0_classifiers[\"gbc\"] = GradientBoostingClassifier(random_state=random_state)\n",
    "        level_0_classifiers[\"forest\"] = RandomForestClassifier(n_estimators=200, criterion=\"gini\", max_features=\"sqrt\", n_jobs=-1, random_state=random_state)\n",
    "        level_0_classifiers[\"xgboost\"] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=random_state)\n",
    "        level_0_classifiers[\"xtrees\"] = ExtraTreesClassifier(n_estimators=200, criterion=\"entropy\", max_features=\"sqrt\", n_jobs=-1, random_state=random_state)\n",
    "\n",
    "        level_1_classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "        stacking_model = StackingClassifier(estimators=list(level_0_classifiers.items()), final_estimator=level_1_classifier, passthrough=True, cv=kfold, stack_method=\"predict_proba\")\n",
    "\n",
    "        # Treinar um modelo para cada transformação e salvar no dicionário\n",
    "        for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Models\"):\n",
    "            trained_stacking_model = stacking_model\n",
    "            for train_index, _ in loo.split(X_trans):\n",
    "                trained_stacking_model.fit_transform(X_trans[train_index], y_train[train_index])\n",
    "            trained_models[rep] = trained_stacking_model  # Salvar o modelo treinado\n",
    "\n",
    "        # Preparar dados para o meta-classificador\n",
    "        meta_features = np.zeros((X_train.shape[0], num_classes))  # Inicializar um vetor para armazenar as somas de probabilidades para cada classe\n",
    "        for i in range(X_train.shape[0]):\n",
    "            for rep, stacking_model in trained_models.items():\n",
    "                proba = stacking_model.predict(X_train_transformed[rep][i].reshape(1, -1))\n",
    "                meta_features[i] += proba.flatten()  # Adicione as probabilidades para cada classe\n",
    "\n",
    "        # Treinar o meta-classificador\n",
    "        meta_classifier = TimeSeriesClassifier.select_model(meta_option, random_state)\n",
    "        meta_classifier.fit(meta_features, y_train)\n",
    "\n",
    "        return trained_models, meta_classifier\n",
    "    \n",
    "    \n",
    "    def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
    "        predictions = []\n",
    "        meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
    "\n",
    "        for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Predict\"):\n",
    "            x_instance = X_test[i].reshape(1, -1)\n",
    "            x_transformed = TimeSeriesClassifier.transform_data_math(x_instance)\n",
    "\n",
    "            instance_features = np.zeros(trained_meta_classifier.n_features_in_)  # Inicialize um vetor para armazenar as características do meta-classificador\n",
    "            for rep, model in trained_base_models.items():\n",
    "                proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))\n",
    "                instance_features += proba.flatten()  # Adicione as probabilidades para cada classe\n",
    "\n",
    "            meta_feature = np.array(instance_features).reshape(1, -1)\n",
    "            predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
    "\n",
    "            meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
    "\n",
    "        # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
    "        meta_features_test = np.array(meta_features_test)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk7b562Qnp12"
   },
   "source": [
    "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_quali_list = [ 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n",
    "dataset_full_list = ['MixedShapesRegularTrain','SmallKitchenAppliances','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','CinCECGTorso','ChlorineConcentration','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','WormsTwoClass','DistalPhalanxOutlineCorrect','InsectWingbeatSound','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','HandOutlines','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
    "rapidos = ['SwedishLeaf', 'ProximalPhalanxOutlineAgeGroup', 'GunPointOldVersusYoung', 'Wine', 'Yoga', 'ArrowHead', 'InsectEPGSmallTrain','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','ItalyPowerDemand','BME','Meat','Earthquakes','BirdChicken','BeetleFly','ACSF1']\n",
    "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFGKV11Rnp12",
    "outputId": "69e5f145-2736-4ca2-efbe-4678d1f562a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [1:23:40<00:00, 1004.14s/it]\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('float64'), dtype('<U1')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m X_test_flat \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m dataset_accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 11\u001b[0m base_models, meta_classifier \u001b[38;5;241m=\u001b[39m TimeSeriesClassifier\u001b[38;5;241m.\u001b[39mtrain_with_meta_classifier(X_train_flat, y_train, base_option\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexrf\u001b[39m\u001b[38;5;124m'\u001b[39m, meta_option\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m predictions_test_meta \u001b[38;5;241m=\u001b[39m TimeSeriesClassifier\u001b[38;5;241m.\u001b[39mpredict_with_meta_classifier(X_test_flat, base_models, meta_classifier)\n\u001b[0;32m     13\u001b[0m test_accuracy_meta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(predictions_test_meta \u001b[38;5;241m==\u001b[39m y_test)\n",
      "Cell \u001b[1;32mIn[57], line 135\u001b[0m, in \u001b[0;36mTimeSeriesClassifier.train_with_meta_classifier\u001b[1;34m(X_train, y_train, base_option, meta_option, random_state)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rep, stacking_model \u001b[38;5;129;01min\u001b[39;00m trained_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    134\u001b[0m         proba \u001b[38;5;241m=\u001b[39m stacking_model\u001b[38;5;241m.\u001b[39mpredict(X_train_transformed[rep][i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 135\u001b[0m         meta_features[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m proba\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Adicione as probabilidades para cada classe\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Treinar o meta-classificador\u001b[39;00m\n\u001b[0;32m    138\u001b[0m meta_classifier \u001b[38;5;241m=\u001b[39m TimeSeriesClassifier\u001b[38;5;241m.\u001b[39mselect_model(meta_option, random_state)\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('float64'), dtype('<U1')) -> None"
     ]
    }
   ],
   "source": [
    "accuracy_data = []\n",
    "for dataset_name in dataset_quali_list:\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "    \n",
    "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    dataset_accuracies = []\n",
    "    base_models, meta_classifier = TimeSeriesClassifier.train_with_meta_classifier(X_train_flat, y_train, base_option='exrf', meta_option='rd')\n",
    "    predictions_test_meta = TimeSeriesClassifier.predict_with_meta_classifier(X_test_flat, base_models, meta_classifier)\n",
    "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "    dataset_accuracies.append(test_accuracy_meta)\n",
    "\n",
    "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
    "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beef</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Car</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBF</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DiatomSizeReduction</td>\n",
       "      <td>0.983660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset Name  Accuracy\n",
       "0                 Beef  0.733333\n",
       "1                  Car  0.916667\n",
       "2                  CBF  1.000000\n",
       "3               Coffee  1.000000\n",
       "4  DiatomSizeReduction  0.983660"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_df.to_csv('model_votingCLF+NN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48vIj_NYnp14"
   },
   "source": [
    "### Gráfico das diferenças de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtbzTbNjnp14"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y_hat  \u001b[38;5;66;03m# depois da transformação\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y2 \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m      6\u001b[0m z1 \u001b[38;5;241m=\u001b[39m y_hat_ \u001b[38;5;66;03m#antes da transformação\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_hat' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y1 = y_hat  # depois da transformação\n",
    "y2 = y_test\n",
    "\n",
    "z1 = y_hat_ #antes da transformação\n",
    "z2 = y_test\n",
    "\n",
    "#suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "y1_smoothed = pd.Series(y1).rolling(window=window_size).mean()\n",
    "y2_smoothed = pd.Series(y2).rolling(window=window_size).mean()\n",
    "z1_smoothed = pd.Series(z1).rolling(window=window_size).mean()\n",
    "z2_smoothed = pd.Series(z2).rolling(window=window_size).mean()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained')\n",
    "\n",
    "# Conjunto de validação do classificador\n",
    "axs[0].set_title('Antes da transformação')\n",
    "axs[0].plot(z1_smoothed, label='Treino')\n",
    "axs[0].plot(z2_smoothed, label='Teste')\n",
    "axs[0].set_xlabel('Tempo (s)')\n",
    "axs[0].set_ylabel('Treino')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Conjunto de validação do meta-classificador\n",
    "axs[1].set_title('Depois da transformação')\n",
    "axs[1].plot(y1_smoothed, label='Treino')\n",
    "axs[1].plot(y2_smoothed, label='Teste')\n",
    "axs[1].set_xlabel('Tempo (s)')\n",
    "axs[1].set_ylabel('Treino')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRjM29_Xnp14"
   },
   "outputs": [],
   "source": [
    "w1 = y_hat  # meta-classificador\n",
    "w2 = y_hat_ #classificação\n",
    "\n",
    "# Suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "w1_smoothed = pd.Series(w1).rolling(window=window_size).mean()\n",
    "w2_smoothed = pd.Series(w2).rolling(window=window_size).mean()\n",
    "\n",
    "# Plotar os dados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(w1_smoothed, label='w1 (Classificação usando meta-caracteristicas)')\n",
    "plt.plot(w2_smoothed, label='w2 (classificação utilizando dados brutos)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Valores suavizados')\n",
    "plt.title('Comparação entre os resultados de um SVM')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBZ-XYnunp15"
   },
   "source": [
    "### Treino em loop de todas as opções de classificadores disponiveis no Select Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOyRjuwQnp15"
   },
   "outputs": [],
   "source": [
    "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'ee', 'shape', 'rf', 'rd']\n",
    "for algo in algos:\n",
    "\n",
    "    print(f'Meta-classificador com modelo extrator {algo.upper()}')\n",
    "\n",
    "    # Training\n",
    "    try:\n",
    "        trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option=algo)\n",
    "        # Testing\n",
    "        predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
    "        test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "\n",
    "        print(f'Acurácia do teste usando o meta-classificador com modelo extrator {algo}: {test_accuracy_meta}')\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro no teste com o {algo}: {e}\")\n",
    "    print(\"-------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w_wfxopCnp1x",
    "ctG_4yBOnp1z",
    "QuvAtr74np10",
    "AP38ocldnp10",
    "5mNDe8USnp11",
    "e0z4yRoAnp11",
    "jk7b562Qnp12",
    "48vIj_NYnp14",
    "fBZ-XYnunp15"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
