{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_wfxopCnp1x"
   },
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1AP99G_oHxu",
    "outputId": "73d44148-db3b-4584-e7e9-c0d331a5c87b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%pip install aeon\n",
    "%pip install tsfresh\n",
    "%pip install tslearn\n",
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nuvyez8anp1y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from aeon.datasets import load_classification\n",
    "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
    "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
    "\n",
    "import pywt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Classifier Dynamic Series Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesClassifier:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.trained_models = {}\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "    @jit\n",
    "    def transform_data_math(X):\n",
    "        n_sax_symbols = int(X.shape[1] / 4)\n",
    "        n_paa_segments = int(X.shape[1] / 4)\n",
    "    \n",
    "        X_fft = np.abs(fft(X, axis=1))\n",
    "    \n",
    "        coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
    "        X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
    "    \n",
    "        paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "        X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
    "        X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
    "        stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1),\n",
    "                               np.std(X_paa, axis=1).reshape(-1,1),\n",
    "                               np.max(X_paa, axis=1).reshape(-1,1),\n",
    "                               np.min(X_paa, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "        X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
    "        X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
    "        stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1),\n",
    "                               np.std(X_sax, axis=1).reshape(-1,1),\n",
    "                               np.max(X_sax, axis=1).reshape(-1,1),\n",
    "                               np.min(X_sax, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "        data_X.resize(data_X.shape[0], data_X.shape[1])\n",
    "        stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1),\n",
    "                             np.std(data_X, axis=1).reshape(-1,1),\n",
    "                             np.max(data_X, axis=1).reshape(-1,1),\n",
    "                             np.min(data_X, axis=1).reshape(-1,1),\n",
    "                             ])\n",
    "    \n",
    "        data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
    "        data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
    "        stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1),\n",
    "                               np.std(data_FFT, axis=1).reshape(-1,1),\n",
    "                               np.max(data_FFT, axis=1).reshape(-1,1),\n",
    "                               np.min(data_FFT, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
    "        data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
    "        stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1),\n",
    "                               np.std(data_DWT, axis=1).reshape(-1,1),\n",
    "                               np.max(data_DWT, axis=1).reshape(-1,1),\n",
    "                               np.min(data_DWT, axis=1).reshape(-1,1),\n",
    "                               ])\n",
    "    \n",
    "        return {\n",
    "            \"TS\": np.hstack([data_X, stats_X]),\n",
    "            \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
    "            \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
    "            \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
    "            \"SAX\": np.hstack([X_sax, stats_SAX])\n",
    "        }\n",
    "    \n",
    "    @jit\n",
    "    def select_model(option, random_state):\n",
    "        if option == '1nn':\n",
    "            return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
    "        elif option == '3nn':\n",
    "            return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
    "        elif option == 'svm':\n",
    "            return SVC(C = 100, gamma=0.01, kernel='linear', probability=True)\n",
    "        elif option == 'gbc':\n",
    "            return GradientBoostingClassifier(n_estimators=5, random_state=random_state)\n",
    "        elif option == 'nb':\n",
    "            return GaussianNB()\n",
    "        elif option == 'shape':\n",
    "            return ShapeDTW(n_neighbors=1)\n",
    "        elif option == 'ee':\n",
    "            return ElasticEnsemble(n_jobs=-1,\n",
    "                                   random_state=random_state,\n",
    "                                   majority_vote=True)\n",
    "        elif option == 'exrf':\n",
    "            return ExtraTreesClassifier(n_estimators=200,\n",
    "                                        criterion=\"entropy\",\n",
    "                                        class_weight=\"balanced\",\n",
    "                                        max_features=\"sqrt\",\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=None)\n",
    "        elif option == 'rd':\n",
    "            return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        else:\n",
    "            return RandomForestClassifier(n_estimators=200,\n",
    "                                          criterion=\"gini\",\n",
    "                                          class_weight=\"balanced_subsample\",\n",
    "                                          max_features=\"sqrt\",\n",
    "                                          n_jobs=-1,\n",
    "                                          random_state=None)\n",
    "    \n",
    "    @jit\n",
    "    def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
    "        trained_models = {}  # Salvar modelos treinados para cada transformação\n",
    "        X_train_transformed = TimeSeriesClassifier.transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
    "        loo = LeaveOneOut()\n",
    "        num_classes = len(np.unique(y_train))  # Número de classes\n",
    "    \n",
    "        # Treinar um modelo para cada transformação e salvar no dicionário\n",
    "        for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, colour='red', desc=\"Training Models\"):\n",
    "            model = TimeSeriesClassifier.select_model(base_option, random_state)\n",
    "            scores = []\n",
    "            for train_index, _ in loo.split(X_trans):\n",
    "                model.fit(X_trans[train_index], y_train[train_index])\n",
    "                score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
    "                scores.append(score)\n",
    "            avg_score = np.mean(scores)\n",
    "            trained_models[rep] = model  # Salvar o modelo treinado\n",
    "    \n",
    "        # Preparar dados para o meta-classificador\n",
    "        meta_features = np.zeros((X_train.shape[0], num_classes))  # Inicializar um vetor para armazenar as somas de probabilidades para cada classe\n",
    "        for i in range(X_train.shape[0]):\n",
    "            for rep, model in trained_models.items():\n",
    "                proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
    "                meta_features[i] += proba.flatten()  # Adicione as probabilidades para cada classe\n",
    "    \n",
    "        # Treinar o meta-classificador\n",
    "        meta_classifier = TimeSeriesClassifier.select_model(meta_option, random_state)\n",
    "        meta_classifier.fit(meta_features, y_train)\n",
    "    \n",
    "        return trained_models, meta_classifier\n",
    "    \n",
    "    @jit\n",
    "    def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
    "        predictions = []\n",
    "        meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
    "    \n",
    "        for i in tqdm(range(len(X_test)), ascii=True, colour='green', desc=\"Predict\"):\n",
    "            x_instance = X_test[i].reshape(1, -1)\n",
    "            x_transformed = TimeSeriesClassifier.transform_data_math(x_instance)\n",
    "    \n",
    "            instance_features = np.zeros(trained_meta_classifier.n_features_in_)  # Inicialize um vetor para armazenar as características do meta-classificador\n",
    "            for rep, model in trained_base_models.items():\n",
    "                proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))\n",
    "                instance_features += proba.flatten()  # Adicione as probabilidades para cada classe\n",
    "    \n",
    "            meta_feature = np.array(instance_features).reshape(1, -1)\n",
    "            predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
    "    \n",
    "            meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
    "    \n",
    "        # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
    "        meta_features_test = np.array(meta_features_test)\n",
    "            \n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk7b562Qnp12"
   },
   "source": [
    "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_quali_list = [ 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour','GunPoint', 'Lightning2', 'Lightning7', 'MedicalImages', 'MoteStrain', 'OliveOil', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl', 'Trace', 'TwoPatterns']\n",
    "dataset_full_list = ['MixedShapesRegularTrain','SmallKitchenAppliances','ProximalPhalanxOutlineCorrect','WordSynonyms', 'RefrigerationDevices','CinCECGTorso','ChlorineConcentration','ToeSegmentation1','TwoLeadECG','ProximalPhalanxTW','WormsTwoClass','DistalPhalanxOutlineCorrect','InsectWingbeatSound','NonInvasiveFetalECGThorax2','CricketX','Haptics','EOGVerticalSignal','MixedShapesSmallTrain','SemgHandGenderCh2','ToeSegmentation2','NonInvasiveFetalECGThorax1','FreezerSmallTrain','OSULeaf','HandOutlines','DistalPhalanxOutlineAgeGroup','FreezerRegularTrain']\n",
    "rapidos = ['SwedishLeaf', 'ProximalPhalanxOutlineAgeGroup', 'GunPointOldVersusYoung', 'Wine', 'Yoga', 'ArrowHead', 'InsectEPGSmallTrain','PowerCons','InsectEPGRegularTrain','GunPointMaleVersusFemale','ItalyPowerDemand','BME','Meat','Earthquakes','BirdChicken','BeetleFly','ACSF1']\n",
    "problematicos = ['Crop','EthanolLevel','ElectricDevices','FordB','ShapesAll','StarLightCurves','Phoneme', 'Computers','InlineSkate','PigAirwayPressure', 'PigCVP','FordA','MedicalImages','PigArtPressure', 'UWaveGestureLibraryX','UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFGKV11Rnp12",
    "outputId": "69e5f145-2736-4ca2-efbe-4678d1f562a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:01<00:00,  4.83it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:00<00:00, 119.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Beef: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:01<00:00,  3.53it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 60/60 [00:00<00:00, 123.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Car: 0.7333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 16.79it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 900/900 [00:04<00:00, 204.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia CBF: 0.8822222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 16.82it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 28/28 [00:00<00:00, 169.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Coffee: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 44.27it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 306/306 [00:01<00:00, 162.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia DiatomSizeReduction: 0.934640522875817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:04<00:00,  1.00it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 100/100 [00:00<00:00, 186.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECG200: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 27.22it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 861/861 [00:04<00:00, 203.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECGFiveDays: 0.8536585365853658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 23.03it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 88/88 [00:00<00:00, 146.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia FaceFour: 0.7954545454545454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00,  5.90it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 150/150 [00:00<00:00, 202.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia GunPoint: 0.9133333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:01<00:00,  3.51it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 61/61 [00:00<00:00, 117.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning2: 0.819672131147541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:01<00:00,  2.56it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 73/73 [00:00<00:00, 164.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning7: 0.6027397260273972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [06:27<00:00, 77.44s/it]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 760/760 [00:11<00:00, 65.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MedicalImages: 0.6986842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 10.20it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 1252/1252 [00:14<00:00, 87.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MoteStrain: 0.8490415335463258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00,  6.00it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 30/30 [00:00<00:00, 46.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia OliveOil: 0.8666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00, 12.26it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 601/601 [00:06<00:00, 87.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SonyAIBORobotSurface1: 0.7104825291181365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:00<00:00,  7.39it/s]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 953/953 [00:11<00:00, 82.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SonyAIBORobotSurface2: 0.8730325288562435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [02:05<00:00, 25.04s/it]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 300/300 [00:01<00:00, 175.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SyntheticControl: 0.9633333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [00:05<00:00,  1.08s/it]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 100/100 [00:00<00:00, 144.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Trace: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|\u001b[31m##########\u001b[0m| 5/5 [1:07:17<00:00, 807.57s/it]\n",
      "Predict: 100%|\u001b[32m##########\u001b[0m| 4000/4000 [00:36<00:00, 111.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia TwoPatterns: 0.9315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_data = []\n",
    "\n",
    "for dataset_name in dataset_quali_list:\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "    \n",
    "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    dataset_accuracies = []\n",
    "    trained_base_models, meta_classifier = TimeSeriesClassifier.train_with_meta_classifier(X_train_flat, y_train, base_option='1nn', meta_option='rd')\n",
    "    predictions_test_meta = TimeSeriesClassifier.predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n",
    "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "    dataset_accuracies.append(test_accuracy_meta)\n",
    "\n",
    "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
    "    accuracy_data.append({'Dataset Name': dataset_name, 'Accuracy': test_accuracy_meta})\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beef</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Car</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBF</td>\n",
       "      <td>0.882222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DiatomSizeReduction</td>\n",
       "      <td>0.934641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG200</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ECGFiveDays</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FaceFour</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GunPoint</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lightning2</td>\n",
       "      <td>0.819672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lightning7</td>\n",
       "      <td>0.602740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MedicalImages</td>\n",
       "      <td>0.698684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MoteStrain</td>\n",
       "      <td>0.849042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OliveOil</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SonyAIBORobotSurface1</td>\n",
       "      <td>0.710483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SonyAIBORobotSurface2</td>\n",
       "      <td>0.873033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SyntheticControl</td>\n",
       "      <td>0.963333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Trace</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TwoPatterns</td>\n",
       "      <td>0.931500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Dataset Name  Accuracy\n",
       "0                    Beef  0.666667\n",
       "1                     Car  0.733333\n",
       "2                     CBF  0.882222\n",
       "3                  Coffee  1.000000\n",
       "4     DiatomSizeReduction  0.934641\n",
       "5                  ECG200  0.870000\n",
       "6             ECGFiveDays  0.853659\n",
       "7                FaceFour  0.795455\n",
       "8                GunPoint  0.913333\n",
       "9              Lightning2  0.819672\n",
       "10             Lightning7  0.602740\n",
       "11          MedicalImages  0.698684\n",
       "12             MoteStrain  0.849042\n",
       "13               OliveOil  0.866667\n",
       "14  SonyAIBORobotSurface1  0.710483\n",
       "15  SonyAIBORobotSurface2  0.873033\n",
       "16       SyntheticControl  0.963333\n",
       "17                  Trace  0.790000\n",
       "18            TwoPatterns  0.931500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_df.to_csv('model_votingCLF+NN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48vIj_NYnp14"
   },
   "source": [
    "### Gráfico das diferenças de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gtbzTbNjnp14"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y_hat  \u001b[38;5;66;03m# depois da transformação\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y2 \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m      6\u001b[0m z1 \u001b[38;5;241m=\u001b[39m y_hat_ \u001b[38;5;66;03m#antes da transformação\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_hat' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y1 = y_hat  # depois da transformação\n",
    "y2 = y_test\n",
    "\n",
    "z1 = y_hat_ #antes da transformação\n",
    "z2 = y_test\n",
    "\n",
    "#suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "y1_smoothed = pd.Series(y1).rolling(window=window_size).mean()\n",
    "y2_smoothed = pd.Series(y2).rolling(window=window_size).mean()\n",
    "z1_smoothed = pd.Series(z1).rolling(window=window_size).mean()\n",
    "z2_smoothed = pd.Series(z2).rolling(window=window_size).mean()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained')\n",
    "\n",
    "# Conjunto de validação do classificador\n",
    "axs[0].set_title('Antes da transformação')\n",
    "axs[0].plot(z1_smoothed, label='Treino')\n",
    "axs[0].plot(z2_smoothed, label='Teste')\n",
    "axs[0].set_xlabel('Tempo (s)')\n",
    "axs[0].set_ylabel('Treino')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Conjunto de validação do meta-classificador\n",
    "axs[1].set_title('Depois da transformação')\n",
    "axs[1].plot(y1_smoothed, label='Treino')\n",
    "axs[1].plot(y2_smoothed, label='Teste')\n",
    "axs[1].set_xlabel('Tempo (s)')\n",
    "axs[1].set_ylabel('Treino')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRjM29_Xnp14"
   },
   "outputs": [],
   "source": [
    "w1 = y_hat  # meta-classificador\n",
    "w2 = y_hat_ #classificação\n",
    "\n",
    "# Suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "w1_smoothed = pd.Series(w1).rolling(window=window_size).mean()\n",
    "w2_smoothed = pd.Series(w2).rolling(window=window_size).mean()\n",
    "\n",
    "# Plotar os dados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(w1_smoothed, label='w1 (Classificação usando meta-caracteristicas)')\n",
    "plt.plot(w2_smoothed, label='w2 (classificação utilizando dados brutos)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Valores suavizados')\n",
    "plt.title('Comparação entre os resultados de um SVM')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBZ-XYnunp15"
   },
   "source": [
    "### Treino em loop de todas as opções de classificadores disponiveis no Select Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOyRjuwQnp15"
   },
   "outputs": [],
   "source": [
    "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'ee', 'shape', 'rf', 'rd']\n",
    "for algo in algos:\n",
    "\n",
    "    print(f'Meta-classificador com modelo extrator {algo.upper()}')\n",
    "\n",
    "    # Training\n",
    "    try:\n",
    "        trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option=algo)\n",
    "        # Testing\n",
    "        predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
    "        test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "\n",
    "        print(f'Acurácia do teste usando o meta-classificador com modelo extrator {algo}: {test_accuracy_meta}')\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro no teste com o {algo}: {e}\")\n",
    "    print(\"-------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w_wfxopCnp1x",
    "ctG_4yBOnp1z",
    "QuvAtr74np10",
    "AP38ocldnp10",
    "5mNDe8USnp11",
    "e0z4yRoAnp11",
    "jk7b562Qnp12",
    "48vIj_NYnp14",
    "fBZ-XYnunp15"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
