{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from aeon.datasets import load_classification\n",
    "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
    "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pywt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
    "    test_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure the Parquet files are in the correct path.\")\n",
    "    raise\n",
    "    \n",
    "    \n",
    "X_train = train_data.drop('target', axis=1).values\n",
    "y_train = train_data['target'].values\n",
    "\n",
    "X_test = test_data.drop('target', axis=1).values\n",
    "y_test = test_data['target'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de transformação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data atualizado\n",
    "def transform_data(X):\n",
    "    n_sax_symbols = int(X.shape[1] / 4)\n",
    "    n_paa_segments = int(X.shape[1] / 4)\n",
    "    \n",
    "    X_fft = np.abs(fft(X, axis=1))\n",
    "\n",
    "    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
    "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
    "\n",
    "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
    "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
    "\n",
    "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
    "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
    "\n",
    "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
    "    \n",
    "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
    "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
    "    \n",
    "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
    "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
    "\n",
    "    return {\n",
    "        \"TS\": data_X,\n",
    "        \"FFT\": data_FFT,\n",
    "        \"DWT\": data_DWT,\n",
    "        \"PAA\": X_paa,\n",
    "        \"SAX\": X_sax\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção do modelo extrator e modelo classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(option, random_state):\n",
    "    if option == '1nn':\n",
    "        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=1)\n",
    "    elif option == '3nn':\n",
    "        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=2)\n",
    "    elif option == 'svm':\n",
    "        return SVC(C = 1000, gamma=0.01, kernel='rbf', probability=True)\n",
    "    elif option == 'gbc':\n",
    "        return GradientBoostingClassifier(n_estimators=100, random_state=random_state)\n",
    "    elif option == 'nb':\n",
    "        return GaussianNB()\n",
    "    elif option == 'shape':\n",
    "        return ShapeDTW(n_neighbors=1)\n",
    "    elif option == 'ee':\n",
    "        return ElasticEnsemble(proportion_of_param_options= 0.5,\n",
    "                               proportion_train_in_param_finding= 0.5,\n",
    "                               proportion_train_for_test=0.5,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=random_state,\n",
    "                               majority_vote=True)\n",
    "    elif option == 'rd':\n",
    "        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "    else:\n",
    "        return RandomForestClassifier(n_estimators=200,\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=random_state)\n",
    "        #return RandomForestClassifier(n_estimators=100,random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino do modelos extrator e classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
    "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
    "    \n",
    "    X_train_transformed = transform_data(X_train)  # Transformar todo o conjunto de treino\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
    "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n",
    "        model = select_model(base_option, random_state)\n",
    "        scores = []\n",
    "        for train_index, _ in loo.split(X_trans):\n",
    "            model.fit(X_trans[train_index], y_train[train_index])\n",
    "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
    "        \n",
    "    # Preparar dados para o meta-classificador\n",
    "    meta_features = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        instance_features = []\n",
    "        for rep, (model, _) in trained_models.items():\n",
    "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
    "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
    "        meta_features.append(instance_features)\n",
    "    \n",
    "    meta_features = np.array(meta_features)\n",
    "    np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\n",
    "    \n",
    "    # Treinar o meta-classificador\n",
    "    meta_classifier = select_model(meta_option, random_state)\n",
    "    meta_classifier.fit(meta_features, y_train)\n",
    "    \n",
    "    return trained_models, meta_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicao do meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
    "    predictions = []\n",
    "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
    "    \n",
    "    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n",
    "        x_instance = X_test[i].reshape(1, -1)\n",
    "        x_transformed = transform_data(x_instance)\n",
    "        \n",
    "        instance_features = []\n",
    "        for rep, (model, _) in trained_base_models.items():  # Ajuste para percorrer os modelos treinados e os scores médios\n",
    "            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))  # Ajuste aqui para pegar o primeiro elemento\n",
    "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
    "        \n",
    "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
    "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
    "        \n",
    "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
    "    \n",
    "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
    "    meta_features_test = np.array(meta_features_test)\n",
    "\n",
    "    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\n",
    "    np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [07:45<00:00, 93.05s/it]\n",
      "Testing Instances: 100%|##########| 391/391 [00:02<00:00, 148.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Adiac: 0.7774936061381074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  5.92it/s]\n",
      "Testing Instances: 100%|##########| 30/30 [00:00<00:00, 92.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Beef: 0.8666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:04<00:00,  1.03it/s]\n",
      "Testing Instances: 100%|##########| 60/60 [00:00<00:00, 92.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Car: 0.8833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 13.29it/s]\n",
      "Testing Instances: 100%|##########| 900/900 [00:04<00:00, 190.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia CBF: 0.9044444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 18.06it/s]\n",
      "Testing Instances: 100%|##########| 28/28 [00:00<00:00, 145.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Coffee: 0.9642857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 33.98it/s]\n",
      "Testing Instances: 100%|##########| 306/306 [00:02<00:00, 124.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia DiatomSizeReduction: 0.934640522875817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:02<00:00,  1.94it/s]\n",
      "Testing Instances: 100%|##########| 100/100 [00:00<00:00, 213.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECG200: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 24.43it/s]\n",
      "Testing Instances: 100%|##########| 861/861 [00:04<00:00, 187.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECGFiveDays: 0.9872241579558653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 12.36it/s]\n",
      "Testing Instances: 100%|##########| 88/88 [00:00<00:00, 119.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia FaceFour: 0.8295454545454546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.35it/s]\n",
      "Testing Instances: 100%|##########| 150/150 [00:00<00:00, 189.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia GunPoint: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:04<00:00,  1.12it/s]\n",
      "Testing Instances: 100%|##########| 61/61 [00:00<00:00, 88.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning2: 0.7704918032786885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:06<00:00,  1.37s/it]\n",
      "Testing Instances: 100%|##########| 73/73 [00:00<00:00, 113.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning7: 0.6164383561643836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 33.40it/s]\n",
      "Testing Instances: 100%|##########| 1252/1252 [00:05<00:00, 215.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MoteStrain: 0.8642172523961661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.32it/s]\n",
      "Testing Instances: 100%|##########| 30/30 [00:00<00:00, 88.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia OliveOil: 0.8666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [04:43<00:00, 56.76s/it]\n",
      "Testing Instances: 100%|##########| 760/760 [00:03<00:00, 194.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MedicalImages: 0.7144736842105263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:09<00:00,  1.91s/it]\n",
      "Testing Instances: 100%|##########| 100/100 [00:00<00:00, 138.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Trace: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "dataset_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour',\n",
    "                'GunPoint', 'Lightning2', 'Lightning7', 'MoteStrain', 'OliveOil','MedicalImages', 'Trace', 'TwoPatterns', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl']\n",
    "#'\n",
    "\n",
    "# Para cada conjunto de dados na lista\n",
    "for dataset_name in dataset_list:\n",
    "    # Carregue os dados de treinamento e teste\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "    \n",
    "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    dataset_accuracies = []\n",
    "    trained_base_models, meta_classifier = train_with_meta_classifier(X_train_flat, y_train, base_option='svm', meta_option='rd')\n",
    "    predictions_test_meta = predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n",
    "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "    dataset_accuracies.append(test_accuracy_meta)\n",
    "        \n",
    "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
    "        \n",
    "#np.savetxt(\"Results_MSLOO_.csv\", dataset_accuracies, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'random_forest']\n",
    "    # Carregue os dados de treinamento e teste\n",
    "try:\n",
    "    train_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\GunPoint_TRAIN.parquet')\n",
    "    test_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\GunPoint_TEST.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure the Parquet files are in the correct path.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_data.drop('target', axis=1).values\n",
    "y_train = train_data['target'].values\n",
    "\n",
    "X_test = test_data.drop('target', axis=1).values\n",
    "y_test = test_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.09it/s]\n",
      "Testing Instances: 100%|##########| 150/150 [00:00<00:00, 177.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9533333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option='rd')\n",
    "predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
    "test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "print(f\"Accuracy: {test_accuracy_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Testing Instances: 100%|##########| 60/60 [00:02<00:00, 28.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Treino\n",
    "trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, train_labels, base_option='svm', meta_option='rd', random_state=42)\n",
    "\n",
    "# Teste\n",
    "predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n",
    "\n",
    "# Resultado\n",
    "test_accuracy_meta = accuracy_score(test_labels, predictions_test_meta)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy_meta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando um único modelo - SVM como extrator e meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treino\n",
    "trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, train_labels, base_option='random_forest', meta_option='svm', random_state=42)\n",
    "\n",
    "# Teste\n",
    "predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n",
    "\n",
    "# Resultado\n",
    "test_accuracy_meta = accuracy_score(test_labels, predictions_test_meta)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy_meta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste utilizando o classificador SVM\n",
    "meta_attrib_train = np.loadtxt(\"meta-features-train.csv\", delimiter=\",\")\n",
    "meta_attrib_test = np.loadtxt(\"meta-features-test.csv\", delimiter=\",\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(meta_attrib_train, y_train)\n",
    "y_hat = clf.predict(meta_attrib_test)\n",
    "test_accuracy_meta = accuracy_score(y_hat, y_test)\n",
    "print(f\"Accuracy: {test_accuracy_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = SVC(probability=True)\n",
    "clf_2.fit(X_train, y_train)\n",
    "y_hat_ = clf_2.predict(X_test)\n",
    "test_accuracy_meta_2 = accuracy_score(y_hat_,y_test)\n",
    "print(f\"Accuracy: {test_accuracy_meta_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico das diferenças de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y1 = y_hat  # depois da transformação\n",
    "y2 = y_test  \n",
    "\n",
    "z1 = y_hat_ #antes da transformação\n",
    "z2 = y_test\n",
    "\n",
    "#suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "y1_smoothed = pd.Series(y1).rolling(window=window_size).mean()\n",
    "y2_smoothed = pd.Series(y2).rolling(window=window_size).mean()\n",
    "z1_smoothed = pd.Series(z1).rolling(window=window_size).mean()\n",
    "z2_smoothed = pd.Series(z2).rolling(window=window_size).mean()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained')\n",
    "\n",
    "# Conjunto de validação do classificador\n",
    "axs[0].set_title('Antes da transformação')\n",
    "axs[0].plot(z1_smoothed, label='Treino')\n",
    "axs[0].plot(z2_smoothed, label='Teste')\n",
    "axs[0].set_xlabel('Tempo (s)')\n",
    "axs[0].set_ylabel('Treino')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Conjunto de validação do meta-classificador\n",
    "axs[1].set_title('Depois da transformação')\n",
    "axs[1].plot(y1_smoothed, label='Treino')\n",
    "axs[1].plot(y2_smoothed, label='Teste')\n",
    "axs[1].set_xlabel('Tempo (s)')\n",
    "axs[1].set_ylabel('Treino')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = y_hat  # meta-classificador\n",
    "w2 = y_hat_ #classificação\n",
    "\n",
    "# Suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "w1_smoothed = pd.Series(w1).rolling(window=window_size).mean()\n",
    "w2_smoothed = pd.Series(w2).rolling(window=window_size).mean()\n",
    "\n",
    "# Plotar os dados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(w1_smoothed, label='w1 (Classificação usando meta-caracteristicas)')\n",
    "plt.plot(w2_smoothed, label='w2 (classificação utilizando dados brutos)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Valores suavizados')\n",
    "plt.title('Comparação entre os resultados de um SVM')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino em loop de todas as opções de classificadores disponiveis no Select Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'ee', 'shape', 'rf', 'rd']\n",
    "for algo in algos:\n",
    "    \n",
    "    print(f'Meta-classificador com modelo extrator {algo.upper()}')\n",
    "    \n",
    "    # Training\n",
    "    try:\n",
    "        trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option=algo)\n",
    "        # Testing\n",
    "        predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
    "        test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "        \n",
    "        print(f'Acurácia do teste usando o meta-classificador com modelo extrator {algo}: {test_accuracy_meta}')\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro no teste com o {algo}: {e}\")\n",
    "    print(\"-------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
