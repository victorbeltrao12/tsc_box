{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "from aeon.datasets import load_classification\n",
    "from aeon.datasets.tsc_data_lists import univariate_equal_length\n",
    "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier, ShapeDTW, ElasticEnsemble\n",
    "from aeon.utils.numba.stats import (\n",
    "    row_iqr,\n",
    "    row_mean,\n",
    "    row_median,\n",
    "    row_numba_max,\n",
    "    row_numba_min,\n",
    "    row_slope,\n",
    "    row_std,\n",
    ")\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"try:\n",
    "    train_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
    "    test_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\Car_TRAIN.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure the Parquet files are in the correct path.\")\n",
    "    raise\n",
    "    \n",
    "    \n",
    "X_train = train_data.drop('target', axis=1).values\n",
    "y_train = train_data['target'].values\n",
    "\n",
    "X_test = test_data.drop('target', axis=1).values\n",
    "y_test = test_data['target'].values\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de transformação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data atualizado\n",
    "def transform_data(X):\n",
    "    n_sax_symbols = int(X.shape[1] / 4)\n",
    "    n_paa_segments = int(X.shape[1] / 4)\n",
    "    \n",
    "    X_fft = np.abs(fft(X, axis=1))\n",
    "\n",
    "    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
    "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
    "\n",
    "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
    "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
    "\n",
    "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
    "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
    "\n",
    "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
    "    \n",
    "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
    "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
    "    \n",
    "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
    "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
    "\n",
    "    return {\n",
    "        \"TS\": data_X,\n",
    "        \"FFT\": data_FFT,\n",
    "        \"DWT\": data_DWT,\n",
    "        \"PAA\": X_paa,\n",
    "        \"SAX\": X_sax\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acrescentando algumas métricas nas transformações como média, soma, máximo, minimo etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def transform_data_math(X):\n",
    "    n_sax_symbols = int(X.shape[1] / 4)\n",
    "    n_paa_segments = int(X.shape[1] / 4)\n",
    "    \n",
    "    X_fft = np.abs(fft(X, axis=1))\n",
    "\n",
    "    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
    "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
    "\n",
    "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
    "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
    "    stats_PAA = np.hstack([np.mean(X_paa, axis=1).reshape(-1,1), \n",
    "                           np.std(X_paa, axis=1).reshape(-1,1), \n",
    "                           np.max(X_paa, axis=1).reshape(-1,1), \n",
    "                           np.min(X_paa, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
    "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
    "    stats_SAX = np.hstack([np.mean(X_sax, axis=1).reshape(-1,1), \n",
    "                           np.std(X_sax, axis=1).reshape(-1,1), \n",
    "                           np.max(X_sax, axis=1).reshape(-1,1), \n",
    "                           np.min(X_sax, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
    "    stats_X = np.hstack([np.mean(data_X, axis=1).reshape(-1,1), \n",
    "                         np.std(data_X, axis=1).reshape(-1,1), \n",
    "                         np.max(data_X, axis=1).reshape(-1,1), \n",
    "                         np.min(data_X, axis=1).reshape(-1,1),\n",
    "                         ])\n",
    "\n",
    "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
    "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
    "    stats_FFT = np.hstack([np.mean(data_FFT, axis=1).reshape(-1,1), \n",
    "                           np.std(data_FFT, axis=1).reshape(-1,1), \n",
    "                           np.max(data_FFT, axis=1).reshape(-1,1), \n",
    "                           np.min(data_FFT, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
    "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
    "    stats_DWT = np.hstack([np.mean(data_DWT, axis=1).reshape(-1,1), \n",
    "                           np.std(data_DWT, axis=1).reshape(-1,1), \n",
    "                           np.max(data_DWT, axis=1).reshape(-1,1), \n",
    "                           np.min(data_DWT, axis=1).reshape(-1,1),\n",
    "                           ])\n",
    "\n",
    "    return {\n",
    "        \"TS\": np.hstack([data_X, stats_X]),\n",
    "        \"FFT\": np.hstack([data_FFT, stats_FFT]),\n",
    "        \"DWT\": np.hstack([data_DWT, stats_DWT]),\n",
    "        \"PAA\": np.hstack([X_paa, stats_PAA]),\n",
    "        \"SAX\": np.hstack([X_sax, stats_SAX])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform_data junto de extração de caracteristicas - ERRO na função Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_extract_features(X, default_fc_parameters=MinimalFCParameters()):\n",
    "    n_sax_symbols = int(X.shape[1] / 4)\n",
    "    n_paa_segments = int(X.shape[1] / 4)\n",
    "    \n",
    "    X_fft = np.abs(fft(X, axis=1))\n",
    "\n",
    "    coeffs_cA, coeffs_cD = pywt.dwt(X, 'db1', axis=1)\n",
    "    X_dwt = np.hstack((coeffs_cA, coeffs_cD))\n",
    "\n",
    "    paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "    X_paa_ = paa.inverse_transform(paa.fit_transform(X))\n",
    "    X_paa = X_paa_.reshape(X_paa_.shape[0], -1)\n",
    "\n",
    "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "    X_sax_ = sax.inverse_transform(sax.fit_transform(X))\n",
    "    X_sax = X_sax_.reshape(X_sax_.shape[0], -1)\n",
    "\n",
    "    data_X = TimeSeriesScalerMeanVariance().fit_transform(X)\n",
    "    data_X.resize(data_X.shape[0], data_X.shape[1])\n",
    "    \n",
    "    data_FFT = TimeSeriesScalerMeanVariance().fit_transform(X_fft)\n",
    "    data_FFT.resize(data_FFT.shape[0], data_FFT.shape[1])\n",
    "    \n",
    "    data_DWT = TimeSeriesScalerMeanVariance().fit_transform(X_dwt)\n",
    "    data_DWT.resize(data_DWT.shape[0], data_DWT.shape[1])\n",
    "\n",
    "    id_column = np.arange(X.shape[0]).reshape(-1, 1)  # Criando um índice único para cada linha de X\n",
    "\n",
    "    transformed_data = {\n",
    "        \"TS\": np.hstack([id_column, data_X]),\n",
    "        \"FFT\": np.hstack([id_column, data_FFT]),\n",
    "        \"DWT\": np.hstack([id_column, data_DWT]),\n",
    "        \"PAA\": np.hstack([id_column, X_paa]),\n",
    "        \"SAX\": np.hstack([id_column, X_sax])\n",
    "    }\n",
    "    \n",
    "    extracted_features_dict = {}\n",
    "    for key, value in transformed_data.items():\n",
    "        # Criando DataFrame\n",
    "        df = pd.DataFrame(value, columns=[f'{key}_{i}' for i in range(value.shape[1])])\n",
    "        df['id'] = df.index\n",
    "        \n",
    "        # Extrair características\n",
    "        features = extract_features(df, column_id='id', default_fc_parameters=default_fc_parameters)\n",
    "        clean_features = np.delete(features, np.where(np.std(features, axis=0) < 10e-5), axis=1)\n",
    "        \n",
    "        # Armazenar características\n",
    "        extracted_features_dict[key] = clean_features\n",
    "    \n",
    "    return extracted_features_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste utilizando Math no transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
    "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
    "    \n",
    "    X_train_transformed = transform_data_math(X_train)  # Transformar todo o conjunto de treino\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
    "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n",
    "        model = select_model(base_option, random_state)\n",
    "        scores = []\n",
    "        for train_index, _ in loo.split(X_trans):\n",
    "            model.fit(X_trans[train_index], y_train[train_index])\n",
    "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
    "        \n",
    "    # Preparar dados para o meta-classificador\n",
    "    meta_features = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        instance_features = []\n",
    "        for rep, (model, _) in trained_models.items():\n",
    "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
    "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
    "        meta_features.append(instance_features)\n",
    "    \n",
    "    meta_features = np.array(meta_features)\n",
    "    #np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\n",
    "    \n",
    "    # Treinar o meta-classificador\n",
    "    meta_classifier = select_model(meta_option, random_state)\n",
    "    meta_classifier.fit(meta_features, y_train)\n",
    "    \n",
    "    return trained_models, meta_classifier\n",
    "\n",
    "@jit\n",
    "def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
    "    predictions = []\n",
    "    meta_features_test = []\n",
    "    \n",
    "    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n",
    "        x_instance = X_test[i].reshape(1, -1)\n",
    "        x_transformed = transform_data_math(x_instance)\n",
    "        \n",
    "        instance_features = []\n",
    "        for rep, (model, _) in trained_base_models.items():\n",
    "            proba = model.predict_proba(x_transformed[rep].reshape(1, -1))  # Ajuste para acessar corretamente as características transformadas\n",
    "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
    "        \n",
    "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
    "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])\n",
    "        meta_features_test.append(meta_feature)\n",
    "    \n",
    "    meta_features_test = np.array(meta_features_test)\n",
    "    #np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [08:08<00:00, 97.78s/it] \n",
      "Testing Instances: 100%|##########| 391/391 [00:02<00:00, 165.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Adiac: 0.7979539641943734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  5.57it/s]\n",
      "Testing Instances: 100%|##########| 30/30 [00:00<00:00, 150.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Beef: 0.8666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:05<00:00,  1.08s/it]\n",
      "Testing Instances: 100%|##########| 60/60 [00:00<00:00, 128.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Car: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 13.80it/s]\n",
      "Testing Instances: 100%|##########| 900/900 [00:04<00:00, 215.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia CBF: 0.8711111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 17.46it/s]\n",
      "Testing Instances: 100%|##########| 28/28 [00:00<00:00, 153.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Coffee: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 34.43it/s]\n",
      "Testing Instances: 100%|##########| 306/306 [00:01<00:00, 162.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia DiatomSizeReduction: 0.9705882352941176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:02<00:00,  1.91it/s]\n",
      "Testing Instances: 100%|##########| 100/100 [00:00<00:00, 222.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECG200: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 26.81it/s]\n",
      "Testing Instances: 100%|##########| 861/861 [00:04<00:00, 214.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia ECGFiveDays: 0.9988385598141696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 11.57it/s]\n",
      "Testing Instances: 100%|##########| 88/88 [00:00<00:00, 164.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia FaceFour: 0.8409090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.53it/s]\n",
      "Testing Instances: 100%|##########| 150/150 [00:00<00:00, 214.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia GunPoint: 0.9466666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:04<00:00,  1.00it/s]\n",
      "Testing Instances: 100%|##########| 61/61 [00:00<00:00, 118.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning2: 0.7540983606557377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:07<00:00,  1.50s/it]\n",
      "Testing Instances: 100%|##########| 73/73 [00:00<00:00, 169.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Lightning7: 0.547945205479452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 28.59it/s]\n",
      "Testing Instances: 100%|##########| 1252/1252 [00:05<00:00, 233.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MoteStrain: 0.8674121405750799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  6.85it/s]\n",
      "Testing Instances: 100%|##########| 30/30 [00:00<00:00, 132.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia OliveOil: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [04:48<00:00, 57.66s/it]\n",
      "Testing Instances: 100%|##########| 760/760 [00:03<00:00, 210.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia MedicalImages: 0.7368421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:10<00:00,  2.06s/it]\n",
      "Testing Instances: 100%|##########| 100/100 [00:00<00:00, 175.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Trace: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [1:38:53<00:00, 1186.61s/it]\n",
      "Testing Instances: 100%|##########| 4000/4000 [00:23<00:00, 172.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia TwoPatterns: 0.92925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 37.53it/s]\n",
      "Testing Instances: 100%|##########| 601/601 [00:02<00:00, 240.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SonyAIBORobotSurface1: 0.7753743760399334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00, 25.16it/s]\n",
      "Testing Instances: 100%|##########| 953/953 [00:03<00:00, 241.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SonyAIBORobotSurface2: 0.8551941238195173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:56<00:00, 11.28s/it]\n",
      "Testing Instances: 100%|##########| 300/300 [00:01<00:00, 232.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia SyntheticControl: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour',\n",
    "                'GunPoint', 'Lightning2', 'Lightning7', 'MoteStrain', 'OliveOil','MedicalImages', 'Trace', 'TwoPatterns',\n",
    "                'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl']\n",
    "\n",
    "# Para cada conjunto de dados na lista\n",
    "for dataset_name in dataset_list:\n",
    "    # Carregue os dados de treinamento e teste\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "    \n",
    "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    dataset_accuracies = []\n",
    "    trained_base_models, meta_classifier = train_with_meta_classifier(X_train_flat, y_train, base_option='svm', meta_option='rd')\n",
    "    predictions_test_meta = predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n",
    "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "    dataset_accuracies.append(test_accuracy_meta)\n",
    "        \n",
    "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
    "        \n",
    "#np.savetxt(\"Results_MSLOO_.csv\", dataset_accuracies, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "dataset_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour',\n",
    "                'GunPoint', 'Lightning2', 'Lightning7', 'MoteStrain', 'OliveOil','MedicalImages', 'Trace', 'TwoPatterns',\n",
    "                'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl']\n",
    "\n",
    "# Para cada conjunto de dados na lista\n",
    "for dataset_name in dataset_list:\n",
    "    # Carregue os dados de treinamento e teste\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "    \n",
    "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    dataset_accuracies = []\n",
    "    trained_base_models, meta_classifier = train_with_meta_classifier(X_train_flat, y_train, base_option='3nn', meta_option='rd')\n",
    "    predictions_test_meta = predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n",
    "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "    dataset_accuracies.append(test_accuracy_meta)\n",
    "        \n",
    "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção do modelo extrator e modelo classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def select_model(option, random_state):\n",
    "    if option == '1nn':\n",
    "        return KNeighborsTimeSeriesClassifier(distance='euclidean', n_neighbors=1, n_jobs=-1)\n",
    "    elif option == '3nn':\n",
    "        return KNeighborsTimeSeriesClassifier(distance='dtw', n_neighbors=3, n_jobs=-1)\n",
    "    elif option == 'svm':\n",
    "        return SVC(C = 1000, gamma=0.01, kernel='rbf', probability=True)\n",
    "    elif option == 'gbc':\n",
    "        return GradientBoostingClassifier(n_estimators=100, random_state=random_state)\n",
    "    elif option == 'nb':\n",
    "        return GaussianNB()\n",
    "    elif option == 'shape':\n",
    "        return ShapeDTW(n_neighbors=1)\n",
    "    elif option == 'ee':\n",
    "        return ElasticEnsemble(proportion_of_param_options= 0.5,\n",
    "                               proportion_train_in_param_finding= 0.5,\n",
    "                               proportion_train_for_test=0.5,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=random_state,\n",
    "                               majority_vote=True)\n",
    "    elif option == 'rd':\n",
    "        return RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "    else:\n",
    "        return RandomForestClassifier(n_estimators=200,\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=random_state)\n",
    "        #return RandomForestClassifier(n_estimators=100,random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino do modelos extrator e classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_meta_classifier(X_train, y_train, base_option='random_forest', meta_option='1nn', random_state=42):\n",
    "    trained_models = {}  # Salvar modelos treinados para cada transformação\n",
    "    \n",
    "    X_train_transformed = transform_data(X_train)  # Transformar todo o conjunto de treino\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Treinar um modelo para cada transformação e salvar no dicionário\n",
    "    for rep, X_trans in tqdm(X_train_transformed.items(), ascii=True, desc=\"Training Base Models\"):\n",
    "        model = select_model(base_option, random_state)\n",
    "        scores = []\n",
    "        for train_index, _ in loo.split(X_trans):\n",
    "            model.fit(X_trans[train_index], y_train[train_index])\n",
    "            score = model.score(X_trans[train_index], y_train[train_index])  # Score do modelo nos dados de treino\n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        trained_models[rep] = (model, avg_score)  # Salvar o modelo treinado e a média dos scores\n",
    "        \n",
    "    # Preparar dados para o meta-classificador\n",
    "    meta_features = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        instance_features = []\n",
    "        for rep, (model, _) in trained_models.items():\n",
    "            proba = model.predict_proba(X_train_transformed[rep][i].reshape(1, -1))\n",
    "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
    "        meta_features.append(instance_features)\n",
    "    \n",
    "    meta_features = np.array(meta_features)\n",
    "    np.savetxt(\"meta-features-train.csv\", meta_features, delimiter=\",\")\n",
    "    \n",
    "    # Treinar o meta-classificador\n",
    "    meta_classifier = select_model(meta_option, random_state)\n",
    "    meta_classifier.fit(meta_features, y_train)\n",
    "    \n",
    "    return trained_models, meta_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicao do meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_meta_classifier(X_test, trained_base_models, trained_meta_classifier):\n",
    "    predictions = []\n",
    "    meta_features_test = []  # Inicialize uma lista para armazenar todos os meta-recursos dos dados de teste\n",
    "    \n",
    "    for i in tqdm(range(len(X_test)), ascii=True, desc=\"Testing Instances\"):\n",
    "        x_instance = X_test[i].reshape(1, -1)\n",
    "        x_transformed = transform_data(x_instance)\n",
    "        \n",
    "        instance_features = []\n",
    "        for rep, (model, _) in trained_base_models.items():  # Ajuste para percorrer os modelos treinados e os scores médios\n",
    "            proba = model.predict_proba(x_transformed[rep][0].reshape(1, -1))  # Ajuste aqui para pegar o primeiro elemento\n",
    "            instance_features.extend(proba.flatten())  # Estender a lista com todas as probabilidades\n",
    "        \n",
    "        meta_feature = np.array(instance_features).reshape(1, -1)\n",
    "        predictions.append(trained_meta_classifier.predict(meta_feature)[0])  # Adicionar a previsão à lista de previsões\n",
    "        \n",
    "        meta_features_test.append(meta_feature.flatten())  # Adicionar meta-recursos da instância atual à lista\n",
    "    \n",
    "    # Converter a lista de meta-recursos dos dados de teste em um array numpy\n",
    "    meta_features_test = np.array(meta_features_test)\n",
    "\n",
    "    # Salvar todos os meta-recursos dos dados de teste em um arquivo CSV\n",
    "    np.savetxt(\"meta-features-test.csv\", meta_features_test, delimiter=\",\")\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando um único modelo - Random Forest como extrator e SVM como meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['Adiac', 'Beef', 'Car', 'CBF', 'Coffee', 'DiatomSizeReduction', 'ECG200', 'ECGFiveDays', 'FaceFour',\n",
    "                'GunPoint', 'Lightning2', 'Lightning7', 'MoteStrain', 'OliveOil','MedicalImages', 'Trace', 'TwoPatterns', 'SonyAIBORobotSurface1','SonyAIBORobotSurface2', 'SyntheticControl']\n",
    "#'\n",
    "\n",
    "# Para cada conjunto de dados na lista\n",
    "for dataset_name in dataset_list:\n",
    "    # Carregue os dados de treinamento e teste\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"TRAIN\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "    \n",
    "    # Achatando os dados para 2D, pois alguns algoritmos esperam 2D\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    dataset_accuracies = []\n",
    "    trained_base_models, meta_classifier = train_with_meta_classifier(X_train_flat, y_train, base_option='1nn', meta_option='rd')\n",
    "    predictions_test_meta = predict_with_meta_classifier(X_test_flat, trained_base_models, meta_classifier)\n",
    "    test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "    dataset_accuracies.append(test_accuracy_meta)\n",
    "        \n",
    "    print(f\"Acurácia {dataset_name}: {test_accuracy_meta}\")\n",
    "        \n",
    "#np.savetxt(\"Results_MSLOO_.csv\", dataset_accuracies, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'random_forest']\n",
    "    # Carregue os dados de treinamento e teste\n",
    "try:\n",
    "    train_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\GunPoint_TRAIN.parquet')\n",
    "    test_data = pd.read_parquet('D:\\_MESTRADO\\_Meta_Learning\\MSC\\CSV_Parquet\\GunPoint_TEST.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure the Parquet files are in the correct path.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_data.drop('target', axis=1).values\n",
    "y_train = train_data['target'].values\n",
    "\n",
    "X_test = test_data.drop('target', axis=1).values\n",
    "y_test = test_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:00<00:00,  7.09it/s]\n",
      "Testing Instances: 100%|##########| 150/150 [00:00<00:00, 177.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9533333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option='rd')\n",
    "predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
    "test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "print(f\"Accuracy: {test_accuracy_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Base Models: 100%|##########| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Testing Instances: 100%|##########| 60/60 [00:02<00:00, 28.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Treino\n",
    "trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, train_labels, base_option='svm', meta_option='rd', random_state=42)\n",
    "\n",
    "# Teste\n",
    "predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n",
    "\n",
    "# Resultado\n",
    "test_accuracy_meta = accuracy_score(test_labels, predictions_test_meta)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy_meta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando um único modelo - SVM como extrator e meta-classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treino\n",
    "trained_base_models, meta_classifier = train_with_meta_classifier(xtrain, train_labels, base_option='random_forest', meta_option='svm', random_state=42)\n",
    "\n",
    "# Teste\n",
    "predictions_test_meta = predict_with_meta_classifier(xtest, trained_base_models, meta_classifier)\n",
    "\n",
    "# Resultado\n",
    "test_accuracy_meta = accuracy_score(test_labels, predictions_test_meta)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy_meta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste utilizando o classificador SVM\n",
    "meta_attrib_train = np.loadtxt(\"meta-features-train.csv\", delimiter=\",\")\n",
    "meta_attrib_test = np.loadtxt(\"meta-features-test.csv\", delimiter=\",\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(meta_attrib_train, y_train)\n",
    "y_hat = clf.predict(meta_attrib_test)\n",
    "test_accuracy_meta = accuracy_score(y_hat, y_test)\n",
    "print(f\"Accuracy: {test_accuracy_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = SVC(probability=True)\n",
    "clf_2.fit(X_train, y_train)\n",
    "y_hat_ = clf_2.predict(X_test)\n",
    "test_accuracy_meta_2 = accuracy_score(y_hat_,y_test)\n",
    "print(f\"Accuracy: {test_accuracy_meta_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico das diferenças de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y1 = y_hat  # depois da transformação\n",
    "y2 = y_test  \n",
    "\n",
    "z1 = y_hat_ #antes da transformação\n",
    "z2 = y_test\n",
    "\n",
    "#suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "y1_smoothed = pd.Series(y1).rolling(window=window_size).mean()\n",
    "y2_smoothed = pd.Series(y2).rolling(window=window_size).mean()\n",
    "z1_smoothed = pd.Series(z1).rolling(window=window_size).mean()\n",
    "z2_smoothed = pd.Series(z2).rolling(window=window_size).mean()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained')\n",
    "\n",
    "# Conjunto de validação do classificador\n",
    "axs[0].set_title('Antes da transformação')\n",
    "axs[0].plot(z1_smoothed, label='Treino')\n",
    "axs[0].plot(z2_smoothed, label='Teste')\n",
    "axs[0].set_xlabel('Tempo (s)')\n",
    "axs[0].set_ylabel('Treino')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Conjunto de validação do meta-classificador\n",
    "axs[1].set_title('Depois da transformação')\n",
    "axs[1].plot(y1_smoothed, label='Treino')\n",
    "axs[1].plot(y2_smoothed, label='Teste')\n",
    "axs[1].set_xlabel('Tempo (s)')\n",
    "axs[1].set_ylabel('Treino')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = y_hat  # meta-classificador\n",
    "w2 = y_hat_ #classificação\n",
    "\n",
    "# Suavizar os dados do gráfico\n",
    "window_size = 15\n",
    "w1_smoothed = pd.Series(w1).rolling(window=window_size).mean()\n",
    "w2_smoothed = pd.Series(w2).rolling(window=window_size).mean()\n",
    "\n",
    "# Plotar os dados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(w1_smoothed, label='w1 (Classificação usando meta-caracteristicas)')\n",
    "plt.plot(w2_smoothed, label='w2 (classificação utilizando dados brutos)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Valores suavizados')\n",
    "plt.title('Comparação entre os resultados de um SVM')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino em loop de todas as opções de classificadores disponiveis no Select Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = ['1nn', '3nn', 'svm', 'nb', 'gbc', 'ee', 'shape', 'rf', 'rd']\n",
    "for algo in algos:\n",
    "    \n",
    "    print(f'Meta-classificador com modelo extrator {algo.upper()}')\n",
    "    \n",
    "    # Training\n",
    "    try:\n",
    "        trained_base_models, meta_classifier = train_with_meta_classifier(X_train, y_train, base_option='svm', meta_option=algo)\n",
    "        # Testing\n",
    "        predictions_test_meta = predict_with_meta_classifier(X_test, trained_base_models, meta_classifier)\n",
    "        test_accuracy_meta = np.mean(predictions_test_meta == y_test)\n",
    "        \n",
    "        print(f'Acurácia do teste usando o meta-classificador com modelo extrator {algo}: {test_accuracy_meta}')\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro no teste com o {algo}: {e}\")\n",
    "    print(\"-------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
